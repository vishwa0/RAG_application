{"docstore/metadata": {"fe7ef5a9-c427-43b4-946f-d6f93765dde7": {"doc_hash": "98bc69d52388092482a99203beaac1a88167c190080d8fd540441e27a8f2ca12"}, "60534e99-9827-4f3b-9823-84d6aaab600d": {"doc_hash": "0974e6e159ce3a5df25b1c1c3a97c3530eb4178721760528f7c38a6a0ae14b0a"}, "4dc5e6ce-203f-481b-9cc5-8ed7ea0ee624": {"doc_hash": "ef1f7cf17c81b933bb5a75799733b83b904bbec2dd7a41e91a3b7263684da1b1"}, "b054b4cf-4c7d-4fcc-8ed7-e2599a3213e1": {"doc_hash": "28a1854ff1025f315a96541ba6917b8879f90b9da27a72400bc608490b342d2a"}, "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a": {"doc_hash": "2e178ba08a1978993ef067e5d07dfdf7a5adbab83c605a67f988ffd8fb0e5a11"}, "fd150fb1-388d-4359-8477-79a451983254": {"doc_hash": "ef056758abae0174684a86693de79f337799a25c95d3c96a20118d606bb350cd", "ref_doc_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7"}, "72ceec9f-a67d-405d-a90d-5cf289ad9bb8": {"doc_hash": "01fecaa489aac31c53addf7c2fd75e5e0145ffcc489ca4aa1d34a573a6f29fa3", "ref_doc_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7"}, "c3774bb9-5d96-4b9e-8614-fbb5361baa07": {"doc_hash": "4007654a099505db5d4fa233d32b5df053c8a9104742bacd58a12b9663cd7347", "ref_doc_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7"}, "631617a8-4364-422b-bb85-da005f29f8c2": {"doc_hash": "0f11330d9fbfb4d45c90bf2b35fee9ed28f1d25b9880c3e0ccf734b719096421", "ref_doc_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7"}, "6c6a9365-d48a-44e5-b3ad-a1b4a3a49a62": {"doc_hash": "cc0f70e85690fd3a3cd5b14cec67c3653aaaf8902e2d990759f1faccaa0c3dbe", "ref_doc_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7"}, "84a5d28d-c149-4062-96d2-305e5209dd44": {"doc_hash": "d7fb3616d3b1f98cc0e9c678e591c69519f7e3e252c637155c993985e7cd33a0", "ref_doc_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7"}, "e844b618-3096-4a04-9ef1-057604988838": {"doc_hash": "0fedc6e0b4729f1a6e663a6a56ca9fa835ed295fe6ee9b8dc996b5ad4b734581", "ref_doc_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7"}, "e69c32cb-fb80-4e56-9f6b-cef9238a95cf": {"doc_hash": "caace1c49ff881ef236a303d2e2cbc3095950a5de6de4859358d055b40aea40e", "ref_doc_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7"}, "87689162-96e4-4395-9f13-31c3710a19cf": {"doc_hash": "d0ece0f49f38d395a542c47f4c3a704f16d32a8c910f26049da4c0846aa859c7", "ref_doc_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7"}, "8422b33b-b068-4bcf-b083-60fbb4fbe50b": {"doc_hash": "af6a220f87cb3b795242ff6892c554ef4e10322cb1831c6a6f2c9033b48d279e", "ref_doc_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7"}, "d1ff20f6-ccc9-45fa-a72a-60171d2b0c92": {"doc_hash": "08c18bed00b183e3ac693989f4f9ba5d80c825ba164e3647a6d9d7c04b490fe2", "ref_doc_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7"}, "c155ef56-7eca-4a18-a396-2f7eb428d7c4": {"doc_hash": "f156f78b67d7d7fa5c6b8aa16f596afe5bb7744652e45ac7dd75e20494f5d970", "ref_doc_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7"}, "e7c512d8-cda4-4438-a1f3-36ecedbf8f73": {"doc_hash": "5ce6382c7bbdb68f83bad4492669980b13090337e870ad97e9ec6873a65eb3e5", "ref_doc_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7"}, "dc6779c2-4747-4628-bbdc-7af4ecf9449a": {"doc_hash": "79975665745f987da5d9bdc9945154879b0ed270a6198e9e7f15181d3852ef0b", "ref_doc_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7"}, "0b70dbb3-7efd-49d8-84a7-813edd0196e6": {"doc_hash": "09d4f01eda5ff864e2238868e51c8e25ebe13a0f9380ae23138a62539b6ab4ce", "ref_doc_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7"}, "a8f228fa-d17f-4ef3-8175-48fe13ee2ad6": {"doc_hash": "873795c9004666454606039f2b97467cda316519c4f9c6fc070a64427ac17add", "ref_doc_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7"}, "bfdcb818-99f4-4a0c-b76a-eefb141e73be": {"doc_hash": "7518e88f7a867fb5d365686bb5c832a845de273a0c0780af4c257c95439c7969", "ref_doc_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7"}, "be3c7305-6e99-4db0-ad3f-d729adf163e1": {"doc_hash": "5121d6f99b30fbedd11f0b6e260b980d30e9a944afe4d36661b7a0ccf5144bff", "ref_doc_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7"}, "619ba4a9-6d0e-40d1-8e7a-5e5b15686c33": {"doc_hash": "aa166a1dae58469b884c47c85f54471aa9cf815e3b21d30c4b20e7c60bc4fa83", "ref_doc_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7"}, "c227d33d-2e43-4176-b557-b913384f103a": {"doc_hash": "2adf4cb466e18792feda56b84292784b464849d18d628757a850a07f1f0efd36", "ref_doc_id": "60534e99-9827-4f3b-9823-84d6aaab600d"}, "b119e89a-46de-4864-ab90-faae17968b6d": {"doc_hash": "266c80ced2cd8c4f64a12d2cea103501db59697c4a163f1e563c57ca382000f6", "ref_doc_id": "60534e99-9827-4f3b-9823-84d6aaab600d"}, "e9501756-36af-4385-a6cc-cdce40361954": {"doc_hash": "475fa89082f8c1ffef2366ec93b1fb921c668c425b87dd36d2c8a58b4a899f86", "ref_doc_id": "60534e99-9827-4f3b-9823-84d6aaab600d"}, "7c047e1b-d810-48e8-8349-c4c51d06c128": {"doc_hash": "72725a763c5c6e6ef01bf2bf32bf654b5ec0b6bd94be6261753cab2a6ae92fd7", "ref_doc_id": "60534e99-9827-4f3b-9823-84d6aaab600d"}, "50c4a20a-f71b-4772-930b-7ff05adb5450": {"doc_hash": "4069b41ee5496589eb2979980513ffafd77ee6d43b57f4808d438ef485c4ee62", "ref_doc_id": "60534e99-9827-4f3b-9823-84d6aaab600d"}, "0c9e59be-ec21-4eac-b871-36c291565153": {"doc_hash": "1fe57b73672a0ea701869f585f6cf9f8ae89112c5a35ddcc143095f26d8e6d53", "ref_doc_id": "60534e99-9827-4f3b-9823-84d6aaab600d"}, "e55d7182-4c9f-4466-b8aa-af0b01c9ca7d": {"doc_hash": "b720e81dda1e3587cf11e6d32101a0ffc1678a11fe02db1310a7f97a984179b0", "ref_doc_id": "60534e99-9827-4f3b-9823-84d6aaab600d"}, "e04d0610-0d39-48fd-a88a-ab2eb23cf03f": {"doc_hash": "641759034013448770bb081eee62ec6e80b34fa0039323f21600e43cf0244a74", "ref_doc_id": "60534e99-9827-4f3b-9823-84d6aaab600d"}, "ede0823c-d276-454d-bdae-217d6a5b68e5": {"doc_hash": "7f5b03f6e5948ef27a6bda66b121c2768337d4f630ea56485028256588f5cbf6", "ref_doc_id": "60534e99-9827-4f3b-9823-84d6aaab600d"}, "90a7a2df-f028-4023-a33d-b46ca7473bd6": {"doc_hash": "e6feaf43bcd429522b29850899e93fdd1f1f8b63c658396e6aabca195891780a", "ref_doc_id": "60534e99-9827-4f3b-9823-84d6aaab600d"}, "b1b6fea6-94a1-4a48-856b-3bc96fd7ffa8": {"doc_hash": "5b4ab7cc49866c494deb19e7bd40c20b80b2db8c6b80d69dfa35dc161c73d642", "ref_doc_id": "60534e99-9827-4f3b-9823-84d6aaab600d"}, "b4c218e3-4e77-42d9-88d3-789cb5d1a843": {"doc_hash": "7e213efce914e6141f039ff8a544f20918478c81580bbac3dde1d5250463cab3", "ref_doc_id": "60534e99-9827-4f3b-9823-84d6aaab600d"}, "136fb64b-5597-468d-be0a-67296d92d1ce": {"doc_hash": "22f5a6c99214da5aac9d2bf7441d80425d77fea53b36a10ab4a7af724f87e770", "ref_doc_id": "60534e99-9827-4f3b-9823-84d6aaab600d"}, "ca23bbc0-f0b4-4662-9f00-07cb35a6fca0": {"doc_hash": "bede03a8e13b77bcc7059000fcb0f24e675140d2dc2ab30ee94ff9b6980b8087", "ref_doc_id": "4dc5e6ce-203f-481b-9cc5-8ed7ea0ee624"}, "55b0b76e-94f8-4fb6-a951-0e4ae2da4530": {"doc_hash": "8a2217dcb14e82871e8709efb9cdb64bf3b825ab9df84169bd07ee8080905c36", "ref_doc_id": "4dc5e6ce-203f-481b-9cc5-8ed7ea0ee624"}, "5f43d60b-e4f3-4270-ac71-274b0e43925f": {"doc_hash": "67893cf171586522c2b2e011dbc0b56b62bd6b8c55b35bcc598a8b75d40dacc1", "ref_doc_id": "4dc5e6ce-203f-481b-9cc5-8ed7ea0ee624"}, "61664e49-76ac-415f-a515-a427d5e69e5b": {"doc_hash": "ca59a54ee793fa7ac18c7a5fa1ab6602b660ac920dcaaea5f8216fae77743807", "ref_doc_id": "4dc5e6ce-203f-481b-9cc5-8ed7ea0ee624"}, "efebbb85-ec13-4515-9c24-78d3590af7ac": {"doc_hash": "d369b17c449c0a36941263d1d0c40656e5543f9a155b57739c0ebdf629b7c42a", "ref_doc_id": "4dc5e6ce-203f-481b-9cc5-8ed7ea0ee624"}, "82f74e8a-c8c2-4ab9-b558-f3ea8c8dd10c": {"doc_hash": "56c530e461f94e99b51424737c489b6dc7449bd06acd68b37494021a970bede0", "ref_doc_id": "4dc5e6ce-203f-481b-9cc5-8ed7ea0ee624"}, "01b25f4b-c875-474d-8f30-c0239c28dd84": {"doc_hash": "c3a151cd4db0272b186dea9c833202fc47d5fea17f9ea4188596951471bb1eb1", "ref_doc_id": "4dc5e6ce-203f-481b-9cc5-8ed7ea0ee624"}, "09444cf9-5cf7-43c1-916c-ed03555a5041": {"doc_hash": "ac1a376da03a81025c65ad653730b11baa6e169e7b2678e2a44ca81171316a01", "ref_doc_id": "b054b4cf-4c7d-4fcc-8ed7-e2599a3213e1"}, "ca504482-fe85-4ae5-af0d-016753d626e2": {"doc_hash": "fb31edb9402754c973cb729939b0aaeb3899f16f84eabc14646f88390470b1ba", "ref_doc_id": "b054b4cf-4c7d-4fcc-8ed7-e2599a3213e1"}, "216330e4-a90e-4a22-b0fa-e98462b7a588": {"doc_hash": "d653bda647f000739d71c91c365d85b0afd26d128e7024e72778cd99eb99d069", "ref_doc_id": "b054b4cf-4c7d-4fcc-8ed7-e2599a3213e1"}, "d62b6bf6-a9fa-47f4-8e30-ebcef27b9bd7": {"doc_hash": "10abba2b15fe5e56abfbb225bed9a4c9aad8699ec00551391f70647998669388", "ref_doc_id": "b054b4cf-4c7d-4fcc-8ed7-e2599a3213e1"}, "8bf0243a-df0a-411a-800d-6293f8cfca58": {"doc_hash": "ca0f5354cd859b03264c7d67128e787542fb4a7b19dff15505d2612c0317c9a2", "ref_doc_id": "b054b4cf-4c7d-4fcc-8ed7-e2599a3213e1"}, "b4c87216-d2e8-4abe-926f-65b7f4043ebf": {"doc_hash": "2ebd6e77a7431186f812b701efcfb1a52bb378e0a9b2518ae6681916c715ec58", "ref_doc_id": "b054b4cf-4c7d-4fcc-8ed7-e2599a3213e1"}, "c4ebf6a3-5e9b-44b0-9da1-20f7d0aa4cc2": {"doc_hash": "6cda9d5d43d427d33eb0faa7acc5f18cf5462e98600a8f061541fa04f3a667b8", "ref_doc_id": "b054b4cf-4c7d-4fcc-8ed7-e2599a3213e1"}, "48402b8e-ac2f-4ce6-ab19-7669795e5c7a": {"doc_hash": "2daac1ae27b47c9d7dd129f9ed7e2af9c219a2b214688981d76e9aa8bf2e4d97", "ref_doc_id": "b054b4cf-4c7d-4fcc-8ed7-e2599a3213e1"}, "0b2ff66a-7653-421e-aafd-afa1920966c3": {"doc_hash": "089104cd74a9efb8fa548290e1e4fbb79726bf1667245882e77b931041cae6d7", "ref_doc_id": "b054b4cf-4c7d-4fcc-8ed7-e2599a3213e1"}, "8cb18571-79e9-4976-b199-cafc5bd7697f": {"doc_hash": "3af6a37c0f7c636d23712036477c51afe0de2e8cb8fe7f8eaf437e8890b2d0f4", "ref_doc_id": "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a"}, "78774d3a-d138-4c85-8718-fcf6d36289ac": {"doc_hash": "ee94609241c17c53f21b48a14b112fe22e2cb82fc41a29764985dca4fe5cd431", "ref_doc_id": "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a"}, "6af489ec-5950-45b6-a1fd-a5d538289ec9": {"doc_hash": "7498c6b78c5951cd6f8107e7e1c713b2e9107c5f4dc977b7de44c1f2959ea646", "ref_doc_id": "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a"}, "01215cdd-59d5-4833-b636-be288b1a908c": {"doc_hash": "75cf585727719eb1c78f83e502105052cafe6e886e08a9277d5a5236e3a84077", "ref_doc_id": "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a"}, "48dbf910-759d-4211-a5da-dd84f5253ea9": {"doc_hash": "3cff0cfbd6f20e751697984f4dc637a647eaf9ffde726c094833c15d8b317d22", "ref_doc_id": "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a"}, "48c8e6c5-d64c-402c-bd84-f97d641d5e2e": {"doc_hash": "c9db70c7c84765b03402c65bde497c76393555e446bcdd899d133c094599a535", "ref_doc_id": "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a"}, "e53571d2-5585-4ad1-aca5-adcbafb53b44": {"doc_hash": "46f091c688eaa77aac4ace0a8480e2444315347a7490f2f970e194a077e3898d", "ref_doc_id": "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a"}, "f197ee4d-20a1-45d1-8984-446d552d3e19": {"doc_hash": "d4441018058d56ac018753bb5b0da37cc2b2eb9afd751ed249477b60a881d049", "ref_doc_id": "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a"}, "c41e041e-ede6-483c-97a7-ae54740694bb": {"doc_hash": "745635bce5fab3ad2654739831a615d4cdaf428abaf1b4651c53edd90849ab5b", "ref_doc_id": "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a"}, "b8ddd884-9b64-437e-a4b4-ef65397bbddc": {"doc_hash": "a2c89d782800569634224b2c808ced00a2926199f2a0069bcfdb6b0b9b33bf1f", "ref_doc_id": "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a"}, "909b333f-56e4-4b4b-89d0-b198b7cc82ea": {"doc_hash": "437c675bade54afab96b1ed2889580e4462e6229cb54763c40a3157dd4e0e77b", "ref_doc_id": "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a"}}, "docstore/data": {"fd150fb1-388d-4359-8477-79a451983254": {"__data__": {"id_": "fd150fb1-388d-4359-8477-79a451983254", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7", "node_type": null, "metadata": {}, "hash": "98bc69d52388092482a99203beaac1a88167c190080d8fd540441e27a8f2ca12"}, "3": {"node_id": "72ceec9f-a67d-405d-a90d-5cf289ad9bb8", "node_type": null, "metadata": {}, "hash": "01fecaa489aac31c53addf7c2fd75e5e0145ffcc489ca4aa1d34a573a6f29fa3"}}, "hash": "ef056758abae0174684a86693de79f337799a25c95d3c96a20118d606bb350cd", "text": "ColBERT: Efficient and Effective Passage Search viaContextualized Late Interaction over BERTOmar KhattabStanford Universityokhattab@stanford.eduMatei ZahariaStanford Universitymatei@cs.stanford.eduABSTRACTRecent progress in Natural Language Understanding (NLU) is driv-ing fast-paced advances in Information Retrieval (IR), largely owedto fine-tuning deep language models (LMs) for document ranking.While remarkably effective, the ranking models based on these LMsincrease computational cost by orders of magnitude over prior ap-proaches, particularly as they must feed each query\u2013document pairthrough a massive neural network to compute a single relevancescore. To tackle this, we present ColBERT, a novel ranking modelthat adapts deep LMs (in particular, BERT) for efficient retrieval.ColBERT introduces a late interaction architecture that indepen-dently encodes the query and the document using BERT and thenemploys a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granularinteraction, ColBERT can leverage the expressiveness of deep LMswhile simultaneously gaining the ability to pre-compute documentrepresentations offline, considerably speeding up query processing.Crucially, ColBERT\u2019s pruning-friendly interaction mechanism en-ables leveraging vector-similarity indexes for end-to-end retrievaldirectly from millions of documents. We extensively evaluate Col-BERT using two recent passage search datasets. Results show thatColBERT\u2019s effectiveness is competitive with existing BERT-basedmodels (and outperforms every non-BERT baseline), while exe-cuting two orders-of-magnitude faster and requiring up to fourorders-of-magnitude fewer FLOPs per query.KEYWORDSNeural IR; Efficiency; Deep Language Models; BERTACM Reference Format:Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Pas-sage Search via Contextualized Late Interaction over BERT. In Proceedings ofthe 43rd International ACM SIGIR Conference on Research and Development inInformation Retrieval (SIGIR \u201920), July 25\u201330, 2020, Virtual Event, China. ACM,New York, NY, USA, 10 pages. https://doi.org/10.1145/3397271.34010751 INTRODUCTIONOver the past few years, the Information Retrieval (IR) communityhas witnessed the introduction of a host of neural ranking models,including DRMM [ 7], KNRM [ 4,36], and Duet [ 20,22]. In contrastPermission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.SIGIR \u201920, July 25\u201330, 2020, Virtual Event, China\u00a92020 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-8016-4/20/07. . . $15.00https://doi.org/10.1145/3397271.34010750.15 0.20 0.25 0.30 0.35 0.40MRR@10101102103104105Query Latency (ms)BM25doc2queryKNRMDuetDeepCTfT+ConvKNRMdocTTTTTqueryBERT-baseBERT-largeColBERT (re-rank)ColBERT (full retrieval)Bag-of-Words (BoW) ModelBoW Model with NLU AugmentationNeural Matching ModelDeep Language ModelColBERT (ours)Figure 1: Effectiveness (MRR@10) versus Mean Query La-tency (log-scale) for a number of representative rankingmodels on MS MARCO Ranking [24]. The figure", "start_char_idx": 0, "end_char_idx": 3622, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "72ceec9f-a67d-405d-a90d-5cf289ad9bb8": {"__data__": {"id_": "72ceec9f-a67d-405d-a90d-5cf289ad9bb8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7", "node_type": null, "metadata": {}, "hash": "98bc69d52388092482a99203beaac1a88167c190080d8fd540441e27a8f2ca12"}, "2": {"node_id": "fd150fb1-388d-4359-8477-79a451983254", "node_type": null, "metadata": {}, "hash": "ef056758abae0174684a86693de79f337799a25c95d3c96a20118d606bb350cd"}, "3": {"node_id": "c3774bb9-5d96-4b9e-8614-fbb5361baa07", "node_type": null, "metadata": {}, "hash": "4007654a099505db5d4fa233d32b5df053c8a9104742bacd58a12b9663cd7347"}}, "hash": "01fecaa489aac31c53addf7c2fd75e5e0145ffcc489ca4aa1d34a573a6f29fa3", "text": "for a number of representative rankingmodels on MS MARCO Ranking [24]. The figure also showsColBERT. Neural re-rankers run on top of the official BM25top-1000 results and use a Tesla V100 GPU. Methodology anddetailed results are in \u00a74.to prior learning-to-rank methods that rely on hand-crafted fea-tures, these models employ embedding-based representations ofqueries and documents and directly model local interactions (i.e.,fine-granular relationships) between their contents. Among them,a recent approach has emerged that fine-tunes deep pre-trainedlanguage models (LMs) like ELMo [ 29] and BERT [ 5] for estimatingrelevance. By computing deeply-contextualized semantic repre-sentations of query\u2013document pairs, these LMs help bridge thepervasive vocabulary mismatch [ 21,42] between documents andqueries [ 30]. Indeed, in the span of just a few months, a numberof ranking models based on BERT have achieved state-of-the-artresults on various retrieval benchmarks [ 3,18,25,39] and havebeen proprietarily adapted for deployment by Google1and Bing2.However, the remarkable gains delivered by these LMs comeat a steep increase in computational cost. Hofst\u00e4tter et al. [9] andMacAvaney et al. [18] observe that BERT-based models in the lit-erature are 100-1000 \u00d7more computationally expensive than priormodels\u2014some of which are arguably notinexpensive to begin with[13]. This quality\u2013cost tradeoff is summarized by Figure 1, whichcompares two BERT-based rankers [ 25,27] against a representativeset of ranking models. The figure uses MS MARCO Ranking [ 24],a recent collection of 9M passages and 1M queries from Bing\u2019slogs. It reports retrieval effectiveness (MRR@10) on the officialvalidation set as well as average query latency (log-scale) using ahigh-end server that dedicates one Tesla V100 GPU per query forneural re-rankers. Following the re-ranking setup of MS MARCO,ColBERT (re-rank), the Neural Matching Models, and the Deep LMsre-rank MS MARCO\u2019s official top-1000 documents per query. Other1https://blog.google/products/search/search-language-understanding-bert/2https://azure.microsoft.com/en-us/blog/bing-delivers-its-largest-improvement-in-search-experience-using-azure-gpus/Session 1A: NeuIR and Semantic Matching  SIGIR \u201920, July 25\u201330, 2020, Virtual Event, China39Query DocumentMaxSim\u2211MaxSim MaxSimsQueryCNN  /  Match KernelsCNN  /  Match Kernels / MLPMLPsDocument(c) All-to-all Interaction(e.g., BERT)(b) Query-Document Interaction(e.g., DRMM, KNRM, Conv-KNRM)(d) Late Interaction(i.e., the proposed ColBERT)(a) Representation-based Similarity(e.g., DSSM, SNRM)Query DocumentsQuery DocumentsFigure 2: Schematic diagrams illustrating query\u2013document matching paradigms in neural IR. The figure contrasts existingapproaches (sub-figures (a), (b), and (c)) with the proposed late interaction paradigm (sub-figure (d)).methods, including ColBERT (full retrieval), directly retrieve thetop-1000 results from the entire collection.As the figure shows, BERT considerably improves search preci-sion, raising MRR@10 by almost 7% against the best previous meth-ods; simultaneously, it increases latency by up to tens of thousandsof milliseconds even with a high-end GPU. This poses a challengingtradeoff since raising query response times by as little as 100ms isknown to impact user experience and even measurably diminishrevenue [ 17]. To tackle this problem, recent work has started explor-ing using Natural Language Understanding (NLU) techniques toaugment traditional retrieval models like BM25 [ 32]. For example,Nogueira et al. [26,28] expand", "start_char_idx": 3554, "end_char_idx": 7110, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c3774bb9-5d96-4b9e-8614-fbb5361baa07": {"__data__": {"id_": "c3774bb9-5d96-4b9e-8614-fbb5361baa07", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7", "node_type": null, "metadata": {}, "hash": "98bc69d52388092482a99203beaac1a88167c190080d8fd540441e27a8f2ca12"}, "2": {"node_id": "72ceec9f-a67d-405d-a90d-5cf289ad9bb8", "node_type": null, "metadata": {}, "hash": "01fecaa489aac31c53addf7c2fd75e5e0145ffcc489ca4aa1d34a573a6f29fa3"}, "3": {"node_id": "631617a8-4364-422b-bb85-da005f29f8c2", "node_type": null, "metadata": {}, "hash": "0f11330d9fbfb4d45c90bf2b35fee9ed28f1d25b9880c3e0ccf734b719096421"}}, "hash": "4007654a099505db5d4fa233d32b5df053c8a9104742bacd58a12b9663cd7347", "text": "[ 32]. For example,Nogueira et al. [26,28] expand documents with NLU-generatedqueries before indexing with BM25 scores and Dai & Callan [ 2] re-place BM25\u2019s term frequency with NLU-estimated term importance.Despite successfully reducing latency, these approaches generallyreduce precision substantially relative to BERT.To reconcile efficiency and contextualization in IR, we proposeColBERT , a ranking model based on contextualized late interac-tion over BERT . As the name suggests, ColBERT proposes a novellate interaction paradigm for estimating relevance between a query\ud835\udc5eand a document \ud835\udc51. Under late interaction, \ud835\udc5eand\ud835\udc51are separatelyencoded into two sets of contextual embeddings, and relevance isevaluated using cheap and pruning-friendly computations betweenboth sets\u2014that is, fast computations that enable ranking withoutexhaustively evaluating every possible candidate.Figure 2 contrasts our proposed late interaction approach withexisting neural matching paradigms. On the left, Figure 2 (a) illus-trates representation-focused rankers, which independently computean embedding for \ud835\udc5eand another for \ud835\udc51and estimate relevance asa single similarity score between two vectors [ 12,41]. Moving tothe right, Figure 2 (b) visualizes typical interaction-focused rankers.Instead of summarizing \ud835\udc5eand\ud835\udc51into individual embeddings, theserankers model word- and phrase-level relationships across \ud835\udc5eand\ud835\udc51and match them using a deep neural network (e.g., with CNNs/MLPs[22] or kernels [ 36]). In the simplest case, they feed the neural net-work an interaction matrix that reflects the similiarity betweenevery pair of words across \ud835\udc5eand\ud835\udc51. Further right, Figure 2 (c) illus-trates a more powerful interaction-based paradigm, which modelsthe interactions between words within as well as across\ud835\udc5eand\ud835\udc51atthe same time, as in BERT\u2019s transformer architecture [25].These increasingly expressive architectures are in tension. Whileinteraction-based models (i.e., Figure 2 (b) and (c)) tend to be su-perior for IR tasks [ 8,21], a representation-focused model\u2014by iso-lating the computations among \ud835\udc5eand\ud835\udc51\u2014makes it possible to pre-compute document representations offline [ 41], greatly reducingthe computational load per query. In this work, we observe thatthe fine-grained matching in interaction-based models and the pre-computation in representation-based models can be combined byretaining yet judiciously delaying the query\u2013document interaction.Figure 2 (d) illustrates an architecture that precisely does so. Asillustrated, every query embedding interacts with all documentembeddings via a MaxSim operator, which computes maximumsimilarity (e.g., cosine), and the scalar outputs of these operatorsare summed across query terms. This paradigm allows ColBERT toexploit deep LM-based representations while shifting the cost ofencoding documents offline and amortizing the cost of encodingthe query once across all ranked documents. Crucially, it enablesColBERT to leverage vector-similarity search indexes (e.g., [ 1,15])to retrieve the top- \ud835\udc58results directly from a large document collec-tion. This ability substantially improves recall over existing models,which only re-rank the output of term-based retrieval.As Figure 1 illustrates, ColBERT can serve queries in tens or, forend-to-end retrieval from millions of documents, few hundreds ofmilliseconds. For instance, when used for re-ranking as in \u201cColBERT(re-rank)\u201d, it delivers over 170 \u00d7speedup (and requires 14,000 \u00d7fewerFLOPs) relative to existing BERT-based models [ 25,27], while be-ing more effective than every non-BERT baseline (\u00a74.2 & 4.3). Col-BERT\u2019s indexing\u2014the only time it needs", "start_char_idx": 7138, "end_char_idx": 10763, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "631617a8-4364-422b-bb85-da005f29f8c2": {"__data__": {"id_": "631617a8-4364-422b-bb85-da005f29f8c2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7", "node_type": null, "metadata": {}, "hash": "98bc69d52388092482a99203beaac1a88167c190080d8fd540441e27a8f2ca12"}, "2": {"node_id": "c3774bb9-5d96-4b9e-8614-fbb5361baa07", "node_type": null, "metadata": {}, "hash": "4007654a099505db5d4fa233d32b5df053c8a9104742bacd58a12b9663cd7347"}, "3": {"node_id": "6c6a9365-d48a-44e5-b3ad-a1b4a3a49a62", "node_type": null, "metadata": {}, "hash": "cc0f70e85690fd3a3cd5b14cec67c3653aaaf8902e2d990759f1faccaa0c3dbe"}}, "hash": "0f11330d9fbfb4d45c90bf2b35fee9ed28f1d25b9880c3e0ccf734b719096421", "text": "& 4.3). Col-BERT\u2019s indexing\u2014the only time it needs to feed documents throughBERT\u2014is also practical: it can index the MS MARCO collection of9M passages in about 3 hours using a single server with four GPUs(\u00a74.5), retaining its effectiveness with a space footprint of as littleas few tens of GiBs. Our ablation study (\u00a74.4) shows that late in-teraction, its implementation via MaxSim operations, and crucialdesign choices within our BERT-based encoders are all essential toColBERT\u2019s effectiveness.Our main contributions are as follows.(1)We propose late interaction (\u00a73.1) as a paradigm for efficientand effective neural ranking.(2)We present ColBERT (\u00a73.2 & 3.3), a highly-effective modelthat employs novel BERT-based query and document en-coders within the late interaction paradigm.Session 1A: NeuIR and Semantic Matching  SIGIR \u201920, July 25\u201330, 2020, Virtual Event, China40(3)We show how to leverage ColBERT both for re-ranking ontop of a term-based retrieval model (\u00a73.5) and for searchinga full collection using vector similarity indexes (\u00a73.6).(4)We evaluate ColBERT on MS MARCO and TREC CAR, tworecent passage search collections.We release our reference implementation as open source.32 RELATED WORKNeural Matching Models. Over the past few years, IR researchershave introduced numerous neural architectures for ranking. Inthis work, we compare against KNRM [ 4,36], Duet [ 20,22], Con-vKNRM [ 4], and fastText+ConvKNRM [ 10]. KNRM proposes a dif-ferentiable kernel-pooling technique for extracting matching sig-nals from an interaction matrix, while Duet combines signals fromexact-match-based as well as embedding-based similarities for rank-ing. Introduced in 2018, ConvKNRM learns to match \ud835\udc5b-grams in thequery and the document. Lastly, fastText+ConvKNRM (abbreviatedfT+ConvKNRM) tackles the absence of rare words from typicalword embeddings lists by adopting sub-word token embeddings.In 2018, Zamani et al. [41] introduced SNRM, a representation-focused IR model that encodes each query and each document asa single, sparse high-dimensional vector of \u201clatent terms\u201d. By pro-ducing a sparse-vector representation for each document, SNRMis able to use a traditional IR inverted index for representing docu-ments, allowing fast end-to-end retrieval. Despite highly promisingresults and insights, SNRM\u2019s effectiveness is substantially outper-formed by the state of the art on the datasets with which it wasevaluated (e.g., see [ 18,38]). While SNRM employs sparsity to al-low using inverted indexes, we relax this assumption and comparea (dense) BERT-based representation-focused model against ourlate-interaction ColBERT in our ablation experiments in \u00a74.4. For adetailed overview of existing neural ranking models, we refer thereaders to two recent surveys of the literature [8, 21].Language Model Pretraining for IR. Recent work in NLUemphasizes the importance pre-training language representationmodels in an unsupervised fashion before subsequently fine-tuningthem on downstream tasks. A notable example is BERT [ 5], a bi-directional transformer-based language model whose fine-tuningadvanced the state of the art on various NLU benchmarks. Nogueira etal.[25], MacAvaney et al. [18], and Dai & Callan [ 3] investigateincorporating such LMs (mainly BERT, but also ELMo [ 29]) on dif-ferent ranking datasets. As illustrated in Figure 2 (c), the commonapproach (and the one adopted by Nogueira et al. on MS MARCOand TREC CAR) is to feed the query\u2013document pair through BERTand use an MLP on top of BERT\u2019s [CLS]", "start_char_idx": 10763, "end_char_idx": 14283, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6c6a9365-d48a-44e5-b3ad-a1b4a3a49a62": {"__data__": {"id_": "6c6a9365-d48a-44e5-b3ad-a1b4a3a49a62", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7", "node_type": null, "metadata": {}, "hash": "98bc69d52388092482a99203beaac1a88167c190080d8fd540441e27a8f2ca12"}, "2": {"node_id": "631617a8-4364-422b-bb85-da005f29f8c2", "node_type": null, "metadata": {}, "hash": "0f11330d9fbfb4d45c90bf2b35fee9ed28f1d25b9880c3e0ccf734b719096421"}, "3": {"node_id": "84a5d28d-c149-4062-96d2-305e5209dd44", "node_type": null, "metadata": {}, "hash": "d7fb3616d3b1f98cc0e9c678e591c69519f7e3e252c637155c993985e7cd33a0"}}, "hash": "cc0f70e85690fd3a3cd5b14cec67c3653aaaf8902e2d990759f1faccaa0c3dbe", "text": "through BERTand use an MLP on top of BERT\u2019s [CLS] output token to produce arelevance score. Subsequent work by Nogueira et al. [27] introducedduoBERT, which fine-tunes BERT to compare the relevance of apair of documents given a query. Relative to their single-documentBERT, this gives duoBERT about 1% MRR@10 advantage on MSMARCO while increasing the cost by at least 1.4\u00d7.BERT Optimizations. As discussed in \u00a71, these rankers can behighly expensive in practice. Orthogonal to our approach, there areongoing efforts in the NLU literature for distilling [ 14,33], compress-ing [ 40], and pruning [ 19] BERT. Other optimizations may tradequality and speed specifically for IR (e.g., re-ranking with a smaller3https://github.com/stanford-futuredata/ColBERTQuery DocumentQuery Encoder, fQ Document Encoder, fDMaxSim MaxSim MaxSimscoreOffline IndexingFigure 3: The general architecture of ColBERT given a query\ud835\udc5eand a document \ud835\udc51.depth\ud835\udc58(\u00a74.2) or truncating longer documents). While these effortscan be instrumental in narrowing the efficiency gap, they generallyachieve much smaller speedups than our re-designed architecturefor IR, due to their generic nature, and the more aggressive onesoften come at the cost of noticeably lower quality.Efficient NLU-based Models. Recently, a direction emergedthat employs expensive NLU computation offline. This includesdoc2query [ 28] and DeepCT [ 2]. The doc2query model expandseach document with a pre-defined number of synthetic queries, gen-erated by a seq2seq transformer model trained to generate queriesgiven a document. It then relies on a BM25 index for retrieval fromthe (expanded) documents. DeepCT uses BERT to produce the termfrequency component of BM25 in a context-aware manner, essen-tially representing a feasible realization of the term-independenceassumption with neural networks [ 23]. Lastly, docTTTTTquery [ 26]is identical to doc2query except that it fine-tunes a pre-trainedmodel (namely, T5 [31]) for generating the predicted queries.Concurrently with the drafting of this paper, Hofst\u00e4tter et al. [11]published their Transformer-Kernel (TK) model. At a high level, TKimproves the KNRM architecture described earlier: while KNRMemploys kernel pooling on top of word-embedding-based inter-action, TK uses a Transformer [ 34] component for contextuallyencoding queries and documents before kernel pooling. TK estab-lishes a new state-of-the-art for non-BERT models on MS MARCO(Dev); however, the best non-ensemble MRR@10 it achieves is 31%while ColBERT reaches up to 36%. Moreover, due to indexing docu-ment representations offline and employing a MaxSim-based lateinteraction mechanism, ColBERT is much more scalable, enablingend-to-end retrieval which is not supported by TK.3 COLBERTColBERT prescribes a simple framework for balancing the qualityand cost of neural IR, particularly deep language models like BERT.As introduced earlier, delaying the query\u2013document interaction canfacilitate cheap neural re-ranking (i.e., through pre-computation)and even support practical end-to-end neural retrieval (i.e., throughpruning via vector-similarity search). ColBERT addresses how todo so while still preserving the effectiveness of state-of-the-artmodels, which condition the bulk of their computations on the jointquery\u2013document pair.Session 1A: NeuIR and Semantic Matching  SIGIR \u201920, July 25\u201330, 2020, Virtual Event, China41Even though ColBERT\u2019s late-interaction framework can be ap-plied to a wide variety of architectures (e.g., CNNs, RNNs, transform-ers, etc.), we choose to focus this work on bi-directional transformer-based encoders", "start_char_idx": 14287, "end_char_idx": 17882, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "84a5d28d-c149-4062-96d2-305e5209dd44": {"__data__": {"id_": "84a5d28d-c149-4062-96d2-305e5209dd44", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7", "node_type": null, "metadata": {}, "hash": "98bc69d52388092482a99203beaac1a88167c190080d8fd540441e27a8f2ca12"}, "2": {"node_id": "6c6a9365-d48a-44e5-b3ad-a1b4a3a49a62", "node_type": null, "metadata": {}, "hash": "cc0f70e85690fd3a3cd5b14cec67c3653aaaf8902e2d990759f1faccaa0c3dbe"}, "3": {"node_id": "e844b618-3096-4a04-9ef1-057604988838", "node_type": null, "metadata": {}, "hash": "0fedc6e0b4729f1a6e663a6a56ca9fa835ed295fe6ee9b8dc996b5ad4b734581"}}, "hash": "d7fb3616d3b1f98cc0e9c678e591c69519f7e3e252c637155c993985e7cd33a0", "text": "we choose to focus this work on bi-directional transformer-based encoders (i.e., BERT) owing to their state-of-the-art effective-ness yet very high computational cost.3.1 ArchitectureFigure 3 depicts the general architecture of ColBERT, which com-prises: (a) a query encoder \ud835\udc53\ud835\udc44, (b) a document encoder \ud835\udc53\ud835\udc37, and (c)the late interaction mechanism. Given a query \ud835\udc5eand document \ud835\udc51,\ud835\udc53\ud835\udc44encodes\ud835\udc5einto a bag of fixed-size embeddings \ud835\udc38\ud835\udc5ewhile\ud835\udc53\ud835\udc37en-codes\ud835\udc51into another bag \ud835\udc38\ud835\udc51. Crucially, each embeddings in \ud835\udc38\ud835\udc5eand\ud835\udc38\ud835\udc51iscontextualized based on the other terms in \ud835\udc5eor\ud835\udc51, respectively.We describe our BERT-based encoders in \u00a73.2.Using\ud835\udc38\ud835\udc5eand\ud835\udc38\ud835\udc51, ColBERT computes the relevance score be-tween\ud835\udc5eand\ud835\udc51via late interaction, which we define as a summationof maximum similarity (MaxSim) operators. In particular, we findthe maximum cosine similarity of each \ud835\udc63\u2208\ud835\udc38\ud835\udc5ewith vectors in \ud835\udc38\ud835\udc51,and combine the outputs via summation. Besides cosine, we alsoevaluate squared L2 distance as a measure of vector similarity. In-tuitively, this interaction mechanism softly searches for each queryterm\ud835\udc61\ud835\udc5e\u2014in a manner that reflects its context in the query\u2014againstthe document\u2019s embeddings, quantifying the strength of the \u201cmatch\u201dvia the largest similarity score between \ud835\udc61\ud835\udc5eand a document term \ud835\udc61\ud835\udc51.Given these term scores, it then estimates the document relevanceby summing the matching evidence across all query terms.While more sophisticated matching is possible with other choicessuch as deep convolution and attention layers (i.e., as in typicalinteraction-focused models), a summation of maximum similaritycomputations has two distinctive characteristics. First, it standsout as a particularly cheap interaction mechanism, as we examineits FLOPs in \u00a74.2. Second, and more importantly, it is amenableto highly-efficient pruning for top- \ud835\udc58retrieval, as we evaluate in\u00a74.3. This enables using vector-similarity algorithms for skippingdocuments without materializing the full interaction matrix or evenconsidering each document in isolation. Other cheap choices (e.g.,a summation of average similarity scores, instead of maximum) arepossible; however, many are less amenable to pruning. In \u00a74.4, weconduct an extensive ablation study that empirically verifies the ad-vantage of our MaxSim-based late interaction against alternatives.3.2 Query & Document EncodersPrior to late interaction, ColBERT encodes each query or documentinto a bag of embeddings, employing BERT-based encoders. Weshare a single BERT model among our query and document en-coders but distinguish input sequences that correspond to queriesand documents by prepending a special token [Q]to queries andanother token [D]to documents.Query Encoder. Given a textual query \ud835\udc5e, we tokenize it into itsBERT-based WordPiece [ 35] tokens\ud835\udc5e1\ud835\udc5e2...\ud835\udc5e\ud835\udc59. We prepend the token[Q]to the query. We place this token right after BERT\u2019s sequence-start token [CLS] . If the query has fewer than a pre-defined numberof tokens\ud835\udc41\ud835\udc5e, we pad it with BERT\u2019s special [mask] tokens upto length\ud835\udc41\ud835\udc5e(otherwise, we truncate it to the first \ud835\udc41\ud835\udc5etokens).This padded sequence of input tokens is then passed into BERT\u2019sdeep transformer architecture, which computes a contextualizedrepresentation of each token.We denote the padding with masked tokens as query augmen-tation ,", "start_char_idx": 17859, "end_char_idx": 21101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e844b618-3096-4a04-9ef1-057604988838": {"__data__": {"id_": "e844b618-3096-4a04-9ef1-057604988838", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7", "node_type": null, "metadata": {}, "hash": "98bc69d52388092482a99203beaac1a88167c190080d8fd540441e27a8f2ca12"}, "2": {"node_id": "84a5d28d-c149-4062-96d2-305e5209dd44", "node_type": null, "metadata": {}, "hash": "d7fb3616d3b1f98cc0e9c678e591c69519f7e3e252c637155c993985e7cd33a0"}, "3": {"node_id": "e69c32cb-fb80-4e56-9f6b-cef9238a95cf", "node_type": null, "metadata": {}, "hash": "caace1c49ff881ef236a303d2e2cbc3095950a5de6de4859358d055b40aea40e"}}, "hash": "0fedc6e0b4729f1a6e663a6a56ca9fa835ed295fe6ee9b8dc996b5ad4b734581", "text": "denote the padding with masked tokens as query augmen-tation , a step that allows BERT to produce query-based embeddingsat the positions corresponding to these masks. Query augmentationis intended to serve as a soft, differentiable mechanism for learningto expand queries with new terms or to re-weigh existing termsbased on their importance for matching the query. As we show in\u00a74.4, this operation is essential for ColBERT\u2019s effectiveness.Given BERT\u2019s representation of each token, our encoder passesthe contextualized output representations through a linear layerwith no activations. This layer serves to control the dimension ofColBERT\u2019s embeddings, producing \ud835\udc5a-dimensional embeddings forthe layer\u2019s output size \ud835\udc5a. As we discuss later, we typically fix \ud835\udc5atobe much smaller than BERT\u2019s fixed hidden dimension.While ColBERT\u2019s embedding dimension has limited impact onthe efficiency of query encoding, this step is crucial for controllingthe space footprint of documents, as we show in \u00a74.5. In addition,it can have a significant impact on query execution time, particu-larly the time taken for transferring the document representationsonto the GPU from system memory (where they reside before pro-cessing a query). In fact, as we show in \u00a74.2, gathering, stacking,and transferring the embeddings from CPU to GPU can be themost expensive step in re-ranking with ColBERT. Finally, the out-put embeddings are normalized so each has L2 norm equal to one.The result is that the dot-product of any two embeddings becomesequivalent to their cosine similarity, falling in the [\u22121,1]range.Document Encoder. Our document encoder has a very similararchitecture. We first segment a document \ud835\udc51into its constituenttokens\ud835\udc511\ud835\udc512...\ud835\udc51\ud835\udc5a, to which we prepend BERT\u2019s start token [CLS] fol-lowed by our special token [D]that indicates a document sequence.Unlike queries, we do not append [mask] tokens to documents. Af-ter passing this input sequence through BERT and the subsequentlinear layer, the document encoder filters out the embeddings corre-sponding to punctuation symbols, determined via a pre-defined list.This filtering is meant to reduce the number of embeddings per doc-ument, as we hypothesize that (even contextualized) embeddingsof punctuation are unnecessary for effectiveness.In summary, given \ud835\udc5e=\ud835\udc5e0\ud835\udc5e1...\ud835\udc5e\ud835\udc59and\ud835\udc51=\ud835\udc510\ud835\udc511...\ud835\udc51\ud835\udc5b, we computethe bags of embeddings \ud835\udc38\ud835\udc5eand\ud835\udc38\ud835\udc51in the following manner, where#refers to the [mask] tokens:\ud835\udc38\ud835\udc5e:=Normalize(CNN(BERT(\u201c[\ud835\udc44]\ud835\udc5e0\ud835\udc5e1...\ud835\udc5e\ud835\udc59##...#\u201d))) (1)\ud835\udc38\ud835\udc51:=Filter(Normalize(CNN(BERT(\u201c[\ud835\udc37]\ud835\udc510\ud835\udc511...\ud835\udc51\ud835\udc5b\u201d)))) (2)3.3 Late InteractionGiven the representation of a query \ud835\udc5eand a document \ud835\udc51, the rele-vance score of \ud835\udc51to\ud835\udc5e, denoted as\ud835\udc46\ud835\udc5e,\ud835\udc51, is estimated via late interactionbetween their bags of contextualized embeddings. As mentionedbefore, this is conducted as a sum of maximum similarity computa-tions, namely cosine similarity (implemented as dot-products dueto the embedding normalization) or squared L2 distance.\ud835\udc46\ud835\udc5e,\ud835\udc51:=\u00d5\ud835\udc56\u2208[|\ud835\udc38\ud835\udc5e|]max\ud835\udc57\u2208[|\ud835\udc38\ud835\udc51|]\ud835\udc38\ud835\udc5e\ud835\udc56\u00b7\ud835\udc38\ud835\udc47\ud835\udc51\ud835\udc57(3)Session 1A: NeuIR and Semantic Matching  SIGIR \u201920, July 25\u201330, 2020, Virtual Event, China42ColBERT is differentiable", "start_char_idx": 21113, "end_char_idx": 24184, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e69c32cb-fb80-4e56-9f6b-cef9238a95cf": {"__data__": {"id_": "e69c32cb-fb80-4e56-9f6b-cef9238a95cf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7", "node_type": null, "metadata": {}, "hash": "98bc69d52388092482a99203beaac1a88167c190080d8fd540441e27a8f2ca12"}, "2": {"node_id": "e844b618-3096-4a04-9ef1-057604988838", "node_type": null, "metadata": {}, "hash": "0fedc6e0b4729f1a6e663a6a56ca9fa835ed295fe6ee9b8dc996b5ad4b734581"}, "3": {"node_id": "87689162-96e4-4395-9f13-31c3710a19cf", "node_type": null, "metadata": {}, "hash": "d0ece0f49f38d395a542c47f4c3a704f16d32a8c910f26049da4c0846aa859c7"}}, "hash": "caace1c49ff881ef236a303d2e2cbc3095950a5de6de4859358d055b40aea40e", "text": "July 25\u201330, 2020, Virtual Event, China42ColBERT is differentiable end-to-end. We fine-tune the BERTencoders and train from scratch the additional parameters (i.e., thelinear layer and the [Q] and [D] markers\u2019 embeddings) using theAdam [ 16] optimizer. Notice that our interaction mechanism hasno trainable parameters. Given a triple \u27e8\ud835\udc5e,\ud835\udc51+,\ud835\udc51\u2212\u27e9with query \ud835\udc5e,positive document \ud835\udc51+and negative document \ud835\udc51\u2212, ColBERT is usedto produce a score for each document individually and is optimizedvia pairwise softmax cross-entropy loss over the computed scoresof\ud835\udc51+and\ud835\udc51\u2212.3.4 Offline Indexing: Computing & StoringDocument EmbeddingsBy design, ColBERT isolates almost all of the computations be-tween queries and documents to enable pre-computing documentrepresentations offline. At a high level, our indexing procedure isstraight-forward: we proceed over the documents in the collectionin batches, running our document encoder \ud835\udc53\ud835\udc37on each batch andstoring the output embeddings per document. Although indexing aset of documents is an offline process, we incorporate a few simpleoptimizations for enhancing its throughput. As we show in \u00a74.5,these can considerably reduce the offline cost of indexing.To begin with, we exploit multiple GPUs, if available, for fasterencoding of batches of documents in parallel. When batching, wepad all documents to the maximum length of a document withinthe batch.4To make capping the sequence length on a per-batchbasis effective, our indexer proceeds through documents in largegroups of\ud835\udc35(e.g.,\ud835\udc35=100,000) documents. It sorts these documentsby length and then feeds batches of \ud835\udc4f(e.g.,\ud835\udc4f=128) documents ofcomparable length through our encoder. Such length-based bucket-ing is sometimes refered to as a BucketIterator in some libraries(e.g., allenNLP). Lastly, while most computations occur on the GPU,we found that a non-trivial portion of the indexing time is spent onpre-processing the text sequences, primarily BERT\u2019s WordPiece to-kenization. Exploiting that these operations are independent acrossdocuments in a batch, we parallelize the pre-processing across theavailable CPU cores.Once the document representations are produced, they are savedto disk using 32-bit or 16-bit values to represent each dimension.As we describe in \u00a73.5 and 3.6, these representations are eithersimply loaded from disk for ranking or are subsequently indexedfor vector-similarity search, respectively.3.5 Top-\ud835\udc58Re-ranking with ColBERTRecall that ColBERT can be used for re-ranking the output of an-other retrieval model, typically a term-based model, or directlyfor end-to-end retrieval from a document collection. In this sec-tion, we discuss how we use ColBERT for ranking a small set of\ud835\udc58(e.g.,\ud835\udc58=1000) documents given a query \ud835\udc5e. Since\ud835\udc58is small, werely on batch computations to exhaustively score each document(unlike our approach in \u00a73.6). To begin with, our query serving sub-system loads the indexed documents representations into memory,representing each document as a matrix of embeddings.Given a query \ud835\udc5e, we compute its bag of contextualized embed-dings\ud835\udc38\ud835\udc5e(Equation 1) and, concurrently, gather the document repre-sentations into a 3-dimensional tensor \ud835\udc37consisting of \ud835\udc58document4The public BERT implementations we saw simply pad to a pre-defined length.matrices. We pad the \ud835\udc58documents to their maximum length tofacilitate batched operations, and move the tensor \ud835\udc37to the GPU\u2019smemory. On the GPU, we compute a batch dot-product of \ud835\udc38\ud835\udc5eand\ud835\udc37, possibly over multiple mini-batches. The output materializes a3-dimensional tensor that is a collection of cross-match", "start_char_idx": 24180, "end_char_idx": 27737, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "87689162-96e4-4395-9f13-31c3710a19cf": {"__data__": {"id_": "87689162-96e4-4395-9f13-31c3710a19cf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7", "node_type": null, "metadata": {}, "hash": "98bc69d52388092482a99203beaac1a88167c190080d8fd540441e27a8f2ca12"}, "2": {"node_id": "e69c32cb-fb80-4e56-9f6b-cef9238a95cf", "node_type": null, "metadata": {}, "hash": "caace1c49ff881ef236a303d2e2cbc3095950a5de6de4859358d055b40aea40e"}, "3": {"node_id": "8422b33b-b068-4bcf-b083-60fbb4fbe50b", "node_type": null, "metadata": {}, "hash": "af6a220f87cb3b795242ff6892c554ef4e10322cb1831c6a6f2c9033b48d279e"}}, "hash": "d0ece0f49f38d395a542c47f4c3a704f16d32a8c910f26049da4c0846aa859c7", "text": "The output materializes a3-dimensional tensor that is a collection of cross-match matricesbetween\ud835\udc5eand each document. To compute the score of each docu-ment, we reduce its matrix across document terms via a max-pool(i.e., representing an exhaustive implementation of our MaxSimcomputation) and reduce across query terms via a summation. Fi-nally, we sort the \ud835\udc58documents by their total scores.Relative to existing neural rankers (especially, but not exclusively,BERT-based ones), this computation is very cheap that, in fact, thecost of a simple implementation is dominated by the gatheringand transferring of the pre-computed embeddings. To illustrate,ranking\ud835\udc58documents via typical BERT rankers requires feedingBERT\ud835\udc58different inputs each of length \ud835\udc59=|\ud835\udc5e|+|\ud835\udc51\ud835\udc56|for query\ud835\udc5eanddocuments\ud835\udc51\ud835\udc56, where attention has quadratic cost in the length ofthe sequence. In contrast, ColBERT feeds BERT only a single, muchshorter sequence of length \ud835\udc59=|\ud835\udc5e|. Consequently, ColBERT is notonly cheaper, it also scales much better with \ud835\udc58(\u00a74.2).3.6 End-to-end Top- \ud835\udc58Retrieval with ColBERTAs mentioned before, ColBERT\u2019s late-interaction operator is specifi-cally designed to enable end-to-end retrieval from a large collection,largely to improve recall relative to term-based retrieval approaches.This section is concerned with cases where the number of docu-ments to be ranked is too large for exhaustive evaluation of eachpossible candidate document, particularly when we are only in-terested in the highest scoring ones. Concretely, we focus here onretrieving the top- \ud835\udc58results directly from a large document collec-tion with\ud835\udc41(e.g.,\ud835\udc41=10,000,000) documents, where \ud835\udc58\u226a\ud835\udc41.To do so, we leverage the pruning-friendly nature of the MaxSimoperations at the backbone of late interaction. Instead of applyingMaxSim between one of the query embeddings and all of one docu-ment\u2019s embeddings, we can use fast vector-similarity data structuresto efficiently conduct this search between the query embeddingandalldocument embeddings across the full collection. For this,we employ an off-the-shelf library for large-scale vector-similaritysearch, namely faiss [15] from Facebook.5In particular, at theend of offline indexing (\u00a73.4), we maintain a mapping from eachembedding to its document of origin and then index all documentembeddings into faiss.Subsequently, when serving queries, we use a two-stage pro-cedure to retrieve the top- \ud835\udc58documents from the entire collection.Both stages rely on ColBERT\u2019s scoring: the first is an approximatestage aimed at filtering while the second is a refinement stage. Forthe first stage, we concurrently issue \ud835\udc41\ud835\udc5evector-similarity queries(corresponding to each of the embeddings in \ud835\udc38\ud835\udc5e) onto our faiss in-dex. This retrieves the top- \ud835\udc58\u2032(e.g.,\ud835\udc58\u2032=\ud835\udc58/2) matches for that vectorover all document embeddings. We map each of those to its docu-ment of origin, producing \ud835\udc41\ud835\udc5e\u00d7\ud835\udc58\u2032document IDs, only \ud835\udc3e\u2264\ud835\udc41\ud835\udc5e\u00d7\ud835\udc58\u2032of which are unique. These \ud835\udc3edocuments likely contain one or moreembeddings that are highly similar to the query embeddings. Forthe second stage, we refine this set by exhaustively re-ranking onlythose\ud835\udc3edocuments in the usual manner described in \u00a73.5.5https://github.com/facebookresearch/faissSession 1A: NeuIR and Semantic Matching  SIGIR \u201920, July 25\u201330, 2020, Virtual Event, China43In our faiss -based implementation, we use an IVFPQ index(\u201cinverted file with product quantization\u201d). This index partitionsthe embedding space into", "start_char_idx": 27725, "end_char_idx": 31137, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8422b33b-b068-4bcf-b083-60fbb4fbe50b": {"__data__": {"id_": "8422b33b-b068-4bcf-b083-60fbb4fbe50b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7", "node_type": null, "metadata": {}, "hash": "98bc69d52388092482a99203beaac1a88167c190080d8fd540441e27a8f2ca12"}, "2": {"node_id": "87689162-96e4-4395-9f13-31c3710a19cf", "node_type": null, "metadata": {}, "hash": "d0ece0f49f38d395a542c47f4c3a704f16d32a8c910f26049da4c0846aa859c7"}, "3": {"node_id": "d1ff20f6-ccc9-45fa-a72a-60171d2b0c92", "node_type": null, "metadata": {}, "hash": "08c18bed00b183e3ac693989f4f9ba5d80c825ba164e3647a6d9d7c04b490fe2"}}, "hash": "af6a220f87cb3b795242ff6892c554ef4e10322cb1831c6a6f2c9033b48d279e", "text": "file with product quantization\u201d). This index partitionsthe embedding space into \ud835\udc43(e.g.,\ud835\udc43=1000) cells based on \ud835\udc58-meansclustering and then assigns each document embedding to its nearestcell based on the selected vector-similarity metric. For servingqueries, when searching for the top- \ud835\udc58\u2032matches for a single queryembedding, only the nearest \ud835\udc5d(e.g.,\ud835\udc5d=10) partitions are searched.To improve memory efficiency, every embedding is divided into \ud835\udc60(e.g.,\ud835\udc60=16) sub-vectors, each represented using one byte. Moreover,the index conducts the similarity computations in this compresseddomain, leading to cheaper computations and thus faster search.4 EXPERIMENTAL EVALUATIONWe now turn our attention to empirically testing ColBERT, address-ing the following research questions.RQ1: In a typical re-ranking setup, how well can ColBERT bridgethe existing gap (highlighted in \u00a71) between highly-efficient andhighly-effective neural models? (\u00a74.2)RQ2: Beyond re-ranking, can ColBERT effectively support end-to-end retrieval directly from a large collection? (\u00a74.3)RQ3: What does each component of ColBERT (e.g., late interac-tion, query augmentation) contribute to its quality? (\u00a74.4)RQ4: What are ColBERT\u2019s indexing-related costs in terms ofoffline computation and memory overhead? (\u00a74.5)4.1 Methodology4.1.1 Datasets & Metrics. Similar to related work [ 2,27,28], weconduct our experiments on the MS MARCO Ranking [ 24] (hence-forth, MS MARCO) and TREC Complex Answer Retrieval (TREC-CAR) [ 6] datasets. Both of these recent datasets provide large train-ing data of the scale that facilitates training and evaluating deepneural networks. We describe both in detail below.MS MARCO. MS MARCO is a dataset (and a correspondingcompetition) introduced by Microsoft in 2016 for reading compre-hension and adapted in 2018 for retrieval. It is a collection of 8.8Mpassages from Web pages, which were gathered from Bing\u2019s re-sults to 1M real-world queries. Each query is associated with sparserelevance judgements of one (or very few) documents marked asrelevant and no documents explicitly indicated as irrelevant. Perthe official evaluation, we use MRR@10 to measure effectiveness.We use three query sets in our evaluation. The official develop-ment and evaluation sets contain roughly 7k queries. The relevancejudgements of the evaluation set are held-out by Microsoft andMRR@10 results can only be obtained by submitting to the com-petition\u2019s organizers. We submitted our main re-ranking ColBERTmodel for \u00a74.2. In addition, the collection includes roughly 55kqueries (with labels) that are provided as additional validation data.We re-purpose a random sample of 5k queries among those (i.e.,ones not in our development or training sets) as a \u201clocal\u201d evaluationset. Along with the official development set, we use this held-outset for testing our models as well as baselines in \u00a74.3. We do so toavoid submitting multiple variants of the same model at once, asthe organizers discourage too many submissions by the same team.TREC CAR. Introduced by Dietz [ 6]et al. in 2017, TREC CAR isa synthetic dataset based on Wikipedia that consists of about 29Mpassages. Similar to related work [ 25], we dedicate the first fourof five pre-defined folds for training (and the fifth for validation),which amounts to roughly 3M queries generated by concatenatingthe title of a Wikipedia page with the heading of one of its sections.That section\u2019s passages are marked as relevant to the correspondingquery. Our evaluation is conducted on the test set used in TREC2017 CAR, which contains 2,254 queries.4.1.2 Implementation. Our ColBERT models are implemented", "start_char_idx": 31139, "end_char_idx": 34760, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d1ff20f6-ccc9-45fa-a72a-60171d2b0c92": {"__data__": {"id_": "d1ff20f6-ccc9-45fa-a72a-60171d2b0c92", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7", "node_type": null, "metadata": {}, "hash": "98bc69d52388092482a99203beaac1a88167c190080d8fd540441e27a8f2ca12"}, "2": {"node_id": "8422b33b-b068-4bcf-b083-60fbb4fbe50b", "node_type": null, "metadata": {}, "hash": "af6a220f87cb3b795242ff6892c554ef4e10322cb1831c6a6f2c9033b48d279e"}, "3": {"node_id": "c155ef56-7eca-4a18-a396-2f7eb428d7c4", "node_type": null, "metadata": {}, "hash": "f156f78b67d7d7fa5c6b8aa16f596afe5bb7744652e45ac7dd75e20494f5d970"}}, "hash": "08c18bed00b183e3ac693989f4f9ba5d80c825ba164e3647a6d9d7c04b490fe2", "text": "queries.4.1.2 Implementation. Our ColBERT models are implemented us-ing Python 3 and PyTorch 1. We use the popular transformers6library for pre-trained BERT. Similar to [ 25], we fine-tune all Col-BERT models with learning rate 3\u00d710\u22126with a batch size 32. We fixthe number of embeddings per query at \ud835\udc41\ud835\udc5e=32. Unless otherwisestated, we set our ColBERT embedding dimension \ud835\udc5ato 128; \u00a74.5demonstrates ColBERT\u2019s robustness to a wide range of dimensions.For MS MARCO, we initialize the BERT components of the Col-BERT query and document encoders using Google\u2019s official pre-trained BERT basemodel and train all models for 200k iterations.For TREC CAR, we follow related work [ 2,25] and use a differentpre-trained model to the official ones. To explain, the official BERTmodels were pre-trained on Wikipedia, which is the source of TRECCAR\u2019s training and test sets. To avoid leaking test data into train,Nogueira and Cho [ 25] pre-train a BERT model on the Wiki pagescorresponding to training subset of TREC CAR. They release theirBERT large pre-trained model, which we fine-tune for ColBERT\u2019s ex-periments on TREC CAR. As BERT large embeddings are larger, weset\ud835\udc5ato 200, and since fine-tuning this model is significantly slowerthan BERT base, we train on TREC CAR for only 125k iterations.In our re-ranking results, unless otherwise stated, we use 4 bytesper dimension in our embeddings and employ cosine as our vector-similarity function. For end-to-end ranking, we use (squared) L2distance, as we found our faiss index was faster at L2-based re-trieval. For our faiss index, we set the number of partitions to\ud835\udc43=2,000, and search the nearest \ud835\udc5d=10to each query embedding toretrieve\ud835\udc58\u2032=\ud835\udc58=1000 document vectors per query embedding. Wedivide each embedding into \ud835\udc60=16sub-vectors, each encoded usingone byte. To represent the index used for the second stage of ourend-to-end retrieval procedure, we use 16-bit values per dimension.4.1.3 Hardware & Time Measurements. To evaluate the latency ofneural re-ranking models in \u00a74.2, we use a single Tesla V100 GPUthat has 32 GiBs of memory on a server with two Intel Xeon Gold6132 CPUs, each with 14 physical cores (24 hyperthreads), and 469GiBs of RAM. For the mostly CPU-based retrieval experiments in\u00a74.3 and the indexing experiments in \u00a74.5, we use another serverwith the same CPU and system memory specifications but whichhas four Titan V GPUs attached, each with 12 GiBs of memory.Across all experiments, only one GPU is dedicated per query forretrieval (i.e., for methods with neural computations) but we useup to all four GPUs during indexing.4.2 Quality\u2013Cost Tradeoff: Top- \ud835\udc58Re-rankingIn this section, we examine ColBERT\u2019s efficiency and effectivenessat re-ranking the top- \ud835\udc58results extracted by a bag-of-words retrievalmodel, which is the most typical setting for testing and deployingneural ranking models. We begin with the MS MARCO dataset. We6https://github.com/huggingface/transformersSession 1A: NeuIR and Semantic Matching  SIGIR \u201920, July 25\u201330, 2020, Virtual Event, China44Method MRR@10 (Dev) MRR@10 (Eval) Re-ranking Latency (ms) FLOPs/queryBM25 (official) 16.7 16.5 - -KNRM 19.8 19.8 3 592M (0.085\u00d7)Duet 24.3 24.5 22 159B (23\u00d7)fastText+ConvKNRM 29.0 27.7 28 78B (11\u00d7)BERT base[25] 34.7 - 10,700 97T (13,900\u00d7)BERT base(our training) 36.0 - 10,700 97T", "start_char_idx": 34773, "end_char_idx": 38074, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c155ef56-7eca-4a18-a396-2f7eb428d7c4": {"__data__": {"id_": "c155ef56-7eca-4a18-a396-2f7eb428d7c4", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7", "node_type": null, "metadata": {}, "hash": "98bc69d52388092482a99203beaac1a88167c190080d8fd540441e27a8f2ca12"}, "2": {"node_id": "d1ff20f6-ccc9-45fa-a72a-60171d2b0c92", "node_type": null, "metadata": {}, "hash": "08c18bed00b183e3ac693989f4f9ba5d80c825ba164e3647a6d9d7c04b490fe2"}, "3": {"node_id": "e7c512d8-cda4-4438-a1f3-36ecedbf8f73", "node_type": null, "metadata": {}, "hash": "5ce6382c7bbdb68f83bad4492669980b13090337e870ad97e9ec6873a65eb3e5"}}, "hash": "f156f78b67d7d7fa5c6b8aa16f596afe5bb7744652e45ac7dd75e20494f5d970", "text": "base(our training) 36.0 - 10,700 97T (13,900\u00d7)BERT large [25] 36.5 35.9 32,900 340T (48,600\u00d7)ColBERT (over BERT base) 34.9 34.9 61 7B (1\u00d7)Table 1: \u201cRe-ranking\u201d results on MS MARCO. Each neural model re-ranks the official top-1000 results produced by BM25.Latency is reported for re-ranking only. To obtain the end-to-end latency in Figure 1, we add the BM25 latency from Table 2.Method MRR@10 (Dev) MRR@10 (Local Eval) Latency (ms) Recall@50 Recall@200 Recall@1000BM25 (official) 16.7 - - - - 81.4BM25 (Anserini) 18.7 19.5 62 59.2 73.8 85.7doc2query 21.5 22.8 85 64.4 77.9 89.1DeepCT 24.3 - 62(est.) 69 [2] 82 [2] 91 [2]docTTTTTquery 27.7 28.4 87 75.6 86.9 94.7ColBERT L2(re-rank) 34.8 36.4 - 75.3 80.5 81.4ColBERT L2(end-to-end) 36.0 36.7 458 82.9 92.3 96.8Table 2: End-to-end retrieval results on MS MARCO. Each model retrieves the top-1000 documents per query directly from theentire 8.8M document collection.compare against KNRM, Duet, and fastText+ConvKNRM, a repre-sentative set of neural matching models that have been previouslytested on MS MARCO. In addition, we compare against the adapta-tion of BERT for ranking by Nogueira and Cho [ 25], in particular,their BERT baseand its deeper counterpart BERT large.7We report the competition\u2019s official metric, namely MRR@10, onthe validation set (Dev) and the evaluation set (Eval). We also reportthe re-ranking latency, which we measure using a single Tesla V100GPU, and the FLOPs per query for each neural ranking model. To doso, we adapt the baselines\u2019 publicly-available reference implemen-tations into our pytorch testbed. For ColBERT, our reported latencysubsumes the entire computation from gathering the documentrepresentations, moving them to the GPU, tokenizing then encod-ing the query, and applying late interaction to compute documentscores. For the baselines, we measure the scoring computations onthe GPU and exclude the CPU-based text preprocessing (similarto [9]). In principle, the baselines can pre-compute most of thispreprocessing (e.g., document tokenization) offline. We estimatethe FLOPs using the torchprofile8library.We now proceed to study the results, which are reported in Ta-ble 1. To begin with, we notice the fast progress from KNRM in2017 to the BERT-based models in 2019, manifesting itself in over16% increase in MRR@10. As described in \u00a71, the simultaneousincrease in computational cost is difficult to miss. Judging by theirrather monotonic pattern of increasingly larger cost and higher ef-fectiveness, these results appear to paint a picture where expensivemodels are necessary for high-quality ranking.In contrast with this trend, ColBERT (which employs late in-teraction over BERT base) performs competitively with the original7https://github.com/nyu-dl/dl4marco-bert/8https://github.com/mit-han-lab/torchprofileadaptation of BERT baseand BERT large for ranking by Nogueiraand Cho [ 25,27]. Interestingly, ColBERT appears no worse thanBERT basein MRR@10\u2014although the latter uses a different loss func-tion to ColBERT\u2019s (\u00a73.3). To confirm the intuition that ColBERT\u2019slate interaction does trade away some of BERT\u2019s quality, the tablealso reports results of \u201cBERT base(our training)\u201d, which is based onNogueira and Cho\u2019s model of the same size but is optimized", "start_char_idx": 38102, "end_char_idx": 41355, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e7c512d8-cda4-4438-a1f3-36ecedbf8f73": {"__data__": {"id_": "e7c512d8-cda4-4438-a1f3-36ecedbf8f73", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7", "node_type": null, "metadata": {}, "hash": "98bc69d52388092482a99203beaac1a88167c190080d8fd540441e27a8f2ca12"}, "2": {"node_id": "c155ef56-7eca-4a18-a396-2f7eb428d7c4", "node_type": null, "metadata": {}, "hash": "f156f78b67d7d7fa5c6b8aa16f596afe5bb7744652e45ac7dd75e20494f5d970"}, "3": {"node_id": "dc6779c2-4747-4628-bbdc-7af4ecf9449a", "node_type": null, "metadata": {}, "hash": "79975665745f987da5d9bdc9945154879b0ed270a6198e9e7f15181d3852ef0b"}}, "hash": "5ce6382c7bbdb68f83bad4492669980b13090337e870ad97e9ec6873a65eb3e5", "text": "is based onNogueira and Cho\u2019s model of the same size but is optimized withpairwise softmax cross-entropy loss. We train it with learning rate3\u00d710\u22126and batch size 16 for 200k iterations. Unlike the originalBERT baseranker, results show that this model does in fact have anedge over ColBERT\u2019s effectiveness.While highly competitive in retrieval quality, ColBERT is ordersof magnitude cheaper than BERT base, in particular, by over 170 \u00d7inlatency and 13,900\u00d7in FLOPs. This highlights the expressivenessof our proposed late interaction mechanism when coupled with apowerful pre-trained LM like BERT. While ColBERT\u2019s re-rankinglatency is slightly higher than the non-BERT models shown (i.e., by10s of milliseconds), this difference is explained by the time it takesa simple Python implementation to gather, stack, and transfer thedocument embeddings to the GPU. In particular, the query encodingand interaction in ColBERT consume only 13 milliseconds of itstotal execution time.Diving deeper into the quality\u2013cost tradeoff between BERT andColBERT, Figure 4 demonstrates the relationships between FLOPsand effectiveness (MRR@10) as a function of the re-ranking depth\ud835\udc58when re-ranking the top- \ud835\udc58results by BM25, comparing ColBERTand BERT base(our training). We conduct this experiment on MSMARCO (Dev). We note here that as the official top-1000 rankingdoes not provide the BM25 order (and also lacks documents beyondthe top-1000 per query), the models in this experiment re-rank theSession 1A: NeuIR and Semantic Matching  SIGIR \u201920, July 25\u201330, 2020, Virtual Event, China450.27 0.29 0.31 0.33 0.35 0.37MRR@10103104105106107108109Million FLOPs (log-scale)k=10205010020050010002000k=10 20 50 10020050010002000BERTbase (our training)ColBERTFigure 4: FLOPs (in millions) and MRR@10 as functionsof the re-ranking depth \ud835\udc58. Since the official BM25 rankingis not ordered, the initial top- \ud835\udc58retrieval is conducted withAnserini\u2019s BM25.Anserini [ 37] toolkit\u2019s BM25 output. Consequently, both MRR@10values at\ud835\udc58=1000 are slightly higher from those reported in Table 1.Studying the results in Figure 4, we notice that not only is Col-BERT much cheaper than BERT for the same model size (i.e., 12-layer \u201cbase\u201d transformer encoder), it also scales better with thenumber of ranked documents. In part, this is because ColBERTonly needs to process the query once, irrespective of the number ofdocuments evaluated. For instance, at \ud835\udc58=10, BERT requires nearly180\u00d7more FLOPs than ColBERT; at \ud835\udc58=1000, BERT\u2019s overheadjumps to 13,900\u00d7. It then reaches 23,000\u00d7 at\ud835\udc58=2000.We observe that this orders-of-magnitude reduction in FLOPsmakes it practical to run ColBERT entirely on the CPU. In fact, sub-sequent informal experimentation suggests that ColBERT\u2019s latencyand FLOPs can be considerably reduced further by a number ofoptimizations, some entailing a controllable quality tradeoff. Theseinclude using smaller vector dimensions (whose MRR@10 is testedin \u00a74.5), padding queries to shorter \ud835\udc41\ud835\udc5e, processing documents ina lengths-aware fashion, and distilling/quantizing the encoder(s)(\u00a72), the final two of which are also applicable to the baseline BERTreference implementation by Nogueira and Cho [ 25]. Addition-ally, caching the document embeddings on the GPU(s)\u2014if sufficientGPU memory exists\u2014can significantly reduce ColBERT\u2019s latency.Lastly, batch-processing of multiple queries can enhance ColBERT\u2019sthroughput by improving the GPU utilization of query encoding.We leave exploring these opportunities for future work.Method MAP MRR@10BM25", "start_char_idx": 41330, "end_char_idx": 44839, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dc6779c2-4747-4628-bbdc-7af4ecf9449a": {"__data__": {"id_": "dc6779c2-4747-4628-bbdc-7af4ecf9449a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7", "node_type": null, "metadata": {}, "hash": "98bc69d52388092482a99203beaac1a88167c190080d8fd540441e27a8f2ca12"}, "2": {"node_id": "e7c512d8-cda4-4438-a1f3-36ecedbf8f73", "node_type": null, "metadata": {}, "hash": "5ce6382c7bbdb68f83bad4492669980b13090337e870ad97e9ec6873a65eb3e5"}, "3": {"node_id": "0b70dbb3-7efd-49d8-84a7-813edd0196e6", "node_type": null, "metadata": {}, "hash": "09d4f01eda5ff864e2238868e51c8e25ebe13a0f9380ae23138a62539b6ab4ce"}}, "hash": "79975665745f987da5d9bdc9945154879b0ed270a6198e9e7f15181d3852ef0b", "text": "leave exploring these opportunities for future work.Method MAP MRR@10BM25 (Anserini) 15.3 -doc2query 18.1 -DeepCT 24.6 33.2BM25 + BERT base 31.0 -BM25 + BERT large 33.5 -BM25 + ColBERT 31.3 44.2Table 3: Results on TREC CAR.Having studied our results on MS MARCO, we now considerTREC CAR, whose official metric is MAP. Similar to Table 1, wealso report MRR@10. The results are summarized in Table 3, whichincludes a number of important baselines (BM25, doc2query, andDeepCT) in addition to re-ranking baselines that have previouslybeen tested on this dataset. As the table shows, the results mirrorthose seen with MS MARCO.4.3 End-to-end Top- \ud835\udc58RetrievalBeyond cheap re-ranking, ColBERT is amenable to top- \ud835\udc58retrieval di-rectly from a full collection. Table 2 considers full retrieval, whereineach model retrieves the top-1000 documents directly from MSMARCO\u2019s 8.8M documents per query. In addition to MRR@10 andlatency in milliseconds, the table reports Recall@50, Recall@200,and Recall@1000, important metrics for a full-retrieval model thatessentially filters down a large collection on a per-query basis.We compare against BM25, in particular MS MARCO\u2019s officialBM25 ranking as well as a well-tuned baseline based on the Anserinitoolkit.9While many other traditional models exist, we are notaware of any that substantially outperform Anserini\u2019s BM25 im-plementation (e.g., see RM3 in [ 28], LMDir in [ 2], or Microsoft\u2019sproprietary feature-based RankSVM on the leaderboard).We also compare against doc2query, DeepCT, and docTTTTT-query. All three rely on a traditional bag-of-words model (primarilyBM25) for retrieval. Crucially, however, they re-weigh the frequencyof terms per document and/or expand the set of terms in each doc-ument before building the BM25 index. In particular, doc2queryexpands each document with a pre-defined number of syntheticqueries generated by a seq2seq transformer model (which docTTT-Tquery replaced with a pre-trained language model, T5 [ 31]). Incontrast, DeepCT uses BERT to produce the term frequency com-ponent of BM25 in a context-aware manner.For the latency of Anserini\u2019s BM25, doc2query, and docTTTT-query, we use the authors\u2019 [ 26,28] Anserini-based implementation.While this implementation supports multi-threading, it only utilizesparallelism across different queries. We thus report single-threadedlatency for these models, noting that simply parallelizing their com-putation over shards of the index can substantially decrease theiralready-low latency. For DeepCT, we only estimate its latency us-ing that of BM25 (as denoted by (est.) in the table), since DeepCTre-weighs BM25\u2019s term frequency without modifying the indexotherwise.10As discussed in \u00a74.1, we use ColBERT L2for end-to-end retrieval, which employs negative squared L2 distance as itsvector-similarity function. For its latency, we measure the time forfaiss -based candidate filtering and the subsequent re-ranking. Inthis experiment, faiss uses all available CPU cores.Looking at Table 2, we first see Anserini\u2019s BM25 baseline at 18.7MRR@10, noticing its very low latency as implemented in Anserini(which extends the well-known Lucene system), owing to bothvery cheap operations and decades of bag-of-words top- \ud835\udc58retrievaloptimizations. The three subsequent baselines, namely doc2query,DeepCT, and docTTTTquery, each brings a decisive enhancementto effectiveness. These improvements come at negligible overheadsin latency, since these baselines ultimately rely on BM25-basedretrieval. The most effective among these three,", "start_char_idx": 44832, "end_char_idx": 48367, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0b70dbb3-7efd-49d8-84a7-813edd0196e6": {"__data__": {"id_": "0b70dbb3-7efd-49d8-84a7-813edd0196e6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7", "node_type": null, "metadata": {}, "hash": "98bc69d52388092482a99203beaac1a88167c190080d8fd540441e27a8f2ca12"}, "2": {"node_id": "dc6779c2-4747-4628-bbdc-7af4ecf9449a", "node_type": null, "metadata": {}, "hash": "79975665745f987da5d9bdc9945154879b0ed270a6198e9e7f15181d3852ef0b"}, "3": {"node_id": "a8f228fa-d17f-4ef3-8175-48fe13ee2ad6", "node_type": null, "metadata": {}, "hash": "873795c9004666454606039f2b97467cda316519c4f9c6fc070a64427ac17add"}}, "hash": "09d4f01eda5ff864e2238868e51c8e25ebe13a0f9380ae23138a62539b6ab4ce", "text": "ultimately rely on BM25-basedretrieval. The most effective among these three, docTTTTquery,demonstrates a massive 9% gain over vanilla BM25 by fine-tuningthe recent language model T5.9http://anserini.io/10In practice, a myriad of reasons could still cause DeepCT\u2019s latency to differslightly from BM25\u2019s. For instance, the top- \ud835\udc58pruning strategy employed, if any, couldinteract differently with a changed distribution of scores.Session 1A: NeuIR and Semantic Matching  SIGIR \u201920, July 25\u201330, 2020, Virtual Event, China46Shifting our attention to ColBERT\u2019s end-to-end retrieval effec-tiveness, we see its major gains in MRR@10 over all of these end-to-end models. In fact, using ColBERT in the end-to-end setup is su-perior in terms of MRR@10 to re-ranking with the same model dueto the improved recall. Moving beyond MRR@10, we also see largegains in Recall@ \ud835\udc58for\ud835\udc58equals to 50, 200, and 1000. For instance, itsRecall@50 actually exceeds the official BM25\u2019s Recall@1000 andeven all but docTTTTTquery\u2019s Recall@200, emphasizing the valueof end-to-end retrieval (instead of just re-ranking) with ColBERT.4.4 Ablation Studies0.220.240.260.280.300.320.340.36MRR@10BERT [CLS]-based dot-product (5-layer)  [A]ColBERT via average similarity (5-layer)  [B]ColBERT without query augmentation (5-layer)  [C]ColBERT (5-layer)  [D]ColBERT (12-layer)  [E]ColBERT + e2e retrieval (12-layer)  [F]Figure 5: Ablation results on MS MARCO (Dev). Betweenbrackets is the number of BERT layers used in each model.The results from \u00a74.2 indicate that ColBERT is highly effectivedespite the low cost and simplicity of its late interaction mechanism.To better understand the source of this effectiveness, we examine anumber of important details in ColBERT\u2019s interaction and encoderarchitecture. For this ablation, we report MRR@10 on the validationset of MS MARCO in Figure 5, which shows our main re-rankingColBERT model [E], with MRR@10 of 34.9%.Due to the cost of training all models, we train a copy of ourmain model that retains only the first 5 layers of BERT out of 12(i.e., model [D]) and similarly train all our ablation models for200k iterations with five BERT layers. To begin with, we ask if thefine-granular interaction in late interaction is necessary. Model [A]tackles this question: it uses BERT to produce a single embeddingvector for the query and another for the document, extracted fromBERT\u2019s [CLS] contextualized embedding and expanded through alinear layer to dimension 4096 (which equals \ud835\udc41\ud835\udc5e\u00d7128=32\u00d7128).Relevance is estimated as the inner product of the query\u2019s and thedocument\u2019s embeddings, which we found to perform better thancosine similarity for single-vector re-ranking. As the results show,this model is considerably less effective than ColBERT, reinforcingthe importance of late interaction.Subsequently, we ask if our MaxSim-based late interaction is bet-ter than other simple alternatives. We test a model [B] that replacesColBERT\u2019s maximum similarity with average similarity. The resultssuggest the importance of individual terms in the query payingspecial attention to particular terms in the document. Similarly,the figure emphasizes the importance of our query augmentationmechanism: without query augmentation [C], ColBERT has a no-ticeably lower MRR@10. Lastly, we see the impact of end-to-endretrieval not only on recall but also on MRR@10. By retrievingdirectly from the full collection, ColBERT is able to retrieve to thetop-10", "start_char_idx": 48365, "end_char_idx": 51805, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a8f228fa-d17f-4ef3-8175-48fe13ee2ad6": {"__data__": {"id_": "a8f228fa-d17f-4ef3-8175-48fe13ee2ad6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7", "node_type": null, "metadata": {}, "hash": "98bc69d52388092482a99203beaac1a88167c190080d8fd540441e27a8f2ca12"}, "2": {"node_id": "0b70dbb3-7efd-49d8-84a7-813edd0196e6", "node_type": null, "metadata": {}, "hash": "09d4f01eda5ff864e2238868e51c8e25ebe13a0f9380ae23138a62539b6ab4ce"}, "3": {"node_id": "bfdcb818-99f4-4a0c-b76a-eefb141e73be", "node_type": null, "metadata": {}, "hash": "7518e88f7a867fb5d365686bb5c832a845de273a0c0780af4c257c95439c7969"}}, "hash": "873795c9004666454606039f2b97467cda316519c4f9c6fc070a64427ac17add", "text": "from the full collection, ColBERT is able to retrieve to thetop-10 documents missed entirely from BM25\u2019s top-1000.4.5 Indexing Throughput & FootprintLastly, we examine the indexing throughput and space footprintof ColBERT. Figure 6 reports indexing throughput on MS MARCO0 10000 20000 30000 40000 50000Throughput (documents/minute)Basic ColBERT Indexing+multi-GPU document processing+per-batch maximum sequence length+length-based bucketing+multi-core pre-processingFigure 6: Effect of ColBERT\u2019s indexing optimizations on theoffline indexing throughput.documents with ColBERT and four other ablation settings, whichindividually enable optimizations described in \u00a73.4 on top of basicbatched indexing. Based on these throughputs, ColBERT can indexMS MARCO in about three hours. Note that any BERT-based modelmust incur the computational cost of processing each documentat least once. While ColBERT encodes each document with BERTexactly once, existing BERT-based rankers would repeat similarcomputations on possibly hundreds of documents for each query.Setting Dimension( \ud835\udc5a) Bytes/Dim Space(GiBs) MRR@10Re-rank Cosine 128 4 286 34.9End-to-end L2 128 2 154 36.0Re-rank L2 128 2 143 34.8Re-rank Cosine 48 4 54 34.4Re-rank Cosine 24 2 27 33.9Table 4: Space Footprint vs MRR@10 (Dev) on MS MARCO.Table 4 reports the space footprint of ColBERT under varioussettings as we reduce the embeddings dimension and/or the bytesper dimension. Interestingly, the most space-efficient setting, thatis, re-ranking with cosine similarity with 24-dimensional vectorsstored as 2-byte floats, is only 1% worse in MRR@10 than the mostspace-consuming one, while the former requires only 27 GiBs torepresent the MS MARCO collection.5 CONCLUSIONSIn this paper, we introduced ColBERT, a novel ranking model thatemploys contextualized late interaction over deep LMs (in particular,BERT) for efficient retrieval. By independently encoding queriesand documents into fine-grained representations that interact viacheap and pruning-friendly computations, ColBERT can leveragethe expressiveness of deep LMs while greatly speeding up queryprocessing. Crucially, doing so allows scaling ColBERT to end-to-end neural retrieval directly from a large document collection,which can greatly improve recall over existing models. Our resultsshow that ColBERT is two orders-of-magnitude faster than existingBERT-based models, all while only minimally impacting re-rankingquality and while outperforming every non-BERT baseline.Acknowledgments. OK was supported by the Eltoukhy FamilyGraduate Fellowship at the Stanford School of Engineering. Thisresearch was supported in part by affiliate members and othersupporters of the Stanford DAWN project\u2014Ant Financial, Facebook,Google, Infosys, NEC, and VMware\u2014as well as Cisco, SAP, and theNSF under CAREER grant CNS-1651570. Any opinions, findings,and conclusions or recommendations expressed in this material arethose of the authors and do not necessarily reflect the views of theNational Science Foundation.Session 1A: NeuIR and Semantic Matching  SIGIR \u201920, July 25\u201330, 2020, Virtual Event, China47REFERENCES[1]Firas Abuzaid, Geet Sethi, Peter Bailis, and Matei Zaharia. 2019. To Index or Notto Index: Optimizing Exact Maximum Inner Product Search. In 2019 IEEE 35thInternational Conference on Data Engineering (ICDE). IEEE, 1250\u20131261.[2]Zhuyun Dai and Jamie Callan. 2019. Context-Aware Sentence/Passage TermImportance Estimation For First Stage Retrieval. arXiv preprint arXiv:1910.10687(2019).[3]Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR withContextual Neural Language Modeling. arXiv preprint arXiv:1905.09217 (2019).[4]Zhuyun Dai, Chenyan Xiong, Jamie", "start_char_idx": 51818, "end_char_idx": 55504, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bfdcb818-99f4-4a0c-b76a-eefb141e73be": {"__data__": {"id_": "bfdcb818-99f4-4a0c-b76a-eefb141e73be", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7", "node_type": null, "metadata": {}, "hash": "98bc69d52388092482a99203beaac1a88167c190080d8fd540441e27a8f2ca12"}, "2": {"node_id": "a8f228fa-d17f-4ef3-8175-48fe13ee2ad6", "node_type": null, "metadata": {}, "hash": "873795c9004666454606039f2b97467cda316519c4f9c6fc070a64427ac17add"}, "3": {"node_id": "be3c7305-6e99-4db0-ad3f-d729adf163e1", "node_type": null, "metadata": {}, "hash": "5121d6f99b30fbedd11f0b6e260b980d30e9a944afe4d36661b7a0ccf5144bff"}}, "hash": "7518e88f7a867fb5d365686bb5c832a845de273a0c0780af4c257c95439c7969", "text": "(2019).[4]Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutionalneural networks for soft-matching n-grams in ad-hoc search. In Proceedings of theeleventh ACM international conference on web search and data mining. 126\u2013134.[5]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:Pre-training of deep bidirectional transformers for language understanding. arXivpreprint arXiv:1810.04805 (2018).[6]Laura Dietz, Manisha Verma, Filip Radlinski, and Nick Craswell. 2017. TRECComplex Answer Retrieval Overview.. In TREC.[7]Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. 2016. A deep relevancematching model for ad-hoc retrieval. In Proceedings of the 25th ACM Internationalon Conference on Information and Knowledge Management. ACM, 55\u201364.[8]Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani, ChenWu, W Bruce Croft, and Xueqi Cheng. 2019. A deep look into neural rankingmodels for information retrieval. arXiv preprint arXiv:1903.06902 (2019).[9]Sebastian Hofst\u00e4tter and Allan Hanbury. 2019. Let\u2019s measure run time! Extendingthe IR replicability infrastructure to include performance aspects. arXiv preprintarXiv:1907.04614 (2019).[10] Sebastian Hofst\u00e4tter, Navid Rekabsaz, Carsten Eickhoff, and Allan Hanbury. 2019.On the effect of low-frequency terms on neural-IR models. In Proceedings ofthe 42nd International ACM SIGIR Conference on Research and Development inInformation Retrieval. 1137\u20131140.[11] Sebastian Hofst\u00e4tter, Markus Zlabinger, and Allan Hanbury. 2019. TU Wien@TREC Deep Learning\u201919\u2013Simple Contextualization for Re-ranking. arXiv preprintarXiv:1912.01385 (2019).[12] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and LarryHeck. 2013. Learning deep structured semantic models for web search usingclickthrough data. In Proceedings of the 22nd ACM international conference onInformation & Knowledge Management. 2333\u20132338.[13] Shiyu Ji, Jinjin Shao, and Tao Yang. 2019. Efficient Interaction-based NeuralRanking with Locality Sensitive Hashing. In The World Wide Web Conference.ACM, 2858\u20132864.[14] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,and Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding.arXiv preprint arXiv:1909.10351 (2019).[15] Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2017. Billion-scale similaritysearch with GPUs. arXiv preprint arXiv:1702.08734 (2017).[16] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-mization. arXiv preprint arXiv:1412.6980 (2014).[17] Ron Kohavi, Alex Deng, Brian Frasca, Toby Walker, Ya Xu, and Nils Pohlmann.2013. Online controlled experiments at large scale. In SIGKDD.[18] Sean MacAvaney, Andrew Yates, Arman Cohan, and Nazli Goharian. 2019. Cedr:Contextualized embeddings for document ranking. In Proceedings of the 42ndInternational ACM SIGIR Conference on Research and Development in InformationRetrieval. ACM, 1101\u20131104.[19] Paul Michel, Omer Levy, and Graham Neubig. 2019. Are Sixteen Heads ReallyBetter than One?. In Advances in Neural Information Processing Systems", "start_char_idx": 55523, "end_char_idx": 58631, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "be3c7305-6e99-4db0-ad3f-d729adf163e1": {"__data__": {"id_": "be3c7305-6e99-4db0-ad3f-d729adf163e1", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7", "node_type": null, "metadata": {}, "hash": "98bc69d52388092482a99203beaac1a88167c190080d8fd540441e27a8f2ca12"}, "2": {"node_id": "bfdcb818-99f4-4a0c-b76a-eefb141e73be", "node_type": null, "metadata": {}, "hash": "7518e88f7a867fb5d365686bb5c832a845de273a0c0780af4c257c95439c7969"}, "3": {"node_id": "619ba4a9-6d0e-40d1-8e7a-5e5b15686c33", "node_type": null, "metadata": {}, "hash": "aa166a1dae58469b884c47c85f54471aa9cf815e3b21d30c4b20e7c60bc4fa83"}}, "hash": "5121d6f99b30fbedd11f0b6e260b980d30e9a944afe4d36661b7a0ccf5144bff", "text": "Heads ReallyBetter than One?. In Advances in Neural Information Processing Systems . 14014\u201314024.[20] Bhaskar Mitra and Nick Craswell. 2019. An Updated Duet Model for PassageRe-ranking. arXiv preprint arXiv:1903.07666 (2019).[21] Bhaskar Mitra, Nick Craswell, et al .2018. An introduction to neural informationretrieval. Foundations and Trends\u00ae in Information Retrieval 13, 1 (2018), 1\u2013126.[22] Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to match usinglocal and distributed representations of text for web search. In Proceedings of the26th International Conference on World Wide Web. International World Wide WebConferences Steering Committee, 1291\u20131299.[23] Bhaskar Mitra, Corby Rosset, David Hawking, Nick Craswell, Fernando Diaz,and Emine Yilmaz. 2019. Incorporating query term independence assumptionfor efficient retrieval and ranking using deep neural networks. arXiv preprintarXiv:1907.03693 (2019).[24] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, RanganMajumder, and Li Deng. 2016. MS MARCO: A Human-Generated MAchineReading COmprehension Dataset. (2016).[25] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.arXiv preprint arXiv:1901.04085 (2019).[26] Rodrigo Nogueira, Jimmy Lin, and AI Epistemic. 2019. From doc2query todocTTTTTquery. (2019).[27] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-StageDocument Ranking with BERT. arXiv preprint arXiv:1910.14424 (2019).[28] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. DocumentExpansion by Query Prediction. arXiv preprint arXiv:1904.08375 (2019).[29] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, ChristopherClark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized wordrepresentations. arXiv preprint arXiv:1802.05365 (2018).[30] Yifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019. Understandingthe Behaviors of BERT in Ranking. arXiv preprint arXiv:1904.07531 (2019).[31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the lim-its of transfer learning with a unified text-to-text transformer. arXiv preprintarXiv:1910.10683 (2019).[32] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu,Mike Gatford, et al. 1995. Okapi at TREC-3. NIST Special Publication (1995).[33] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin.2019. Distilling task-specific knowledge from BERT into simple neural networks.arXiv preprint arXiv:1903.12136 (2019).[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is allyou need. In Advances in neural information processing systems. 5998\u20136008.[35] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi,Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al .2016. Google\u2019s neural machine translation system: Bridging the gap betweenhuman and machine translation. arXiv preprint", "start_char_idx": 58598, "end_char_idx": 61675, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "619ba4a9-6d0e-40d1-8e7a-5e5b15686c33": {"__data__": {"id_": "619ba4a9-6d0e-40d1-8e7a-5e5b15686c33", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe7ef5a9-c427-43b4-946f-d6f93765dde7", "node_type": null, "metadata": {}, "hash": "98bc69d52388092482a99203beaac1a88167c190080d8fd540441e27a8f2ca12"}, "2": {"node_id": "be3c7305-6e99-4db0-ad3f-d729adf163e1", "node_type": null, "metadata": {}, "hash": "5121d6f99b30fbedd11f0b6e260b980d30e9a944afe4d36661b7a0ccf5144bff"}}, "hash": "aa166a1dae58469b884c47c85f54471aa9cf815e3b21d30c4b20e7c60bc4fa83", "text": "machine translation system: Bridging the gap betweenhuman and machine translation. arXiv preprint arXiv:1609.08144 (2016).[36] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017.End-to-end neural ad-hoc ranking with kernel pooling. In Proceedings of the 40thInternational ACM SIGIR conference on research and development in informationretrieval. 55\u201364.[37] Peilin Yang, Hui Fang, and Jimmy Lin. 2018. Anserini: Reproducible rankingbaselines using Lucene. Journal of Data and Information Quality (JDIQ) 10, 4(2018), 1\u201320.[38] Wei Yang, Kuang Lu, Peilin Yang, and Jimmy Lin. 2019. Critically Examiningthe\" Neural Hype\" Weak Baselines and the Additivity of Effectiveness Gainsfrom Neural Ranking Models. In Proceedings of the 42nd International ACM SIGIRConference on Research and Development in Information Retrieval. 1129\u20131132.[39] Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang, and Jimmy Lin. 2019.Cross-domain modeling of sentence-level evidence for document retrieval. InProceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-cessing and the 9th International Joint Conference on Natural Language Processing(EMNLP-IJCNLP). 3481\u20133487.[40] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8bert:Quantized 8bit bert. arXiv preprint arXiv:1910.06188 (2019).[41] Hamed Zamani, Mostafa Dehghani, W Bruce Croft, Erik Learned-Miller, andJaap Kamps. 2018. From neural re-ranking to neural ranking: Learning a sparserepresentation for inverted indexing. In Proceedings of the 27th ACM InternationalConference on Information and Knowledge Management. ACM, 497\u2013506.[42] Le Zhao. 2012. Modeling and solving term mismatch for full-text retrieval. Ph.D.Dissertation. Carnegie Mellon University.Session 1A: NeuIR and Semantic Matching  SIGIR \u201920, July 25\u201330, 2020, Virtual Event, China48", "start_char_idx": 61650, "end_char_idx": 63499, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c227d33d-2e43-4176-b557-b913384f103a": {"__data__": {"id_": "c227d33d-2e43-4176-b557-b913384f103a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60534e99-9827-4f3b-9823-84d6aaab600d", "node_type": null, "metadata": {}, "hash": "0974e6e159ce3a5df25b1c1c3a97c3530eb4178721760528f7c38a6a0ae14b0a"}, "3": {"node_id": "b119e89a-46de-4864-ab90-faae17968b6d", "node_type": null, "metadata": {}, "hash": "266c80ced2cd8c4f64a12d2cea103501db59697c4a163f1e563c57ca382000f6"}}, "hash": "2adf4cb466e18792feda56b84292784b464849d18d628757a850a07f1f0efd36", "text": "COIL: Revisit Exact Lexical Match in Information Retrievalwith Contextualized Inverted ListLuyu Gao, Zhuyun Dai, Jamie CallanLanguage Technologies InstituteCarnegie Mellon University{luyug, zhuyund, callan}@cs.cmu.eduAbstractClassical information retrieval systems such asBM25 rely on exact lexical match and carryout search ef\ufb01ciently with inverted list index.Recent neural IR models shifts towards softsemantic matching all query document terms,but they lose the computation ef\ufb01ciency ofexact match systems. This paper presentsCOIL, a contextualized exact match retrievalarchitecture that brings semantic lexical match-ing. COIL scoring is based on overlappingquery document tokens\u2019 contextualized repre-sentations. The new architecture stores con-textualized token representations in invertedlists, bringing together the ef\ufb01ciency of exactmatch and the representation power of deeplanguage models. Our experimental resultsshow COIL outperforms classical lexical re-trievers and state-of-the-art deep LM retrieverswith similar or smaller latency.11 IntroductionWidely used, bag-of-words (BOW) information re-trieval (IR) systems such as BM25 rely on exactlexical match2between query and document terms.Recent study in neural IR takes a different approachand compute soft matching between all query anddocument terms to model complex matching.The shift to soft matching in neural IR modelsattempts to address vocabulary mismatch problems,that query and the relevant documents use differ-ent terms, e.g. cat v.s. kitty, for the same con-cept (Huang et al., 2013; Guo et al., 2016; Xionget al., 2017). Later introduction of contextualizedrepresentations (Peters et al., 2018) from deep lan-guage models (LM) further address semantic mis-match , that the same term can refer to differentconcepts, e.g., bank of river vs. bank in \ufb01nance.Fine-tuned deep LM rerankers produce token rep-resentations based on context and achieve state-of-1Our code is available at https://github.com/luyug/COIL .2Exact match up to morphological changes.the-art in text ranking with huge performance leap(Nogueira and Cho, 2019; Dai and Callan, 2019b).Though the idea of soft matching all tokens iscarried through the development of neural IR mod-els, seeing the success brought by deep LMs, wetake a step back and ask: how much gain can we getif we introduce contextualized representations backto lexical exact match systems? In other words, canwe build a system that still performs exact query-document token matching but compute matchingsignals with contextualized token representationsinstead of heuristics? This may seem a constrainton the model, but exact lexical match produce moreexplainable and controlled patterns than soft match-ing. It also allows search to focus on only thesubset of documents that have overlapping termswith query, which can be done ef\ufb01ciently with in-verted list index. Meanwhile, using dense contex-tualized token representations enables the modelto handle semantic mismatch, which has been along-standing problem in classic lexical systems.To answer the question, we propose a new lexi-cal matching scheme that uses vector similaritiesbetween query-document overlapping term contex-tualized representations to replace heuristic scor-ing used in classical systems. We present COn-textualized Inverted List (COIL), a new exact lex-ical match retrieval architecture armed with deepLM representations. COIL processes documentswith deep LM of\ufb02ine and produces representationsfor each document token. The representations aregrouped by their surface tokens into inverted lists.At search time, we build representation vectorsfor query tokens and perform contextualized ex-act match: use each query token to look up itsown inverted list and compute vector similaritywith document vectors stored in the inverted listas matching scores. COIL enables ef\ufb01cient searchwith rich-in-semantic matching", "start_char_idx": 0, "end_char_idx": 3894, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b119e89a-46de-4864-ab90-faae17968b6d": {"__data__": {"id_": "b119e89a-46de-4864-ab90-faae17968b6d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60534e99-9827-4f3b-9823-84d6aaab600d", "node_type": null, "metadata": {}, "hash": "0974e6e159ce3a5df25b1c1c3a97c3530eb4178721760528f7c38a6a0ae14b0a"}, "2": {"node_id": "c227d33d-2e43-4176-b557-b913384f103a", "node_type": null, "metadata": {}, "hash": "2adf4cb466e18792feda56b84292784b464849d18d628757a850a07f1f0efd36"}, "3": {"node_id": "e9501756-36af-4385-a6cc-cdce40361954", "node_type": null, "metadata": {}, "hash": "475fa89082f8c1ffef2366ec93b1fb921c668c425b87dd36d2c8a58b4a899f86"}}, "hash": "266c80ced2cd8c4f64a12d2cea103501db59697c4a163f1e563c57ca382000f6", "text": "COIL enables ef\ufb01cient searchwith rich-in-semantic matching between query anddocument.Our contributions include 1) introduce a novelarXiv:2104.07186v1  [cs.IR]  15 Apr 2021retrieval architecture, contextualized invertedlists (COIL) that brings semantic matching intolexical IR systems, 2) show matching signals in-duced from exact lexical match can capture com-plicated matching patterns, 3) demonstrate COILsigni\ufb01cantly outperform classical and deep LMaugmented lexical retrievers as well as state-of-the-art dense retrievers on two retrieval tasks.2 Related WorkLexical Retriever Classical IR systems rely onexact lexical match retrievers such as BooleanRetrieval, BM25 (Robertson and Walker, 1994)and statistical language models (Lafferty and Zhai,2001). This type of retrieval model can processqueries very quickly by organizing the documentsinto inverted index, where each distinct term hasan inverted list that stores information about docu-ments it appears in. Nowadays, they are still widelyused in production systems. However, these re-trieval models fall short of matching related terms(vocabulary mismatch) or modeling context of theterms (semantic mismatch). Much early effortwas put into improving exact lexical match retriev-ers, such as matching n-grams (Metzler and Croft,2005) or expanding queries with terms from relateddocuments (Lavrenko and Croft, 2001). However,these methods still use BOW framework and havelimited capability of modeling human languages.Neural Ranker In order to deal with vocab-ulary mismatch, neural retrievers that rely onsoft matching between numerical text represen-tations are introduced. Early attempts computesimilarity between pre-trained word embeddingsuch as word2vec (Mikolov et al., 2013) andGLoVe (Pennington et al., 2014) to produce match-ing score (Ganguly et al., 2015; Diaz et al., 2016).One more recent approach encodes query and doc-ument each into a vector and computes vector sim-ilarity (Huang et al., 2013). Later researches real-ized the limited capacity of a single vector to en-code \ufb01ne-grained information and introduced fullinteraction models to perform soft matching be-tween all term vectors (Guo et al., 2016; Xionget al., 2017). In these approaches, scoring isbased on learned neural networks and the hugelyincreased computation cost limited their use toreranking a top candidate list generated by a lexicalretriever.Deep LM Based Ranker and Retriever DeepLM made a huge impact on neural IR. Fine-tuned Transformer (Vaswani et al., 2017) LMBERT (Devlin et al., 2019) achieved state-of-the-art reranking performance for passages and docu-ments (Nogueira and Cho, 2019; Dai and Callan,2019b). As illustrated in Figure 1a, the commonapproach is to feed the concatenated query docu-ment text through BERT and use BERT\u2019s [CLS]output token to produce a relevance score. Thedeep LM rerankers addressed both vocabulary andsemantic mismatch by computing full cross atten-tion between contextualized token representations.Lighter deep LM rankers are developed (MacA-vaney et al., 2020; Gao et al., 2020), but their crossattention operations are still too expensive for full-collection retrieval.Later research therefore resorted to augment-ing lexical retrieval with deep LMs by expandingthe document surface form to narrow the vocab-ulary gap, e.g., DocT5Query (Nogueira and Lin,2019), or altering term weights to emphasize impor-tant terms, e.g., DeepCT (Dai and Callan, 2019a).Smartly combining deep LM retriever and rerankercan offer additive gain for end performance (Gaoet al., 2021a). These retrievers however still sufferfrom vocabulary and semantic mismatch as tradi-tional lexical", "start_char_idx": 3842, "end_char_idx": 7494, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e9501756-36af-4385-a6cc-cdce40361954": {"__data__": {"id_": "e9501756-36af-4385-a6cc-cdce40361954", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60534e99-9827-4f3b-9823-84d6aaab600d", "node_type": null, "metadata": {}, "hash": "0974e6e159ce3a5df25b1c1c3a97c3530eb4178721760528f7c38a6a0ae14b0a"}, "2": {"node_id": "b119e89a-46de-4864-ab90-faae17968b6d", "node_type": null, "metadata": {}, "hash": "266c80ced2cd8c4f64a12d2cea103501db59697c4a163f1e563c57ca382000f6"}, "3": {"node_id": "7c047e1b-d810-48e8-8349-c4c51d06c128", "node_type": null, "metadata": {}, "hash": "72725a763c5c6e6ef01bf2bf32bf654b5ec0b6bd94be6261753cab2a6ae92fd7"}}, "hash": "475fa89082f8c1ffef2366ec93b1fb921c668c425b87dd36d2c8a58b4a899f86", "text": "still sufferfrom vocabulary and semantic mismatch as tradi-tional lexical retrievers.Another line of research continues the work onsingle vector representation and build dense retriev-ers, as illustrated in Figure 1b. They store docu-ment vectors in a dense index and retrieve themthrough Nearest Neighbours search. Using deepLMs, dense retrievers have achieved promising re-sults on several retrieval tasks (Karpukhin et al.,2020). Later researches show that dense retrievalsystems can be further improved by better train-ing (Xiong et al., 2020; Gao et al., 2021b).Single vector systems have also been extendedto multi-vector representation systems. Poly-encoder (Humeau et al., 2020) encodes queriesinto a set of vectors. Similarly, Me-BERT (Luanet al., 2020) represents documents with a set of vec-tors. A concurrent work ColBERT (Figure 1c) usemultiple vectors to encode both queries and docu-ments (Khattab and Zaharia, 2020). In particular, itrepresents a documents with all its terms\u2019 vectorsand a query with an expanded set of term vectors.It then computes all-to-all (Cartesian) soft matchbetween the tokens. ColBERT performs interactionas dot product followed pooling operations, whichCLS bank account SEP bank river bankCLS bank account SEP bank river bankCLS bank account SEP bank river bankCLS bank account SEP bank river bankscore(a) Cross-Attention Model (e.g., BERT reranker)CLS bank account CLS bank river bankCLS bank accountCLS bank accountCLS bank river bankCLS bank river bankscore (b) Dense Retrievers (e.g., DPR)CLS bank account CLS bank river bankCLS bank accountCLS bank accountCLS bank river bankCLS bank river bankEXP EXPscoremax max maxEXPEXPEXPEXPmax max(c) ColBERT: All-to-All MatchCLS bank account CLS bank river bankCLS bank accountCLS bank accountCLS bank river bankCLS bank river bankdot maxsum (d) COIL: Contextualized Exact MatchFigure 1: An illustration of reranking/retrieval mechanisms with deep LM, including our proposed model, COIL.BankRiverAccountBankAccountTraditional Inverted Lists Querydocid: 3tf: 2docid: 9tf: 1docid: 1tf: 1docid: 2tf: 1docid: 4tf: 1docid: 5tf: 2docid: 1tf: 1docid: 3tf: 1docid: 6tf: 1........BM25\u00a0scoringBM25\u00a0scoringBM25\u00a0scoringFigure 2: An illustration of traditional inverted lists.The inverted list maps a term to the list of documentswhere the term occurs. Retriever looks up query terms\u2019inverted lists and scores those documents with storedstatistics such as term frequency (tf).allows it to also leverage a dense index to do fullcorpus retrieval. However, since ColBERT encodesa document with all tokens, it adds another orderof magnitude of index complexity to all aforemen-tioned methods: document tokens in the collectionneed to be stored in a single huge index and con-sidered at query time. Consequently, ColBERT isengineering and hardware demanding.3 MethodologiesIn this section, we \ufb01rst provide some preliminarieson exact lexical match systems. Then we discussCOIL\u2019s contextualized exact match design and howits search index is organized. We also give a com-parison between COIL and other popular retrievers.BankRiverAccountBankAccountContextualized Inverted Lists Querydocid\u00a0 [1 3 6 7]docid\u00a0 [1 2 4 5 5 9]docid\u00a0 [3 3 9]vectorsvectorsvectorsCLSdocid\u00a0 [1 2 3 4 .............C]vectors ...CLSmatrixproductmatrixproductmatrixproductFigure 3: COIL\u2019s index and retrieval architecture.COIL-tok relies on the exact token matching (lower).COIL-full includes in addition CLS matching (upper).3.1 PreliminariesClassic lexical retrieval system", "start_char_idx": 7483, "end_char_idx": 10995, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7c047e1b-d810-48e8-8349-c4c51d06c128": {"__data__": {"id_": "7c047e1b-d810-48e8-8349-c4c51d06c128", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60534e99-9827-4f3b-9823-84d6aaab600d", "node_type": null, "metadata": {}, "hash": "0974e6e159ce3a5df25b1c1c3a97c3530eb4178721760528f7c38a6a0ae14b0a"}, "2": {"node_id": "e9501756-36af-4385-a6cc-cdce40361954", "node_type": null, "metadata": {}, "hash": "475fa89082f8c1ffef2366ec93b1fb921c668c425b87dd36d2c8a58b4a899f86"}, "3": {"node_id": "50c4a20a-f71b-4772-930b-7ff05adb5450", "node_type": null, "metadata": {}, "hash": "4069b41ee5496589eb2979980513ffafd77ee6d43b57f4808d438ef485c4ee62"}}, "hash": "72725a763c5c6e6ef01bf2bf32bf654b5ec0b6bd94be6261753cab2a6ae92fd7", "text": "matching (upper).3.1 PreliminariesClassic lexical retrieval system relies on overlap-ping query document terms under morphologicalgeneralization like stemming, in other words, exactlexical match , to score query document pair. Ascoring function is de\ufb01ned as a sum of matchedterm scores. The scores are usually based on statis-tics like term frequency ( tf). Generally, we canwrite,s=Xt2q\\d\u001bt(hq(q;t);hd(d;t)) (1)where for each overlapping term tbetween query qand document d, functionshqandhdextract terminformation and a term scoring function \u001btcom-bines them. A popular example is BM25, whichcomputes,sBM25 =Xt2q\\didf(t)hBM25q(q;t)hBM25d(d;t)hBM25q(q;t) =tft;q(1 +k2)tft;q+k2hBM25d(d;t) =tft;d(1 +k1)tft;d+k1(1\u0000b+bjdjavgdl)(2)wheretft;drefers to term frequency of term tindocumentd,tft;qrefers to the term frequency inquery,idf(t)is inverse document frequency, and b,k1,k2are hyper-parameters.One key advantage of exact lexical match sys-tems lies in ef\ufb01ciency. With summation over exactmatches, scoring of each query term only goes todocuments that contain matching terms. This canbe done ef\ufb01ciently using inverted list indexing (Fig-ure 2). The inverted list maps back from a termto a list of documents where the term occurs. Tocompute Equation 1, the retriever only needs totraverse the subset of documents in query terms\u2019inverted lists instead of going over the entire docu-ment collection.While recent neural IR research mainly focuseson breaking the exact match bottleneck with softmatching of text, we hypothesize that exact matchitself can be improved by replacing semantic in-dependent frequency-based scoring with semanticrich scoring. In the rest of this section, we showhow to modify the exact lexical match frameworkwith contextualized term representations to buildeffective and ef\ufb01cient retrieval systems.3.2 Contextualized Exact Lexical MatchInstead of term frequency, we desire to encodethe semantics of terms to facilitate more effectivematching. Inspired by recent advancements in deepLM, we encode both query and document tokensinto contextualized vector representations and carryout matching between exact lexical matched tokens.Figure 1d illustrates the scoring model of COIL.In this work, we use a Transformer languagemodel3as the contextualization function. We en-code a query qwith the language model (LM) andrepresent its i-th token by projecting the corre-sponding output:vqi=WtokLM(q;i) +btok (3)3We used the base, uncased variant of BERT.where Wnt\u0002nlmtokis a matrix that maps the LM\u2019snlmdimension output into a vector of lower di-mensionnt. We down project the vectors as wehypothesize that it suf\ufb01ces to use lower dimensiontoken vectors. We con\ufb01rm this in section 5. Simi-larly, we encode a document d\u2019sj-th tokendjwith:vdj=WtokLM(d;j) +btok (4)We then de\ufb01ne the contextualized exact lexicalmatch scoring function between query documentbased on vector similarities between exact matchedquery document token pairs:stok(q;d) =Xqi2q\\dmaxdj=qi(vqi|vdj) (5)Note that, importantly, the summation goes throughonly overlapping terms, qi2q\\d. For each querytokenqi, we \ufb01nds all same tokensdjin the docu-ment, computes their similarity with qiusing thecontextualized token vectors. The maximum sim-ilarities are picked for query token qi. Max op-erator is adopted to capture the most importantsignal (Kim,", "start_char_idx": 11000, "end_char_idx": 14326, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "50c4a20a-f71b-4772-930b-7ff05adb5450": {"__data__": {"id_": "50c4a20a-f71b-4772-930b-7ff05adb5450", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60534e99-9827-4f3b-9823-84d6aaab600d", "node_type": null, "metadata": {}, "hash": "0974e6e159ce3a5df25b1c1c3a97c3530eb4178721760528f7c38a6a0ae14b0a"}, "2": {"node_id": "7c047e1b-d810-48e8-8349-c4c51d06c128", "node_type": null, "metadata": {}, "hash": "72725a763c5c6e6ef01bf2bf32bf654b5ec0b6bd94be6261753cab2a6ae92fd7"}, "3": {"node_id": "0c9e59be-ec21-4eac-b871-36c291565153", "node_type": null, "metadata": {}, "hash": "1fe57b73672a0ea701869f585f6cf9f8ae89112c5a35ddcc143095f26d8e6d53"}}, "hash": "4069b41ee5496589eb2979980513ffafd77ee6d43b57f4808d438ef485c4ee62", "text": "Max op-erator is adopted to capture the most importantsignal (Kim, 2014). This \ufb01ts in the general lexicalmatch formulation, with hqgiving representationforqi,htgiving representations for all dj=qi, and\u001btcompute dot similarities between query vectorwith document vectors and max pool the scores.As with classic lexical systems, stokde\ufb01ned inEquation 5 does not take into account similaritiesbetween lexical-different terms, thus faces vocabu-lary mismatch. Many popular LMs (Devlin et al.,2019; Yang et al., 2019; Liu et al., 2019) use aspecial CLS token to aggregate sequence represen-tation. We project the CLS vectos with Wnc\u0002nlmclsto represent the entire query or document,vqcls=WclsLM(q;CLS) +bclsvdcls=WclsLM(d;CLS) +bcls(6)The similarity between vqclsandvdclsprovides high-level semantic matching and mitigates the issue ofvocabulary mismatch. The full form of COIL is:sfull(q;d) =stok(q;d) +vqcls|vdcls (7)In the rest of the paper, we refer to systems withCLS matching COIL-full and without COIL-tok .COIL\u2019s scoring model (Figure 1d) is fully differ-entiable. Following earlier work (Karpukhin et al.,2020), we train COIL with negative log likelihoodde\ufb01ned over query q, a positive document d+and aset of negative documents fd\u00001;d\u00002;::d\u0000l::gas loss.L=\u0000logexp(s(q;d+))exp(s(q;d+)) +Plexp(s(q;d\u0000l))(8)Following Karpukhin et al. (2020), we use in batchnegatives and hard negatives generated by BM25.Details are discussed in implementation, section 4.3.3 Index and Retrieval with COILCOIL pre-computes the document representationsand builds up a search index, which is illustrated inFigure 3. Documents in the collection are encodedof\ufb02ine into token and CLS vectors. Formally, fora unique token tin the vocabulary V, we collectits contextualized vectors from all of its mentionsfrom documents in collection C, building token t\u2019scontextualized inverted list:It=fvdjjdj=t;d2Cg; (9)where vdjis the BERT-based token encoding de-\ufb01ned in Equation 4. We de\ufb01ne search index tostore inverted lists for all tokens in vocabulary,I=fItjt2Vg. For COIL-full, we also build anindex for the CLS token Icls=fvdclsjd2Cg.As shown in Figure 3, in this work we im-plement COIL\u2019s by stacking vectors in each in-verted listItinto a matrix Mnt\u0002jIkj, so that sim-ilarity computation that traverses an inverted listand computes vector dot product can be done ef-\ufb01ciently as one matrix-vector product with opti-mized BLAS (Blackford et al., 2002) routines onCPU or GPU. All vdclsvectors can also be organizedin a similar fashion into matrix Mclsand queriedwith matrix product. The matrix implementationhere is an exhaustive approach that involves all vec-tors in an inverted list. As a collection of densevectors, it is also possible to organize each invertedlist as an approximate search index (Johnson et al.,2017; Guo et al., 2019) to further speed up search.When a query qcomes in, we encode every ofits token into vectors vqi. The vectors are sent tothe subset of COIL inverted lists that correspondsquery tokens J=fItjt2qg. where the matrixproduct described above is carried out. This isef\ufb01cient asjJj<<jIj, having only a small subsetof all inverted lists to be involved in search. ForCOIL-full, we also use encoded CLS vectors vqclsto query the CLS index", "start_char_idx": 14331, "end_char_idx": 17568, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0c9e59be-ec21-4eac-b871-36c291565153": {"__data__": {"id_": "0c9e59be-ec21-4eac-b871-36c291565153", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60534e99-9827-4f3b-9823-84d6aaab600d", "node_type": null, "metadata": {}, "hash": "0974e6e159ce3a5df25b1c1c3a97c3530eb4178721760528f7c38a6a0ae14b0a"}, "2": {"node_id": "50c4a20a-f71b-4772-930b-7ff05adb5450", "node_type": null, "metadata": {}, "hash": "4069b41ee5496589eb2979980513ffafd77ee6d43b57f4808d438ef485c4ee62"}, "3": {"node_id": "e55d7182-4c9f-4466-b8aa-af0b01c9ca7d", "node_type": null, "metadata": {}, "hash": "b720e81dda1e3587cf11e6d32101a0ffc1678a11fe02db1310a7f97a984179b0"}}, "hash": "1fe57b73672a0ea701869f585f6cf9f8ae89112c5a35ddcc143095f26d8e6d53", "text": "we also use encoded CLS vectors vqclsto query the CLS index to get the CLS matchingscores. The scoring over different inverted lists canserve in parallel. The scores are then combined byEquation 5 to rank the documents.Readers can \ufb01nd detailed illustration \ufb01gures inthe Appendix A, for index building and querying,Figure 4 and Figure 5, respectively.3.4 Connection to Other RetrieversDeep LM based Lexical Index Models likeDeepCT (Dai and Callan, 2019a, 2020) andDocT5Query (Nogueira and Lin, 2019) alter tft;din documents with deep LM BERT or T5. This issimilar to a COIL-tok with token dimension nt= 1.A single degree of freedom however measures moreof a term importance than semantic agreement .Dense Retriever Dense retrievers (Figure 1b)are equivalent to COIL-full\u2019s CLS matching. COILmakes up for the lost token-level interactions indense retriever with exact matching signals.ColBERT ColBERT (Figure 1c) computes rel-evance by soft matching allquery and documentterm\u2019s contextualized vectors.s(q;d) =Xqi2[cls;q;exp]maxdj2[cls;d](vqi|vdj) (10)where interactions happen among query q, docu-mentd,clsand set of query expansion tokens exp.The all-to-all match contrasts COIL that only usesexact match. It requires a dense retrieval over alldocument tokens\u2019 representations as opposed toCOIL which only considers query\u2019s overlapping to-kens, and are therefore much more computationallyexpensive than COIL.4 Experiment MethodologiesDatasets We experiment with two large scale adhoc retrieval benchmarks from the TREC 2019Deep Learning (DL) shared task: MSMARCOpassage (8M English passages of average lengtharound 60 tokens) and MSMARCO document (3MEnglish documents of average length around 900tokens)4. For each, we train models with theMSMARCO Train queries, and record results onMSMARCO Dev queries and TREC DL 2019test queries. We report mainly full-corpus re-trieval results but also include the rerank task onMSMARCO Dev queries where we use neuralscores to reorder BM25 retrieval results providedby MSMARO organizers. Of\ufb01cial metrics include4Both datasets can be downloaded from https://microsoft.github.io/msmarco/MRR@1K and NDCG@10 on test and MRR@10on MSMARCO Dev. We also report recall for thedev queries following prior work (Dai and Callan,2019a; Nogueira and Lin, 2019).Compared Systems Baselines include 1) tradi-tional exact match system BM25, 2) deep LM aug-mented BM25 systems DeepCT (Dai and Callan,2019a) and DocT5Query (Nogueira and Lin, 2019),3) dense retrievers, and 4) soft all-to-all retrieverColBERT. For DeepCT and DocT5Query, we usethe rankings provided by the authors. For denseretrievers, we report two dense retrievers trainedwith BM25 negatives or with mixed BM25 andrandom negatives, published in Xiong et al. (2020).However since these systems use a robust versionof BERT, RoBERTa (Liu et al., 2019) as the LMand train document retriever also on MSMARCOpassage set, we in addition reproduce a third denseretriever, that uses the exact same training setup asCOIL. All dense retrievers use 768 dimension em-bedding. For ColBERT, we report its published re-sults (available only on passage collection). BERTreranker is added in the rerank task.We include 2 COIL systems: 1) COIL-tok, theexact token match only system, and 2) COLL-full,the model with both token match and CLS match.Implementation We build our models with Py-torch (Paszke et al., 2019) based on huggingfacetransformers (Wolf et al., 2019).", "start_char_idx": 17577, "end_char_idx": 21014, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e55d7182-4c9f-4466-b8aa-af0b01c9ca7d": {"__data__": {"id_": "e55d7182-4c9f-4466-b8aa-af0b01c9ca7d", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60534e99-9827-4f3b-9823-84d6aaab600d", "node_type": null, "metadata": {}, "hash": "0974e6e159ce3a5df25b1c1c3a97c3530eb4178721760528f7c38a6a0ae14b0a"}, "2": {"node_id": "0c9e59be-ec21-4eac-b871-36c291565153", "node_type": null, "metadata": {}, "hash": "1fe57b73672a0ea701869f585f6cf9f8ae89112c5a35ddcc143095f26d8e6d53"}, "3": {"node_id": "e04d0610-0d39-48fd-a88a-ab2eb23cf03f", "node_type": null, "metadata": {}, "hash": "641759034013448770bb081eee62ec6e80b34fa0039323f21600e43cf0244a74"}}, "hash": "b720e81dda1e3587cf11e6d32101a0ffc1678a11fe02db1310a7f97a984179b0", "text": "2019) based on huggingfacetransformers (Wolf et al., 2019). COIL\u2019s LM isbased on BERT\u2019s base variant. COIL systems usetoken dimension nt= 32 and COIL-full use CLSdimensionnc= 768 as default, leading to 110Mparameters. We add a Layer Normalization to CLSvector when useful. All models are trained for 5epochs with AdamW optimizer, a learning rate of3e-6, 0.1 warm-up ratio, and linear learning ratedecay, which takes around 12 hours. Hard neg-atives are sampled from top 1000 BM25 results.Each query uses 1 positive and 7 hard negatives;each batch uses 8 queries on MSMARCO passageand 4 on MSMARCO document. Documents aretruncated to the \ufb01rst 512 tokens to \ufb01t in BERT.We conduct validation on randomly selected 512queries from corresponding train set. Latency num-bers are measured on dual Xeon E5-2630 v3 forCPU and RTX 2080 ti for GPU. We implementCOIL\u2019s inverted lists as matrices as described insubsection 3.3, using NumPy (Harris et al., 2020)on CPU and Pytorch on GPU. We perform a) a setof matrix products to compute token similaritiesover contextualized inverted lists, b) scatter to maptoken scores back to documents, and c) sort to rankthe documents. Illustration can be found in theappendix, Figure 5.5 ResultsThis section studies the effectiveness of COILand how vector dimension in COIL affects theeffectiveness-ef\ufb01ciency tradeoff. We also providequalitative analysis on contextualized exact match.5.1 Main ResultsTable 1 reports various systems\u2019 performance onthe MARCO passage collection. COIL-tok ex-act lexical match only system signi\ufb01cantly out-performs all previous lexical retrieval systems.With contextualized term similarities, COIL-tokachieves a MRR of 0.34 compared to BM25\u2019s MRR0.18. DeepCT and DocT5Query, which also usedeep LMs like BERT and T5, are able to break thelimit of heuristic term frequencies but are still lim-ited by semantic mismatch issues. We see COIL-tok outperforms both systems by a large margin.COIL-tok also ranks top of the candidate list bet-ter than dense retrieves. It prevails in MRR andNDCG while performs on par in recall with thebest dense system, indicating that COIL\u2019s tokenlevel interaction can improve precision. With theCLS matching added, COIL-full gains the abilityto handle mismatched vocabulary and enjoys an-other performance leap, outperforming all denseretrievers.COIL-full achieves a very narrow performancegap to ColBERT. Recall that ColBERT computesall-to-all soft matches between all token pairs. Forretrieval, it needs to consider for each query tokenallmentions of alltokens in the collection (MS-MARCO passage collection has around 500M to-ken mentions). COIL-full is able to capture match-ing patterns as effectively with exact match signalsfrom only query tokens\u2019 mentions and a single CLSmatching to bridge the vocabulary gap.We observe a similar pattern in the rerank task.COIL-tok is already able to outperform dense re-triever and COIL-full further adds up to perfor-mance with CLS matching, being on-par with Col-BERT. Meanwhile, previous BERT rerankers havelittle performance advantage over COIL5. In prac-tice, we found BERT rerankers to be much more5Close performance between COIL and BERT rerankersis partially due to the bottleneck of BM25 candidates.Table 1: MSMARCO passage collection results. Results not applicable are denoted \u2018\u2013\u2019 and no available \u2018n.a.\u2019.MS MARCO Passage RankingDev Rerank Dev Retrieval DL2019 RetrievalModel MRR@10 MRR@10 Recall@1K NDCG@10 MRR@1KLexical RetrieverBM25 \u2013 0.184", "start_char_idx": 21012, "end_char_idx": 24494, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e04d0610-0d39-48fd-a88a-ab2eb23cf03f": {"__data__": {"id_": "e04d0610-0d39-48fd-a88a-ab2eb23cf03f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60534e99-9827-4f3b-9823-84d6aaab600d", "node_type": null, "metadata": {}, "hash": "0974e6e159ce3a5df25b1c1c3a97c3530eb4178721760528f7c38a6a0ae14b0a"}, "2": {"node_id": "e55d7182-4c9f-4466-b8aa-af0b01c9ca7d", "node_type": null, "metadata": {}, "hash": "b720e81dda1e3587cf11e6d32101a0ffc1678a11fe02db1310a7f97a984179b0"}, "3": {"node_id": "ede0823c-d276-454d-bdae-217d6a5b68e5", "node_type": null, "metadata": {}, "hash": "7f5b03f6e5948ef27a6bda66b121c2768337d4f630ea56485028256588f5cbf6"}}, "hash": "641759034013448770bb081eee62ec6e80b34fa0039323f21600e43cf0244a74", "text": "NDCG@10 MRR@1KLexical RetrieverBM25 \u2013 0.184 0.853 0.506 0.825DeepCT \u2013 0.243 0.909 0.572 0.883DocT5Query \u2013 0.278 0.945 0.642 0.888BM25+BERT reranker 0.347 \u2013 \u2013 \u2013 \u2013Dense RetrieverDense (BM25 neg) n.a. 0.299 0.928 0.600 n.a.Dense (rand + BM25 neg) n.a. 0.311 0.952 0.576 n.a.Dense (our train) 0.312 0.304 0.932 0.635 0.898ColBERT 0.349 0.360 0.968 n.a. n.a.COIL-tok 0.336 0.341 0.949 0.660 0.915COIL-full 0.348 0.355 0.963 0.704 0.924Table 2: MSMARCO document collection results. Results not applicable are denoted \u2018\u2013\u2019 and no available \u2018n.a.\u2019.MS MARCO Document RankingDev Rerank Dev Retrieval DL2019 RetrievalModel MRR@10 MRR@10 Recall@1K NDCG@10 MRR@1KLexical RetrieverBM25 \u2013 0.230 0.886 0.519 0.805DeepCT \u2013 0.320 0.942 0.544 0.891DocT5Query \u2013 0.288 0.926 0.597 0.837BM25+BERT reranker 0.383 \u2013 \u2013 \u2013 \u2013Dense RetrieverDense (BM25 neg) n.a. 0.299 0.928 0.600 n.a.Dense (rand + BM25 neg) n.a. 0.311 0.952 0.576 n.a.Dense (our train) 0.358 0.340 0.883 0.546 0.785COIL-tok 0.381 0.385 0.952 0.626 0.921COIL-full 0.388 0.397 0.962 0.636 0.913expensive, requiring over 2700 ms for rerankingcompared to around 10ms in the case of COIL.Table 2 reports the results on MSMARCO docu-ment collection. In general, we observe a similarpattern as with the passage case. COIL systemssigni\ufb01cantly outperform both lexical and dense sys-tems in MRR and NDCG and retain a small advan-tage measured in recall. The results suggest thatCOIL can be applicable to longer documents witha consistent advantage in effectiveness.The results indicate exact lexical match mecha-nism can be greatly improved with the introductionof contextualized representation in COIL. COIL\u2019stoken-level match also yields better \ufb01ne-grainedsignals than dense retriever\u2019s global match signal.COIL-full further combines the lexical signals withdense CLS match, forming a system that can dealwith both vocabulary and semantic mismatch, be-ing as effective as all-to-all system.5.2 Analysis of DimensionalityThe second experiment tests how varying COIL\u2019stoken dimension ntand CLS dimension ncaffectmodel effectiveness and ef\ufb01ciency. We record re-trieval performance and latency on MARCO pas-sage collection in Table 3.In COIL-full systems, reducing CLS dimensionfrom 768 to 128 leads to a small drop in perfor-mance on the Dev set, indicating that a full 768dimension may not be necessary for COIL. Keep-ing CLS dimension at 128, systems with tokendimension 32 and 8 have very small performancedifference, suggesting that token-speci\ufb01c semanticconsumes much fewer dimensions. Similar patterninntis also observed in COIL-tok ( nc= 0).On the DL2019 queries, we observe that reduc-ing dimension actually achieves better MRR. Webelieve this is due to a regulatory effect, as theTable 3: Performance and latency of COIL systems with different representation dimensions. Results not applica-ble are denoted \u2018\u2013\u2019 and no available \u2018n.a.\u2019. Here ncdenotes COIL CLS dimension and nttoken vector dimension.*: ColBERT use", "start_char_idx": 24508, "end_char_idx": 27457, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ede0823c-d276-454d-bdae-217d6a5b68e5": {"__data__": {"id_": "ede0823c-d276-454d-bdae-217d6a5b68e5", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60534e99-9827-4f3b-9823-84d6aaab600d", "node_type": null, "metadata": {}, "hash": "0974e6e159ce3a5df25b1c1c3a97c3530eb4178721760528f7c38a6a0ae14b0a"}, "2": {"node_id": "e04d0610-0d39-48fd-a88a-ab2eb23cf03f", "node_type": null, "metadata": {}, "hash": "641759034013448770bb081eee62ec6e80b34fa0039323f21600e43cf0244a74"}, "3": {"node_id": "90a7a2df-f028-4023-a33d-b46ca7473bd6", "node_type": null, "metadata": {}, "hash": "e6feaf43bcd429522b29850899e93fdd1f1f8b63c658396e6aabca195891780a"}}, "hash": "7f5b03f6e5948ef27a6bda66b121c2768337d4f630ea56485028256588f5cbf6", "text": "Here ncdenotes COIL CLS dimension and nttoken vector dimension.*: ColBERT use approximate search and quantization. We exclude I/O time from measurements.Dev Retrieval DL2019 Retrieval Latency/msModel MRR@10 Recall@1K NDCG@10 MRR CPU GPUBM25 0.184 0.853 0.506 0.825 36 n.a.Dense 0.304 0.932 0.635 0.898 293 32ColBERT 0.360 0.968 n.a. n.a. 458* \u2013COILncnt768 32 0.355 0.963 0.704 0.924 380 41128 32 0.350 0.953 0.692 0.956 125 23128 8 0.347 0.956 0.694 0.977 113 210 32 0.341 0.949 0.660 0.915 67 180 8 0.336 0.940 0.678 0.953 55 16Table 4: Sample query document pairs with similarity scores produced by COIL. Tokens in examination are coloredblue. Numbers in brackets are query-document vector similarities computed with vectors generated by COIL.Query Token COIL Contextualized Exact Match Score Relevancewhat is a cabinet in govtCabinet [16.28] (government) A cabinet [16.75] is a body of high-ranking state of\ufb01cials, typically consisting of the top leaders of the ....+Cabinet [7.23] is 20x60 and top is 28x72. .... I had a 2cm granite counter-top installed with a 10 inch overhang on one side and a 14 inch....-what is priority passPriority Pass [11.61] is an independent airport lounge access program. Amembership provides you with access to their network of over 700 ....+Snoqualmie Pass [7.98] is a mountain pass [6.83] that carries Interstate90 through the Cascade Range in the U.S. State of Washington....-what isnjstartNJSTART is [1.25] a self-service online platform that allows vendors tomanage forms, certi\ufb01cations, submit proposals, access training ....+Contract awardees will receive their Blanket P.O. once it is [-0.10] con-verted, and details regarding that process will also be sent...-test queries were labeled differently from the MS-MARCO train/dev queries (Craswell et al., 2020).We also record CPU and GPU search latencyin Table 3. Lowering COIL-full\u2019s CLS dimen-sion from 768 to 128 gives a big speedup, makingCOIL faster than DPR system. Further droppingtoken dimensions provide some extra speedup. TheCOIL-tok systems run faster than COIL-full, with alatency of the same order of magnitude as the tradi-tional BM25 system. Importantly, lower dimensionCOIL systems still retain a performance advantageover dense systems while being much faster. Weinclude ColBERT\u2019s latency reported in the originalpaper, which was optimized by approximate searchand quantization. All COIL systems have lowerlatency than ColBERT even though our current im-plementation does not use those optimization tech-niques. We however note that approximate searchand quantization are applicable to COIL, and leavethe study of speeding up COIL to future work.5.3 Case StudyCOIL differs from all previous embedding-basedmodels in that it does not use a single uni\ufb01ed em-bedding space. Instead, for a speci\ufb01c token, COILlearns an embedding space to encode and measurethe semantic similarity of the token in differentcontexts. In this section, we show examples whereCOIL differentiates different senses of a word un-der different contexts. In Table 4, we show howthe token similarity scores differ across contexts inrelevant and irrelevant query document pairs.The \ufb01rst query looks for \u201ccabinet\u201d in the contextof \u201cgovt\u201d (abbreviation for \u201cgovernment\u201d). Thetwo documents both include query token \"cabinet\"but of a different concept. The \ufb01rst one refers tothe government cabinet and the second to a caseor cupboard. COIL manages to match \u201ccabinet\u201d inthe", "start_char_idx": 27430, "end_char_idx": 30873, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "90a7a2df-f028-4023-a33d-b46ca7473bd6": {"__data__": {"id_": "90a7a2df-f028-4023-a33d-b46ca7473bd6", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60534e99-9827-4f3b-9823-84d6aaab600d", "node_type": null, "metadata": {}, "hash": "0974e6e159ce3a5df25b1c1c3a97c3530eb4178721760528f7c38a6a0ae14b0a"}, "2": {"node_id": "ede0823c-d276-454d-bdae-217d6a5b68e5", "node_type": null, "metadata": {}, "hash": "7f5b03f6e5948ef27a6bda66b121c2768337d4f630ea56485028256588f5cbf6"}, "3": {"node_id": "b1b6fea6-94a1-4a48-856b-3bc96fd7ffa8", "node_type": null, "metadata": {}, "hash": "5b4ab7cc49866c494deb19e7bd40c20b80b2db8c6b80d69dfa35dc161c73d642"}}, "hash": "e6feaf43bcd429522b29850899e93fdd1f1f8b63c658396e6aabca195891780a", "text": "caseor cupboard. COIL manages to match \u201ccabinet\u201d inthe query to \u201ccabinet\u201d in the \ufb01rst document witha much higher score. In the second query, \"pass\"in both documents refer to the concept of permis-sion. However, through contextualization, COILcaptures the variation of the same concept and as-signs a higher score to \u201cpass\u201d in the \ufb01rst document.Stop words like \u201cit\u201d, \u201ca\u201d, and \u201cthe\u201d are com-monly removed in classic exact match IR systemsas they are not informative on their own. In thethird query, on the other hand, we observe thatCOIL is able to differentiate \u201cis\u201d in an explanatorysentence and \u201cis\u201d in a passive form, assigning the\ufb01rst higher score to match query context.All examples here show that COIL can go be-yond matching token surface form and introducerich context information to estimate matching. Dif-ferences in similarity scores across mentions underdifferent contexts demonstrate how COIL systemsgain strength over lexical systems.6 Conclusion and Future WorkExact lexical match systems have been widely usedfor decades in classical IR systems and prove to beeffective and ef\ufb01cient. In this paper, we point outa critical problem, semantic mismatch, that gener-ally limits all IR systems based on surface tokenfor matching. To \ufb01x semantic mismatch, we in-troduce contextualized exact match to differentiatethe same token in different contexts, providing ef-fective semantic-aware token match signals. Wefurther propose contextualized inverted list (COIL)search index which swaps token statistics in in-verted lists with contextualized vector representa-tions to perform effective search.On two large-scale ad hoc retrieval benchmarks,we \ufb01nd COIL substantially improves lexical re-trieval and outperforms state-of-the-art dense re-trieval systems. These results indicate large head-room of the simple-but-ef\ufb01cient exact lexical matchscheme. When the introduction of contextualiza-tion handles the issue of semantic mismatch, exactmatch system gains the capability of modeling com-plicated matching patterns that were not capturedby classical systems.V ocabulary mismatch in COIL can also belargely mitigated with a high-level CLS vectormatching. The full system performs on par withmore expensive and complex all-to-all match re-trievers. The success of the full system also showsthat dense retrieval and COIL\u2019s exact token match-ing give complementary effects, with COIL makingup dense system\u2019s lost token level matching signalsand dense solving the vocabulary mismatch proba-bly for COIL.With our COIL systems showing viable searchlatency, we believe this paper makes a solid steptowards building next-generation index that storessemantics. At the intersection of lexical and neuralsystems, ef\ufb01cient algorithms proposed for both canpush COIL towards real-world systems.ReferencesS. Blackford, J. Demmel, J. Dongarra, I. Duff, S. Ham-marling, Greg Henry, M. H\u00e9roux, L. Kaufman, An-drew Lumsdaine, A. Petitet, R. Pozo, K. Remington,and C. Whaley. 2002. An updated set of basic linearalgebra subprograms (blas). ACM Transactions onMathematical Software , 28.Nick Craswell, Bhaskar Mitra, Emine Yilmaz, DanielCampos, and Ellen M V oorhees. 2020. Overviewof the trec 2019 deep learning track. arXiv preprintarXiv:2003.07820 .Zhuyun Dai and J. Callan. 2019a. Context-aware sen-tence/passage term importance estimation for \ufb01rststage retrieval. ArXiv , abs/1910.10687.Zhuyun Dai and J. Callan. 2020. Context-aware docu-ment term weighting for ad-hoc search.", "start_char_idx": 30894, "end_char_idx": 34359, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b1b6fea6-94a1-4a48-856b-3bc96fd7ffa8": {"__data__": {"id_": "b1b6fea6-94a1-4a48-856b-3bc96fd7ffa8", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60534e99-9827-4f3b-9823-84d6aaab600d", "node_type": null, "metadata": {}, "hash": "0974e6e159ce3a5df25b1c1c3a97c3530eb4178721760528f7c38a6a0ae14b0a"}, "2": {"node_id": "90a7a2df-f028-4023-a33d-b46ca7473bd6", "node_type": null, "metadata": {}, "hash": "e6feaf43bcd429522b29850899e93fdd1f1f8b63c658396e6aabca195891780a"}, "3": {"node_id": "b4c218e3-4e77-42d9-88d3-789cb5d1a843", "node_type": null, "metadata": {}, "hash": "7e213efce914e6141f039ff8a544f20918478c81580bbac3dde1d5250463cab3"}}, "hash": "5b4ab7cc49866c494deb19e7bd40c20b80b2db8c6b80d69dfa35dc161c73d642", "text": "2020. Context-aware docu-ment term weighting for ad-hoc search. Proceedingsof The Web Conference 2020 .Zhuyun Dai and Jamie Callan. 2019b. Deeper text un-derstanding for IR with contextual neural languagemodeling. In Proceedings of the 42nd InternationalACM SIGIR Conference on Research and Develop-ment in Information Retrieval, SIGIR 2019, Paris,France, July 21-25, 2019 , pages 985\u2013988. ACM.J. Devlin, Ming-Wei Chang, Kenton Lee, and KristinaToutanova. 2019. Bert: Pre-training of deep bidirec-tional transformers for language understanding. InNAACL-HLT .Fernando Diaz, Bhaskar Mitra, and Nick Craswell.2016. Query expansion with locally-trained wordembeddings. In Proceedings of the 54th AnnualMeeting of the Association for Computational Lin-guistics .Debasis Ganguly, Dwaipayan Roy, Mandar Mitra,and Gareth J. F. Jones. 2015. Word embeddingbased generalized language model for informationretrieval. In Proceedings of the 38th InternationalACM SIGIR Conference on Research and Develop-ment in Information Retrieval .Luyu Gao, Zhuyun Dai, and Jamie Callan. 2020. Mod-ularized transfomer-based ranking framework. InProceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing, EMNLP2020, Online, November 16-20, 2020 . Associationfor Computational Linguistics.Luyu Gao, Zhuyun Dai, and Jamie Callan. 2021a. Re-think training of BERT rerankers in multi-stage re-trieval pipeline. In Advances in Information Re-trieval - 43rd European Conference on IR Research,ECIR 2021, Virtual Event, March 28 - April 1, 2021,Proceedings, Part II .Luyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Ben-jamin Van Durme, and Jamie Callan. 2021b. Com-plement lexical retrieval model with semantic resid-ual embeddings. In Advances in Information Re-trieval - 43rd European Conference on IR Research,ECIR 2021, Virtual Event, March 28 - April 1, 2021,Proceedings, Part I .J. Guo, Y . Fan, Qingyao Ai, and W. Croft. 2016. Adeep relevance matching model for ad-hoc retrieval.Proceedings of the 25th ACM International on Con-ference on Information and Knowledge Manage-ment .R. Guo, Philip Y . Sun, E. Lindgren, Quan Geng, DavidSimcha, Felix Chern, and S. Kumar. 2019. Accel-erating large-scale inference with anisotropic vectorquantization. arXiv: Learning .Charles R. Harris, K. Jarrod Millman, St\u00e9fan Jvan der Walt, Ralf Gommers, Pauli Virtanen, DavidCournapeau, Eric Wieser, Julian Taylor, Sebas-tian Berg, Nathaniel J. Smith, Robert Kern, MattiPicus, Stephan Hoyer, Marten H. van Kerkwijk,Matthew Brett, Allan Haldane, Jaime Fern\u00e1ndez delR\u00edo, Mark Wiebe, Pearu Peterson, Pierre G\u00e9rard-Marchant, Kevin Sheppard, Tyler Reddy, WarrenWeckesser, Hameer Abbasi, Christoph Gohlke, andTravis E. Oliphant. 2020. Array programming withNumPy. Nature .Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,Alex Acero, and Larry Heck. 2013. Learning deepstructured semantic models for web search usingclickthrough data. In Proceedings of the 22nd ACMinternational conference on Information & Knowl-edge Management .Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux,and J. Weston. 2020. Poly-encoders: Architec-tures and pre-training strategies for fast and accuratemulti-sentence scoring. In ICLR .J. Johnson, M. Douze, and H.", "start_char_idx": 34351, "end_char_idx": 37563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b4c218e3-4e77-42d9-88d3-789cb5d1a843": {"__data__": {"id_": "b4c218e3-4e77-42d9-88d3-789cb5d1a843", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60534e99-9827-4f3b-9823-84d6aaab600d", "node_type": null, "metadata": {}, "hash": "0974e6e159ce3a5df25b1c1c3a97c3530eb4178721760528f7c38a6a0ae14b0a"}, "2": {"node_id": "b1b6fea6-94a1-4a48-856b-3bc96fd7ffa8", "node_type": null, "metadata": {}, "hash": "5b4ab7cc49866c494deb19e7bd40c20b80b2db8c6b80d69dfa35dc161c73d642"}, "3": {"node_id": "136fb64b-5597-468d-be0a-67296d92d1ce", "node_type": null, "metadata": {}, "hash": "22f5a6c99214da5aac9d2bf7441d80425d77fea53b36a10ab4a7af724f87e770"}}, "hash": "7e213efce914e6141f039ff8a544f20918478c81580bbac3dde1d5250463cab3", "text": "scoring. In ICLR .J. Johnson, M. Douze, and H. J\u00e9gou. 2017. Billion-scale similarity search with gpus. ArXiv ,abs/1702.08734.V . Karpukhin, Barlas O \u02d8guz, Sewon Min, PatrickLewis, Ledell Yu Wu, Sergey Edunov, DanqiChen, and W. Yih. 2020. Dense passage re-trieval for open-domain question answering. ArXiv ,abs/2004.04906.O. Khattab and M. Zaharia. 2020. Colbert: Ef\ufb01cientand effective passage search via contextualized lateinteraction over bert. Proceedings of the 43rd Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval .Yoon Kim. 2014. Convolutional neural networks forsentence classi\ufb01cation. In EMNLP .John Lafferty and Chengxiang Zhai. 2001. Documentlanguage models, query models, and risk minimiza-tion for information retrieval. In Proceedings of the24th Annual International ACM SIGIR Conferenceon Research and Development in Information Re-trieval .Victor Lavrenko and W. Bruce Croft. 2001. Relevance-based language models. In Proceedings of the 24thAnnual International ACM SIGIR Conference on Re-search and Development in Information Retrieval .Y . Liu, Myle Ott, Naman Goyal, Jingfei Du, MandarJoshi, Danqi Chen, Omer Levy, M. Lewis, LukeZettlemoyer, and Veselin Stoyanov. 2019. Roberta:A robustly optimized bert pretraining approach.ArXiv , abs/1907.11692.Yi Luan, Jacob Eisenstein, Kristina Toutanova, andM. Collins. 2020. Sparse, dense, and atten-tional representations for text retrieval. ArXiv ,abs/2005.00181.Sean MacAvaney, F. Nardini, R. Perego, N. Tonellotto,Nazli Goharian, and O. Frieder. 2020. Ef\ufb01cient doc-ument re-ranking for transformers by precomputingterm representations. Proceedings of the 43rd Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval .Donald Metzler and W. Bruce Croft. 2005. A markovrandom \ufb01eld model for term dependencies. In SIGIR2005: Proceedings of the 28th Annual InternationalACM SIGIR Conference on Research and Develop-ment in Information Retrieval .Tomas Mikolov, Ilya Sutskever, Kai Chen, G. S. Cor-rado, and J. Dean. 2013. Distributed representationsof words and phrases and their compositionality. InNIPS .Rodrigo Nogueira and Kyunghyun Cho. 2019. Passagere-ranking with bert. ArXiv , abs/1901.04085.Rodrigo Nogueira and Jimmy Lin. 2019. Fromdoc2query to doctttttquery.Adam Paszke, Sam Gross, Francisco Massa, AdamLerer, James Bradbury, Gregory Chanan, TrevorKilleen, Zeming Lin, Natalia Gimelshein, LucaAntiga, Alban Desmaison, Andreas Kopf, EdwardYang, Zachary DeVito, Martin Raison, Alykhan Te-jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,Junjie Bai, and Soumith Chintala. 2019. Py-torch: An imperative style, high-performance deeplearning library. In H. Wallach, H. Larochelle,A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Gar-nett, editors, Advances in Neural Information Pro-cessing Systems 32 . Curran Associates, Inc.Jeffrey Pennington, R. Socher, and Christopher D.Manning. 2014. Glove: Global vectors for word rep-resentation. In EMNLP .Matthew E. Peters, Mark Neumann, Mohit Iyyer, MattGardner, Christopher Clark, Kenton Lee, and LukeZettlemoyer.", "start_char_idx": 37582, "end_char_idx": 40675, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "136fb64b-5597-468d-be0a-67296d92d1ce": {"__data__": {"id_": "136fb64b-5597-468d-be0a-67296d92d1ce", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "60534e99-9827-4f3b-9823-84d6aaab600d", "node_type": null, "metadata": {}, "hash": "0974e6e159ce3a5df25b1c1c3a97c3530eb4178721760528f7c38a6a0ae14b0a"}, "2": {"node_id": "b4c218e3-4e77-42d9-88d3-789cb5d1a843", "node_type": null, "metadata": {}, "hash": "7e213efce914e6141f039ff8a544f20918478c81580bbac3dde1d5250463cab3"}}, "hash": "22f5a6c99214da5aac9d2bf7441d80425d77fea53b36a10ab4a7af724f87e770", "text": "MattGardner, Christopher Clark, Kenton Lee, and LukeZettlemoyer. 2018. Deep contextualized word repre-sentations. ArXiv , abs/1802.05365.Stephen E Robertson and Steve Walker. 1994. Somesimple effective approximations to the 2-poissonmodel for probabilistic weighted retrieval. In Pro-ceedings of the 17th Annual International ACM-SIGIR Conference on Research and Development inInformation Retrieval .Ashish Vaswani, Noam Shazeer, Niki Parmar, JakobUszkoreit, Llion Jones, Aidan N. Gomez, L. Kaiser,and Illia Polosukhin. 2017. Attention is all you need.InNIPS .Thomas Wolf, Lysandre Debut, Victor Sanh, JulienChaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtow-icz, Joe Davison, Sam Shleifer, Patrick von Platen,Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,Teven Le Scao, Sylvain Gugger, Mariama Drame,Quentin Lhoest, and Alexander M. Rush. 2019.Huggingface\u2019s transformers: State-of-the-art naturallanguage processing. ArXiv , abs/1910.03771.Chenyan Xiong, Zhuyun Dai, J. Callan, Zhiyuan Liu,and R. Power. 2017. End-to-end neural ad-hoc rank-ing with kernel pooling. Proceedings of the 40thInternational ACM SIGIR Conference on Researchand Development in Information Retrieval .Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,J. Liu, P. Bennett, Junaid Ahmed, and Arnold Over-wijk. 2020. Approximate nearest neighbor negativecontrastive learning for dense text retrieval. ArXiv ,abs/2007.00808.Z. Yang, Zihang Dai, Yiming Yang, J. Carbonell,R. Salakhutdinov, and Quoc V . Le. 2019. Xlnet:Generalized autoregressive pretraining for languageunderstanding. In NeurIPS .A AppendixA.1 Index Building IllustrationThe following \ufb01gure demonstrates how the document \"apple pie baked ...\" is indexed by COIL. Thedocument is \ufb01rst processed by a \ufb01ne-tuned deep LM to produce for each token a contextualized vector.The vectors of each term \"apple\" and \"juice\" are collected to the corresponding inverted list index alongwith the document id for lookup.appleLMuapplevappleDocument #10 - apple pie baked ...10vpie10uvpie bakedvpie wbakedv10wbakedFigure 4: COIL Index Building of document \"apple pie baked...\"A.2 Search IllustrationThe following \ufb01gure demonstrates how the query \"apple juice\" is processed by COIL. Contextualizedvectors of each term \"apple\" and \"juice\" go to the corresponding inverted list index consisting of a lookupid array and a matrix stacked from document term vectors. For each index, a matrix vector product is runto produce an array of scores. Afterwards a max-scatter of scores followed by a sortproduces the \ufb01nalranking. Note for each index, we show only operations for a subset of vectors (3 vectors) in the indexmatrix.v u776zyx776ScoreIdxzyx ScoreIndexapplev w975rqp975ScoreIdxrqp ScoreIndexjuiceSortMatrix V ector ProductMax ScatterSortingQuery: apple juiceFigure 5: COIL Search of query \"apple juice\".", "start_char_idx": 40649, "end_char_idx": 43512, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ca23bbc0-f0b4-4662-9f00-07cb35a6fca0": {"__data__": {"id_": "ca23bbc0-f0b4-4662-9f00-07cb35a6fca0", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4dc5e6ce-203f-481b-9cc5-8ed7ea0ee624", "node_type": null, "metadata": {}, "hash": "ef1f7cf17c81b933bb5a75799733b83b904bbec2dd7a41e91a3b7263684da1b1"}, "3": {"node_id": "55b0b76e-94f8-4fb6-a951-0e4ae2da4530", "node_type": null, "metadata": {}, "hash": "8a2217dcb14e82871e8709efb9cdb64bf3b825ab9df84169bd07ee8080905c36"}}, "hash": "bede03a8e13b77bcc7059000fcb0f24e675140d2dc2ab30ee94ff9b6980b8087", "text": "A Few Brief Notes on DeepImpact, COIL, and a ConceptualFramework for Information Retrieval TechniquesJimmy Lin andXueguang MaDavid R. Cheriton School of Computer ScienceUniversity of WaterlooAbstractRecent developments in representational learn-ing for information retrieval can be organizedin a conceptual framework that establishes twopairs of contrasts: sparse vs. dense representa-tions and unsupervised vs. learned representa-tions. Sparse learned representations can fur-ther be decomposed into expansion and termweighting components. This framework al-lows us to understand the relationship betweenrecently proposed techniques such as DPR,ANCE, DeepCT, DeepImpact, and COIL, andfurthermore, gaps revealed by our analysispoint to \u201clow hanging fruit\u201d in terms of tech-niques that have yet to be explored. We presenta novel technique dubbed \u201cuniCOIL\u201d, a simpleextension of COIL that achieves to our knowl-edge the current state-of-the-art in sparse re-trieval on the popular MS MARCO passageranking dataset. Our implementation usingthe Anserini IR toolkit is built on the Lucenesearch library and thus fully compatible withstandard inverted indexes.1 IntroductionWe present a novel conceptual framework for un-derstanding recent developments in information re-trieval that organizes techniques along two dimen-sions. The \ufb01rst dimension establishes the contrastbetween sparse and dense vector representationsfor queries and documents.1The second dimen-sion establishes the contrast between unsupervisedand learned (supervised) representations. Figure 1illustrates our framework.Recent proposals for dense retrieval, exempli\ufb01edby DPR (Karpukhin et al., 2020) and ANCE (Xionget al., 2021), but also encompassing many othertechniques (Gao et al., 2021b; Hofst\u00e4tter et al.,2020; Qu et al., 2021; Hofst\u00e4tter et al., 2021; Lin1Consistent with parlance in information retrieval, we use\u201cdocument\u201d throughout this paper in a generic sense to refer tothe unit of retrieved text. To be more precise, our experimentsare in fact focused on passage retrieval.Dense SparseSupervised DPR, ANCE DeepImpact, COILUnsupervised LSI, LDA BM25, tf\u2013idfTable 1: Our conceptual framework for organizing re-cent developments in information retrieval.et al., 2021), can be understood as learned denserepresentations for retrieval. This is formulatedas a representational learning problem where thetask is to learn (transformer-based) encoders thatmap queries and documents into dense \ufb01xed-widthvectors (768 dimensions is typical) in which innerproducts between queries and relevant documentsare maximized, based on supervision signals froma large dataset such as the MS MARCO passageranking test collection (Bajaj et al., 2018). See Linet al. (2020) for a survey.Dense retrieval techniques are typically com-pared against a bag-of-words exact match rankingmodel such as BM25, which in this context can beunderstood as unsupervised sparse retrieval. Al-though it may be unnatural to describe BM25 inthis way, it is technically accurate: each documentis represented by a sparse vector where each dimen-sion corresponds to a unique term in the vocabulary,and the scoring function assigns a weight to each di-mension. As with dense retrieval, query\u2013documentscores are computed via inner products.What about learned sparse retrieval? The mostprominent recent example of this in the literatureis DeepCT (Dai and Callan, 2019), which usesa transformer to learn term weights based on a re-gression model, with the supervision signal comingfrom the MS MARCO passage ranking test collec-tion.2DeepCT has an interesting \u201cquirk\u201d: in truth,it only learns the term frequency (tf) componentof term weights, but still relies on the remaining2Learning sparse representations is by no means a new idea.The earliest example we are aware of is Wilbur (2001), whoattempted to learn", "start_char_idx": 0, "end_char_idx": 3835, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "55b0b76e-94f8-4fb6-a951-0e4ae2da4530": {"__data__": {"id_": "55b0b76e-94f8-4fb6-a951-0e4ae2da4530", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4dc5e6ce-203f-481b-9cc5-8ed7ea0ee624", "node_type": null, "metadata": {}, "hash": "ef1f7cf17c81b933bb5a75799733b83b904bbec2dd7a41e91a3b7263684da1b1"}, "2": {"node_id": "ca23bbc0-f0b4-4662-9f00-07cb35a6fca0", "node_type": null, "metadata": {}, "hash": "bede03a8e13b77bcc7059000fcb0f24e675140d2dc2ab30ee94ff9b6980b8087"}, "3": {"node_id": "5f43d60b-e4f3-4270-ac71-274b0e43925f", "node_type": null, "metadata": {}, "hash": "67893cf171586522c2b2e011dbc0b56b62bd6b8c55b35bcc598a8b75d40dacc1"}}, "hash": "8a2217dcb14e82871e8709efb9cdb64bf3b825ab9df84169bd07ee8080905c36", "text": "earliest example we are aware of is Wilbur (2001), whoattempted to learn global term weights using TREC data, butthe idea likely dates back even further.arXiv:2106.14807v1  [cs.IR]  28 Jun 2021parts of the BM25 scoring function via the gen-eration of pseudo-documents. This approach alsohas a weakness: it only assigns weights to termsthat are already present in the document, whichlimits retrieval to exact match. This is an impor-tant limitation that is addressed by the use of denserepresentations, which are capable of capturing se-mantic matches.These two issues were resolved by the recentlyproposed DeepImpact model (Mallia et al., 2021),which also belongs in the family of learned sparserepresentations. DeepImpact brought together twokey ideas: the use of document expansion to iden-tify dimensions in the sparse vector that shouldhave non-zero weights and a term weighting modelbased on a pairwise loss between relevant and non-relevant texts with respect to a query. Expansionterms were identi\ufb01ed by doc2query\u2013T5 (Nogueiraand Lin, 2019), a sequence-to-sequence model fordocument expansion that predicts queries for whicha text would be relevant. Since the DeepImpactscoring model directly predicts term weights thatare then quantized, it would be more accurate tocall these weights learned impacts, since query\u2013document scores are simply the sum of weights ofdocument terms that are found in the query. Callingthese impact scores draws an explicit connection toa thread of research in information retrieval datingback two decades (Anh et al., 2001).The recently proposed COIL architecture (Gaoet al., 2021a) presents an interesting case for thisconceptual framework. Where does it belong? Theauthors themselves describe COIL as \u201ca new ex-act lexical match retrieval architecture armed withdeep LM representations\u201d. COIL produces repre-sentations for each document token that are thendirectly stored in the inverted index, where theterm frequency usually goes in an inverted list.Although COIL is perhaps best described as theintellectual descendant of ColBERT (Khattab andZaharia, 2020), another way to think about it withinour conceptual framework is that instead of assign-ingscalar weights to terms in a query, the \u201cscoring\u201dmodel assigns each term a vector \u201cweight\u201d. Queryevaluation in COIL involves accumulating innerproducts instead of scalar weights.Our conceptual framework highlights a \ufb01nalclass of techniques: unsupervised dense represen-tations. While there is little work in this space oflate, it does describe techniques such as LSI (Deer-wester et al., 1990; Atreya and Elkan, 2010) andLDA (Wei and Croft, 2006), which have been previ-ously explored. Thus, all quadrants in our proposedconceptual framework are populated with knownexamples from the literature.2 Comments and ObservationsBased on this framework, we can make a number ofinteresting observations that highlight obvious nextsteps in the development of retrieval techniques.We discuss as follows:Choice of bases. Retrieval techniques using learneddense representations and learned sparse represen-tations present an interesting contrast. Nearly allrecent proposals take advantage of transformers, sothat aspect of the design is not a salient difference.The critical contrast is the basis of the vector rep-resentations: In sparse approaches, the basis of thevector space remains \ufb01xed to the corpus vocabulary,and thus techniques such as DeepCT, COIL, andDeepImpact can be understood as term weightingmodels. In dense approaches, the model is giventhe freedom to choose a new basis derived fromtransformer representations. This change in basisallows the encoder to represent the \u201cmeaning\u201d oftexts in relatively small \ufb01xed-width vectors (com-pared to sparse vectors that may have millions ofdimensions). This leads us to the next importantobservation:Expansions for sparse representation. Withoutsome form of expansion, learned sparse represen-tations remain limited to", "start_char_idx": 3775, "end_char_idx": 7728, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5f43d60b-e4f3-4270-ac71-274b0e43925f": {"__data__": {"id_": "5f43d60b-e4f3-4270-ac71-274b0e43925f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4dc5e6ce-203f-481b-9cc5-8ed7ea0ee624", "node_type": null, "metadata": {}, "hash": "ef1f7cf17c81b933bb5a75799733b83b904bbec2dd7a41e91a3b7263684da1b1"}, "2": {"node_id": "55b0b76e-94f8-4fb6-a951-0e4ae2da4530", "node_type": null, "metadata": {}, "hash": "8a2217dcb14e82871e8709efb9cdb64bf3b825ab9df84169bd07ee8080905c36"}, "3": {"node_id": "61664e49-76ac-415f-a515-a427d5e69e5b", "node_type": null, "metadata": {}, "hash": "ca59a54ee793fa7ac18c7a5fa1ab6602b660ac920dcaaea5f8216fae77743807"}}, "hash": "67893cf171586522c2b2e011dbc0b56b62bd6b8c55b35bcc598a8b75d40dacc1", "text": "form of expansion, learned sparse represen-tations remain limited to (better) exact matchingbetween queries and documents. The nature ofsparse representations means that it is impracticalto consider non-zero weights for allelements inthe vector (i.e., the vocabulary space). Thus, docu-ment expansion serves the critical role of proposinga set of candidate terms that should receive non-zero weights; since the number of candidate termsis small compared to the vocabulary size, the re-sulting vector remains sparse. Without expansion,learned sparse representations cannot address thevocabulary mismatch problem (Furnas et al., 1987),because document terms not present in the querycannot contribute any score. For DeepImpact, thisexpansion is performed by doc2query\u2013T5, but inprinciple we can imagine other methods also. Thisleads us to the next important observation:Relating DeepCT, DeepImpact, and COIL. The up-shot of the above analysis is that retrieval tech-niques based on learned sparse representationsshould be divided into an expansion model andSparse Representations MRR@10 NotesTerm Weighting Expansion(1a) BM25 None 0.184 copied from (Nogueira and Lin, 2019)(1b) BM25 doc2query\u2013T5 0.277 copied from (Nogueira and Lin, 2019)(2a) DeepCT None 0.243 copied from (Dai and Callan, 2019)(2b) DeepCT doc2query\u2013T5 ? no publicly reported \ufb01gure(2c) DeepImpact None ? no publicly reported \ufb01gure(2d) DeepImpact doc2query\u2013T5 0.326 copied from (Mallia et al., 2021)(2e) COIL-tok ( d= 32 ) None 0.341 copied from (Gao et al., 2021a)(2f) COIL-tok ( d= 32 ) doc2query\u2013T5 0.361 our experiment(2g) uniCOIL None 0.315 our experiment(2h) uniCOIL doc2query\u2013T5 0.352 our experimentDense Representations MRR@10 Notes(3a) ColBERT 0.360 copied from (Khattab and Zaharia, 2020)(3b) ANCE 0.330 copied from (Xiong et al., 2021)(3c) DistillBERT 0.323 copied from (Hofst\u00e4tter et al., 2020)(3d) RocketQA 0.370 copied from (Qu et al., 2021)(3e) TAS-B 0.347 copied from (Hofst\u00e4tter et al., 2021)(3f) TCT-ColBERTv2 0.359 copied from (Lin et al., 2021)Dense\u2013Sparse Hybrids MRR@10 Notes(4a) CLEAR 0.338 copied from (Gao et al., 2021b)(4b) COIL-full 0.355 copied from (Gao et al., 2021a)(4c) TCT-ColBERTv2 + BM25 (1a) 0.369 copied from (Lin et al., 2021)(4d) TCT-ColBERTv2 + doc2query\u2013T5 (1b) 0.375 copied from (Lin et al., 2021)(4e) TCT-ColBERTv2 + DeepImpact (2d) 0.378 our experiment(4f) TCT-ColBERTv2 + uniCOIL (2h) 0.378 our experiment(4g) TCT-ColBERTv2 + COIL (2f) 0.382 our experimentTable 2: Results on the development queries of the MS MARCO passage ranking task.a term weighting model. For example, DeepCTperforms no expansion and uses a regression-basedscoring model. DeepImpact performs document ex-pansion and uses a pairwise scoring model. COILperforms no expansion and uses a \u201cscoring\u201d modelthat generates a contextualized \u201cweight vector\u201d (in-stead of a scalar weight). This breakdown suggestsa number of obvious experiments that help us un-derstand the contributions of these components,which we report next.3 ExperimentsOur proposed conceptual framework can be usedto organize results from the literature, which areshown in Table 2 on the development", "start_char_idx": 7730, "end_char_idx": 10870, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "61664e49-76ac-415f-a515-a427d5e69e5b": {"__data__": {"id_": "61664e49-76ac-415f-a515-a427d5e69e5b", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4dc5e6ce-203f-481b-9cc5-8ed7ea0ee624", "node_type": null, "metadata": {}, "hash": "ef1f7cf17c81b933bb5a75799733b83b904bbec2dd7a41e91a3b7263684da1b1"}, "2": {"node_id": "5f43d60b-e4f3-4270-ac71-274b0e43925f", "node_type": null, "metadata": {}, "hash": "67893cf171586522c2b2e011dbc0b56b62bd6b8c55b35bcc598a8b75d40dacc1"}, "3": {"node_id": "efebbb85-ec13-4515-9c24-78d3590af7ac", "node_type": null, "metadata": {}, "hash": "d369b17c449c0a36941263d1d0c40656e5543f9a155b57739c0ebdf629b7c42a"}}, "hash": "ca59a54ee793fa7ac18c7a5fa1ab6602b660ac920dcaaea5f8216fae77743807", "text": "usedto organize results from the literature, which areshown in Table 2 on the development queries ofthe MS MARCO passage ranking task (Bajaj et al.,2018). Some of these entries represent \ufb01gures di-rectly copied from previous papers (with referencesshown), while others are novel experimental condi-tions that we report.The \ufb01rst main block of the table shows retrievalwith sparse representations. Row (1a) shows theBM25 baseline, and row (1b) provides the effective-ness of doc2query\u2013T5 expansion. In both cases, theterm weights are from the BM25 scoring function,and hence unsupervised. Learned sparse retrievaltechniques are shown in row group (2). Separat-ing the term weighting component from the ex-pansion component allows us to identify gaps inmodel con\ufb01gurations that would be interesting toexplore. For example, in row (2a), DeepCT pro-posed a regression-based term weighting model,but performed no expansion. However, the termweighting model can be applied to expanded doc-uments, as in row (2b); to our knowledge, thiscon\ufb01guration has not been publicly reported.Similarly, DeepImpact combined doc2query\u2013T5as an expansion model and a term weighting modeltrained with pairwise loss. To better understandthe contributions of each component, we couldrun the term weighting model without documentexpansion, as outlined in row (2c). This ablationexperiment was not reported in Mallia et al. (2021),but would be interesting to conduct.In row (2e) we report the published results ofCOIL-tok (token dimension d= 32 ), which is thesparse component in the full COIL model (whichis a dense\u2013sparse hybrid). Through the lens ofour conceptual framework, a number of extensionsbecome immediately obvious. COIL can be com-bined with doc2query\u2013T5. Using source code pro-vided by the authors,3we trained such a modelfrom scratch, using the same hyperparameters asthe authors. This variant leads to a nearly two-pointgain in effectiveness, as shown in row (2f).In another interesting extension, if we reduce thetoken dimension of COIL to one, the model degen-erates into producing scalar weights, which thenbecomes directly comparable to DeepCT, row (2a)and the \u201cno-expansion\u201d variant of DeepImpact, row(2c). These comparisons isolate the effects of differ-ent term weighting models. We dub this variant ofCOIL \u201cuniCOIL\u201d, on top of which we can also adddoc2query\u2013T5, which produces a fair comparisonto DeepImpact, row (2d). The original formulationof COIL, even with a token dimension of one, isnot directly amenable to retrieval using invertedindexes because weights can be negative. To ad-dress this issue, we added a ReLU operation onthe output term weights of the base COIL model toforce the model to generate non-negative weights.Once again, we retrained the model from scratchusing the same hyperparameters provided by theauthors. When encoding the corpus, we quantizedthese weights into 8 bits to obtain impact scores;query weights are similarly quantized. After thesemodi\ufb01cations, uniCOIL is directly compatible withinverted indexes. Our experimental results are re-ported with the Anserini toolkit (Yang et al., 2017,2018), which is built on Lucene.It is no surprise that uniCOIL without doc2query\u2013T5, row (2g), is less effective than COIL-tok ( d=32), row (2e). However, uniCOIL with doc2query\u2013T5, row (2h), outperforms COIL-tok without need-ing any specialized retrieval infrastructure\u2014theweights are just impact scores, like in DeepImpact.These results suggest that contextualized \u201cweightvectors\u201d in COIL aren\u2019t necessary to achieve goodeffectiveness\u2014adding expansion appears suf\ufb01cientto make up for the lost expressivity of weight vec-tors, as shown in row (2h) vs. row (2e). To ourknowledge, our uniCOIL model, row", "start_char_idx": 10855, "end_char_idx": 14573, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "efebbb85-ec13-4515-9c24-78d3590af7ac": {"__data__": {"id_": "efebbb85-ec13-4515-9c24-78d3590af7ac", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4dc5e6ce-203f-481b-9cc5-8ed7ea0ee624", "node_type": null, "metadata": {}, "hash": "ef1f7cf17c81b933bb5a75799733b83b904bbec2dd7a41e91a3b7263684da1b1"}, "2": {"node_id": "61664e49-76ac-415f-a515-a427d5e69e5b", "node_type": null, "metadata": {}, "hash": "ca59a54ee793fa7ac18c7a5fa1ab6602b660ac920dcaaea5f8216fae77743807"}, "3": {"node_id": "82f74e8a-c8c2-4ab9-b558-f3ea8c8dd10c", "node_type": null, "metadata": {}, "hash": "56c530e461f94e99b51424737c489b6dc7449bd06acd68b37494021a970bede0"}}, "hash": "d369b17c449c0a36941263d1d0c40656e5543f9a155b57739c0ebdf629b7c42a", "text": "vs. row (2e). To ourknowledge, our uniCOIL model, row (2h), repre-sents the state of the art in sparse retrieval usinglearned impact weights, beating DeepImpact byaround two points.The second main block of Table 2 provides anumber of comparable dense retrieval results fromthe literature. The highest score that we are awareof is RocketQA (Qu et al., 2021), whose effective-ness beats all known sparse con\ufb01gurations. Note3https://github.com/luyug/COILthat ColBERT (Khattab and Zaharia, 2020) usesthe more expressive MaxSim operator to comparequery and document representations; all other tech-niques use inner products.The \ufb01nal block of Table 2 presents the results ofdense\u2013sparse hybrids. Lin et al. (2021) reportedthe results of dense\u2013sparse hybrids when TCT-ColBERTv2, row (3f), is combined with BM25,row (1a), and doc2query\u2013T5, row (1b). To this,we added fusion with DeepImpact, uniCOIL, andCOIL-tok (d= 32 ). For a fair comparison, we fol-lowed the same technique for combining dense andsparse results as Lin et al. (2021), which is from Maet al. (2021). For each query q, we used the corre-sponding dense and sparse techniques to retrievetop-1k documents. The \ufb01nal fusion score of eachdocument is calculated by sdense +\u000b\u0001ssparse . Sincethe range of the two different scores are quite differ-ent, we \ufb01rst normalized the scores into range(0, 1).The\u000bwas tuned in the range(0, 2) with a simpleline search on a subset of the MS MARCO passagetraining set.With these hybrid combinations, we are ableto achieve, to our knowledge, the highest reportedscores on the MS MARCO passage ranking task forsingle-stage techniques (i.e., no reranking). Notethat, as before, uniCOIL is compatible with stan-dard inverted indexes, unlike COIL-tok, which re-quires custom infrastructure.4 Next StepsIn most recent work, dense retrieval techniques arecompared to BM25 and experiments show that theyhandily win. However, this is not a fair compari-son, since BM25 is unsupervised, whereas denseretrieval techniques exploit supervised relevancesignals from large datasets. A more appropriatecomparison would be between learned dense vs.sparse representations\u2014and there, no clear win-ner emerges at present. However, it seems clearthat they are complementary, as hybrid approachesappear to be more effective than either alone.An important point to make here is that neu-ral networks, particularly transformers, have notmade sparse representations obsolete. Both denseand sparse learned representations clearly exploittransformers\u2014the trick is that the latter class oftechniques then \u201cprojects\u201d the learned knowledgeback into the sparse vocabulary space. This al-lows us to reuse decades of innovation in invertedindexes (e.g., integer coding techniques to com-press inverted lists) and ef\ufb01cient query evaluationalgorithms (e.g., smart skipping to reduce querylatency): for example, the Lucene index used inour uniCOIL experiments is only 1.3 GB, com-pared to \u001840 GB for COIL-tok, 26 GB for TCT-ColBERTv2, and 154 GB for ColBERT. We note,however, that with dense retrieval techniques, \ufb01xed-width vectors can be approximated with binaryhash codes, yielding far more compact representa-tions with sacri\ufb01cing much effectiveness (Yamadaet al., 2021). Once again, no clear winner emergesat present.The complete design space of modern informa-tion retrieval techniques requires proper accountingof the tradeoffs between output quality (effective-ness), time (query latency), and space (index size).Here, we have only focused on the \ufb01rst aspect.Learned representations for information retrievalare clearly the", "start_char_idx": 14605, "end_char_idx": 18187, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "82f74e8a-c8c2-4ab9-b558-f3ea8c8dd10c": {"__data__": {"id_": "82f74e8a-c8c2-4ab9-b558-f3ea8c8dd10c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4dc5e6ce-203f-481b-9cc5-8ed7ea0ee624", "node_type": null, "metadata": {}, "hash": "ef1f7cf17c81b933bb5a75799733b83b904bbec2dd7a41e91a3b7263684da1b1"}, "2": {"node_id": "efebbb85-ec13-4515-9c24-78d3590af7ac", "node_type": null, "metadata": {}, "hash": "d369b17c449c0a36941263d1d0c40656e5543f9a155b57739c0ebdf629b7c42a"}, "3": {"node_id": "01b25f4b-c875-474d-8f30-c0239c28dd84", "node_type": null, "metadata": {}, "hash": "c3a151cd4db0272b186dea9c833202fc47d5fea17f9ea4188596951471bb1eb1"}}, "hash": "56c530e461f94e99b51424737c489b6dc7449bd06acd68b37494021a970bede0", "text": "aspect.Learned representations for information retrievalare clearly the future, but the advantages and dis-advantages of dense vs. sparse approaches alongthese dimensions are not yet fully understood. It\u2019llbe exciting to see what comes next!5 AcknowledgmentsThis research was supported in part by the CanadaFirst Research Excellence Fund and the Natural Sci-ences and Engineering Research Council (NSERC)of Canada. Computational resources were providedby Compute Ontario and Compute Canada.ReferencesV o Ngoc Anh, Owen de Kretser, and Alistair Moffat.2001. Vector-space ranking with effective early ter-mination. In Proceedings of the 24th Annual Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR 2001) ,pages 35\u201342, New Orleans, Louisiana.Avinash Atreya and Charles Elkan. 2010. Latent se-mantic indexing (LSI) fails for TREC collections.SIGKDD Explorations , 12(2):5\u201310.Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng,Jianfeng Gao, Xiaodong Liu, Rangan Majumder,Andrew McNamara, Bhaskar Mitra, Tri Nguyen,Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Ti-wary, and Tong Wang. 2018. MS MARCO: A Hu-man Generated MAchine Reading COmprehensionDataset. arXiv:1611.09268v3 .Zhuyun Dai and Jamie Callan. 2019. Context-awaresentence/passage term importance estimation for\ufb01rst stage retrieval. arXiv:1910.10687 .Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman. 1990.Indexing by latent semantic analysis. Journal ofthe Association for Information Science , 41(6):391\u2013407.George W. Furnas, Thomas K. Landauer, Louis M.Gomez, and Susan T. Dumais. 1987. The vo-cabulary problem in human-system communication.Communications of the ACM , 30(11):964\u2013971.Luyu Gao, Zhuyun Dai, and Jamie Callan. 2021a.COIL: Revisit exact lexical match in informationretrieval with contextualized inverted list. In Pro-ceedings of the 2021 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies , pages3030\u20133042.Luyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Ben-jamin Van Durme, and Jamie Callan. 2021b. Com-plementing lexical retrieval with semantic residualembedding. In Proceedings of the 43rd EuropeanConference on Information Retrieval (ECIR 2021),Part I , pages 146\u2013160.Sebastian Hofst\u00e4tter, Sophia Althammer, MichaelSchr\u00f6der, Mete Sertkan, and Allan Hanbury.2020. Improving ef\ufb01cient neural ranking mod-els with cross-architecture knowledge distillation.arXiv:2010.02666 .Sebastian Hofst\u00e4tter, Sheng-Chieh Lin, Jheng-HongYang, Jimmy Lin, and Allan Hanbury. 2021. Ef-\ufb01ciently teaching an effective dense retriever withbalanced topic aware sampling. In Proceedings ofthe 44th Annual International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval (SIGIR 2021) .Vladimir Karpukhin, Barlas O \u02d8guz, Sewon Min, PatrickLewis, Ledell Wu, Sergey Edunov, Danqi Chen, andWen-tau Yih. 2020. Dense passage retrieval foropen-domain question answering. In Proceedings ofthe 2020 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP) , pages 6769\u20136781.Omar Khattab and Matei Zaharia. 2020. ColBERT: Ef-\ufb01cient and effective passage search via contextual-ized late interaction over", "start_char_idx": 18168, "end_char_idx": 21404, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "01b25f4b-c875-474d-8f30-c0239c28dd84": {"__data__": {"id_": "01b25f4b-c875-474d-8f30-c0239c28dd84", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4dc5e6ce-203f-481b-9cc5-8ed7ea0ee624", "node_type": null, "metadata": {}, "hash": "ef1f7cf17c81b933bb5a75799733b83b904bbec2dd7a41e91a3b7263684da1b1"}, "2": {"node_id": "82f74e8a-c8c2-4ab9-b558-f3ea8c8dd10c", "node_type": null, "metadata": {}, "hash": "56c530e461f94e99b51424737c489b6dc7449bd06acd68b37494021a970bede0"}}, "hash": "c3a151cd4db0272b186dea9c833202fc47d5fea17f9ea4188596951471bb1eb1", "text": "and effective passage search via contextual-ized late interaction over BERT. In Proceedings ofthe 43rd International ACM SIGIR Conference onResearch and Development in Information Retrieval(SIGIR 2020) , pages 39\u201348.Jimmy Lin, Rodrigo Nogueira, and Andrew Yates.2020. Pretrained transformers for text ranking:BERT and beyond. arXiv:2010.06467 .Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin.2021. In-batch negatives for knowledge distillationwith tightly-coupled teachers for dense retrieval. InProceedings of the 6th Workshop on RepresentationLearning for NLP .Xueguang Ma, Kai Sun, Ronak Pradeep, and JimmyLin. 2021. A replication study of dense passage re-triever. arXiv:2104.05740 .Antonio Mallia, Omar Khattab, Torsten Suel, andNicola Tonellotto. 2021. Learning passage impactsfor inverted indexes. In Proceedings of the 44th An-nual International ACM SIGIR Conference on Re-search and Development in Information Retrieval(SIGIR 2021) .Rodrigo Nogueira and Jimmy Lin. 2019. Fromdoc2query to docTTTTTquery.Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, RuiyangRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu,and Haifeng Wang. 2021. RocketQA: An opti-mized training approach to dense passage retrievalfor open-domain question answering. In Proceed-ings of the 2021 Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics: Human Language Technologies , pages5835\u20135847.Xing Wei and W. Bruce Croft. 2006. LDA-based doc-ument models for ad-hoc retrieval. In Proceedingsof the 29th Annual International ACM SIGIR Con-ference on Research and Development in Informa-tion Retrieval (SIGIR 2006) , pages 178\u2013185, Seattle,Washington.W. John Wilbur. 2001. Global term weights for docu-ment retrieval learned from TREC data. Journal ofInformation Science , 27(5):303\u2013310.Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,Jialin Liu, Paul N. Bennett, Junaid Ahmed, andArnold Overwijk. 2021. Approximate nearest neigh-bor negative contrastive learning for dense text re-trieval. In Proceedings of the 9th International Con-ference on Learning Representations (ICLR 2021) .Ikuya Yamada, Akari Asai, and Hannaneh Ha-jishirzi. 2021. Ef\ufb01cient passage retrieval withhashing for open-domain question answering.arXiv:2106.00882 .Peilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini:enabling the use of Lucene for information retrievalresearch. In Proceedings of the 40th Annual Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR 2017) ,pages 1253\u20131256, Tokyo, Japan.Peilin Yang, Hui Fang, and Jimmy Lin. 2018. Anserini:reproducible ranking baselines using Lucene. Jour-nal of Data and Information Quality , 10(4):Article16.", "start_char_idx": 21399, "end_char_idx": 24071, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "09444cf9-5cf7-43c1-916c-ed03555a5041": {"__data__": {"id_": "09444cf9-5cf7-43c1-916c-ed03555a5041", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b054b4cf-4c7d-4fcc-8ed7-e2599a3213e1", "node_type": null, "metadata": {}, "hash": "28a1854ff1025f315a96541ba6917b8879f90b9da27a72400bc608490b342d2a"}, "3": {"node_id": "ca504482-fe85-4ae5-af0d-016753d626e2", "node_type": null, "metadata": {}, "hash": "fb31edb9402754c973cb729939b0aaeb3899f16f84eabc14646f88390470b1ba"}}, "hash": "ac1a376da03a81025c65ad653730b11baa6e169e7b2678e2a44ca81171316a01", "text": "Doc2Query--: When Less is MoreMitko Gospodinov1, Sean MacAvaney2, and Craig Macdonald2University of Glasgow12024810G@student.gla.ac.uk2{first}.{last}@glasgow.ac.ukAbstract. Doc2Query \u2014 the process of expanding the content of adocument before indexing using a sequence-to-sequence model \u2014 hasemerged as a prominent technique for improving the \ufb01rst-stage retrievale\ufb00ectivenessofsearchengines.However,sequence-to-sequencemodelsareknown to be prone to \u201challucinating\u201d content that is not present in thesource text. We argue that Doc2Query is indeed prone to hallucination,which ultimately harms retrieval e\ufb00ectiveness and in\ufb02ates the index size.In this work, we explore techniques for \ufb01ltering out these harmful queriesprior to indexing. We \ufb01nd that using a relevance model to remove poor-quality queries can improve the retrieval e\ufb00ectiveness of Doc2Query byup to 16%, while simultaneously reducing mean query execution time by23% and cutting the index size by 33%. We release the code, data, anda live demonstration to facilitate reproduction and further exploration.11 IntroductionNeural network models, particularly those based on contextualised languagemodels, have been shown to improve search e\ufb00ectiveness [3]. While some ap-proaches focus on re-ranking document sets from a \ufb01rst-stage retrieval functionto improve precision [27], others aim to improve the \ufb01rst stage itself [4]. In thiswork, we focus on one of these \ufb01rst-stage approaches: Doc2Query [29]. This ap-proach trains a sequence-to-sequence model (e.g., T5 [33]) to predict queries thatmay be relevant to a particular text. Then, when indexing, this model is usedtoexpandthe document by generating a collection of queries and appendingthem to the document. Though computationally expensive at index time [34],this approach has been shown to be remarkably e\ufb00ective even when retrievingusing simple lexical models like BM25 [28]. Numerous works have shown thatthe approach can produce a high-quality pool of results that are e\ufb00ective forsubsequent stages in the ranking pipeline [19,20,23,40].However, sequence-to-sequence models are well-known to be prone to gener-ate content that does not re\ufb02ect the input text \u2013 a defect known in literatureas \u201challucination\u201d [25]. We \ufb01nd that existing Doc2Query models are no excep-tion. Figure 1 provides example generated queries from the state-of-the-art T5Doc2Query model [28]. In this example, we see that many of the generatedqueries cannot actually be answered by the source passage (score \u00141).1https://github.com/terrierteam/pyterrier_doc2queryarXiv:2301.03266v3  [cs.IR]  27 Feb 20232 Gospodinov et al.Original Passage: Barley (Hordeum vulgare L.), amember of the grass family, is a major cereal grain. Itwas one of the \ufb01rst cultivated grains and is now grownwidely. Barley grain is a staple in Tibetan cuisine andwaseatenwidelybypeasantsinMedievalEurope.Bar-ley has also been used as animal fodder, as a sourceof fermentable material for beer and certain distilledbeverages,andasacomponentofvarioushealthfoods.Generated Queries: (1) where does barley originatefrom\u0001(2) what is the name of the cereal grain usedin tibetan cooking? \u0001(3) what is barley used for \u0001(1)what is barley in food \u0001(0) what is bare wheat \u0001(3)what family of organisms is barley in \u0001(1) why is bar-ley important in tibetan diet \u0001(3) what is barley \u0001(2) where is barley grown \u0001(1) where was barley \ufb01rstgrown and eaten \u0001(1) where was barley \ufb01rst used ...Fig.", "start_char_idx": 0, "end_char_idx": 3436, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ca504482-fe85-4ae5-af0d-016753d626e2": {"__data__": {"id_": "ca504482-fe85-4ae5-af0d-016753d626e2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b054b4cf-4c7d-4fcc-8ed7-e2599a3213e1", "node_type": null, "metadata": {}, "hash": "28a1854ff1025f315a96541ba6917b8879f90b9da27a72400bc608490b342d2a"}, "2": {"node_id": "09444cf9-5cf7-43c1-916c-ed03555a5041", "node_type": null, "metadata": {}, "hash": "ac1a376da03a81025c65ad653730b11baa6e169e7b2678e2a44ca81171316a01"}, "3": {"node_id": "216330e4-a90e-4a22-b0fa-e98462b7a588", "node_type": null, "metadata": {}, "hash": "d653bda647f000739d71c91c365d85b0afd26d128e7024e72778cd99eb99d069"}}, "hash": "fb31edb9402754c973cb729939b0aaeb3899f16f84eabc14646f88390470b1ba", "text": "and eaten \u0001(1) where was barley \ufb01rst used ...Fig. 1.Example passage from MS MARCO and generated queries using the T5Doc2Query model. The relevance of each query to the passage is scored by the au-thors on a scale of 0\u20133 using the TREC Deep Learning passage relevance criteria.Based on this observation, we hypothesise that retrieval performance ofDoc2Querywouldimproveifhallucinatedquerieswereremoved.Inthispaper,weconduct experiments where we apply a new \ufb01ltering phase that aims to removepoor queries prior to indexing. Given that this approach removes queries, wecall the approach Doc2Query-- (Doc2Query-minus-minus). Rather than traininga new model for this task, we identify that relevance models are already \ufb01t forthis purpose: they estimate how relevant a passage is to a query. We thereforeexplore \ufb01ltering strategies that make use of existing neural relevance models.Through experimentation on the MS MARCO dataset, we \ufb01nd that our \ufb01l-tering approach can improve the retrieval e\ufb00ectiveness of indexes built usingDoc2Query-- by up to 16%; less can indeed be more. Meanwhile, \ufb01ltering nat-urally reduces the index size, lowering storage and query-time computationalcosts. Finally, we conduct an exploration of the index-time overheads introducedbythe\ufb01lteringprocessandconcludethatthegainsfrom\ufb01lteringmorethanmakeup for the additional time spent generating more queries. The approach also hasa positive impact on the environmental costs of applying Doc2Query; the sameretrieval e\ufb00ectiveness can be achieved with only about a third of the compu-tational cost when indexing. To facilitate last-metre, last-mile, and completereproduction e\ufb00orts [36], we release the code, indices, and \ufb01ltering scores.1Insummary, we contribute a technique to improve the e\ufb00ectiveness and e\ufb03ciencyof Doc2Query by \ufb01ltering out queries that do not re\ufb02ect the original passage.2 Related WorkThe classical lexical mismatch problem is a key one in information retrieval -documents that do not contain the query terms may not be retrieved. In theliterature, various approaches have addressed this: query reformulation \u2013 includ-ing stemming, query expansion models (e.g. Rocchio, Bo1 [1], RM3 [12]) \u2013 anddocument expansion [9,30,35]. Classically, query expansion models have beenpopular, as they avoid the costs associated with making additional processingfor each document needed for document expansion. However, query expansionmay result in reduced performance [11], as queries are typically short and thenecessary evidence to understand the context of the user is limited.Doc2Query--: When Less is More 3The application of latent representations of queries and documents, suchas using latent semantic indexing [8] allow retrieval to not be driven directlyby lexical signals. More recently, transformer-based language models (such asBERT [6]) have resulted in representations of text where the contextualisedmeaning of words are accounted for. In particular, in dense retrieval, queriesand documents are represented in embeddings spaces [14,37], often facilitatedby Approximate Nearest Neighbour (ANN) data structures [13]. However, evenwhen using ANN, retrieval can still be ine\ufb03cient or insu\ufb03ciently e\ufb00ective [15].Others have explored approaches for augmenting lexical representations withadditional terms that may be relevant. In this work, we explore Doc2Query [29],which uses a sequence-to-sequence model that maps a document to queries thatit might be able to answer. By appending these generated queries to a docu-ment\u2019s content before indexing, the document is more likely to be retrieved foruser queries when using a model like BM25. An alternative style of documentexpansion, proposed by MacAvaney et al. [19] and since used by several othermodels (e.g., [10,39,40]),", "start_char_idx": 3396, "end_char_idx": 7153, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "216330e4-a90e-4a22-b0fa-e98462b7a588": {"__data__": {"id_": "216330e4-a90e-4a22-b0fa-e98462b7a588", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b054b4cf-4c7d-4fcc-8ed7-e2599a3213e1", "node_type": null, "metadata": {}, "hash": "28a1854ff1025f315a96541ba6917b8879f90b9da27a72400bc608490b342d2a"}, "2": {"node_id": "ca504482-fe85-4ae5-af0d-016753d626e2", "node_type": null, "metadata": {}, "hash": "fb31edb9402754c973cb729939b0aaeb3899f16f84eabc14646f88390470b1ba"}, "3": {"node_id": "d62b6bf6-a9fa-47f4-8e30-ebcef27b9bd7", "node_type": null, "metadata": {}, "hash": "10abba2b15fe5e56abfbb225bed9a4c9aad8699ec00551391f70647998669388"}}, "hash": "d653bda647f000739d71c91c365d85b0afd26d128e7024e72778cd99eb99d069", "text": "and since used by several othermodels (e.g., [10,39,40]), uses the built-in Masked Language Modelling (MLM)mechanism. MLM expansion generates individual tokens to append to the docu-ment as a bag of words (rather than as a sequence). Although MLM expansion isalso prone to hallucination,2the bag-of-words nature of MLM expansion meansthat individual expansion tokens may not have su\ufb03cient context to apply \ufb01l-tering e\ufb00ectively. We therefore focus only on sequence-style expansion and leavethe exploration of MLM expansion for future work.3 Doc2Query--Doc2Query-- consists of two phases: a generation phrase and a \ufb01ltering phase.In the generation phase, a Doc2Query model generates a set of nqueries thateach document might be able to answer. However, as shown in Figure 1, notall of the queries are necessarily relevant to the document. To mitigate thisproblem, Doc2Query-- then proceeds to a \ufb01ltering phase, which is responsiblefor eliminating the generated queries that are least relevant to the source doc-ument. Because hallucinated queries contain details not present in the originaltext (by de\ufb01nition), we argue that hallucinated queries are less useful for re-trieval than non-hallucinated ones. Filtering is accomplished by retaining onlythe most relevant pproportion of generated queries over the entire corpus. Theretained queries are then concatenated to their corresponding documents priorto indexing, as per the existing Doc2Query approach.More formally, consider an expansion function ethat maps a document to nqueries: e:D7!Qn. In Doc2Query, each document in corpus Dare concate-natedwiththeirexpansionqueries,forminganewcorpus D0=fConcat (d; e(d))jd2Dg,whichisthen indexedbya retrievalsystem.Doc2Query--addsa\ufb01lteringmechanism that uses a relevance model that maps a query and document to areal-valued relevance score s:Q\u0002D7!R(with larger values indicating higher2For instance, we \ufb01nd that SPLADE [10] generates the following seemingly-unrelatedterms for the passage in Figure 1 in the top 20 expansion terms: reed,herb, and troy.4 Gospodinov et al.relevance). The relevance scoring function is used to \ufb01lter down the queries tothose that meet a certain score threshold tas follows:D0=nConcat\u0000d;\bqjq2e(d)^s(q; d)\u0015t\t\u0001jd2Do(1)The relevance threshold tis naturally dependent upon the relevance scoringfunction. It can be set empirically, chosen based on operational criteria (e.g.,targetindexsize),or(forawell-calibratedrelevancescoringfunction)determineda priori. In this work, we combine the \ufb01rst two strategies: we pick tbased onthe distribution of relevance scores across all expansion queries. For instance,atp= 0:3we only keep queries with relevance scores in the top 30%, which ist= 3:215for the ELECTRA [31] scoring model on the MS MARCO dataset [26].4 Experimental SetupWe conduct experiments to answer the following research questions:RQ1 Does Doc2Query-- improve the e\ufb00ectiveness of document expansion?RQ2 What are the trade-o\ufb00s in terms of e\ufb00ectiveness, e\ufb03ciency, and storage whenusing Doc2Query--?Datasets and Measures. We conduct tests using the MS MARCO [26] v1passage corpus. We use \ufb01ve test collections:3(1) the MS MARCO Dev (small)collection, consisting of 6,980 queries (1.1 qrels/query); (2) the Dev2 collection,consisting of 4,281 (1.1 qrels/query); (3) the MS MARCO Eval set, consisting of6,837 queries (held-out leaderboard set);", "start_char_idx": 7145, "end_char_idx": 10509, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d62b6bf6-a9fa-47f4-8e30-ebcef27b9bd7": {"__data__": {"id_": "d62b6bf6-a9fa-47f4-8e30-ebcef27b9bd7", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b054b4cf-4c7d-4fcc-8ed7-e2599a3213e1", "node_type": null, "metadata": {}, "hash": "28a1854ff1025f315a96541ba6917b8879f90b9da27a72400bc608490b342d2a"}, "2": {"node_id": "216330e4-a90e-4a22-b0fa-e98462b7a588", "node_type": null, "metadata": {}, "hash": "d653bda647f000739d71c91c365d85b0afd26d128e7024e72778cd99eb99d069"}, "3": {"node_id": "8bf0243a-df0a-411a-800d-6293f8cfca58", "node_type": null, "metadata": {}, "hash": "ca0f5354cd859b03264c7d67128e787542fb4a7b19dff15505d2612c0317c9a2"}}, "hash": "10abba2b15fe5e56abfbb225bed9a4c9aad8699ec00551391f70647998669388", "text": "set, consisting of6,837 queries (held-out leaderboard set); (4/5) the TREC DL\u201919/\u201920 collections,consisting of 43/54 queries (215/211 qrels/query). We evaluate using the o\ufb03cialtask evaluation measures: Reciprocal Rank at 10 (RR@10) for Dev/Dev2/Eval,nDCG@10 for DL\u201919/\u201920. We tune systems4on Dev, leaving the remaining col-lections as held-out test sets.Models. We use the T5 Doc2Query model from Nogueira and Lin [28], mak-ing use of the inferred queries released by the authors (80 per passage). To thebest of our knowledge, this is the highest-performing Doc2Query model avail-able. We consider three neural relevance models for \ufb01ltering: ELECTRA5[31],MonoT56[32],andTCT-ColBERT7[16],coveringtwostrongcross-encodermod-els and one strong bi-encoder model. We also explored \ufb01lters that use the prob-abilities from the generation process itself but found them to be ine\ufb00ective andtherefore omit these results due to space constraints.Tools and Environment. WeusethePyTerriertoolkit[22]withaPISA[24,17]index to conduct our experiments. We deploy PISA\u2019s Block-Max WAND [7] im-plementation for BM25 retrieval. Inference was conducted on an NVIDIA 3090GPU. Evaluation was conducted using the ir-measures package [18].3ir-datasets [21] IDs: msmarco-passage/dev/small ,msmarco-passage/dev/2 ,msmarco-passage/eval/small , msmarco-passage/trec-dl-2019/judged ,msmarco-passage/trec-dl-2020/judged4BM25\u2019s k1,b, and whether toremove stopwords were tuned for all systems; the \ufb01ltering percentage ( p)was also tuned for \ufb01ltered systems.5crystina-z/monoELECTRA_LCE_nneg316castorini/monot5-base-msmarco7castorini/tct_colbert-v2-hnp-msmarcoDoc2Query--: When Less is More 5Table 1. E\ufb00ectiveness and e\ufb03ciency measurements for Doc2Query-- and baselines.Signi\ufb01cant di\ufb00erences between Doc2Query and their corresponding \ufb01ltered versionsfor Dev, Dev2, DL\u201919 and DL\u201920 are indicated with * (paired t-test, p < 0:05). Valuesmarked withyare taken from the corresponding submissions to the public leaderboard.RR@10 nDCG@10 ms/q GBSystem Dev Dev2 Eval DL\u201919 DL\u201920 MRT IndexBM25 0.185 0.182y0.186 0.499 0.479 5 0.71Doc2Query ( n= 40) 0.277 0.265y0.272 0.626 0.607 30 1.17w/ ELECTRA Filter (30%) *0.316 *0.310 -0.667 0.611 23 0.89w/ MonoT5 Filter (40%) *0.308 *0.298 0.306 0.650 0.611 29 0.93w/ TCT Filter (50%) *0.287 *0.280 - 0.640 0.599 30 0.94Doc2Query ( n= 80) 0.279 0.267 - 0.627 0.605 30 1.41w/ ELECTRA Filter (30%) *0.323 *0.316 0.325 0.670 0.614 23 0.95w/ MonoT5 Filter (40%) *0.311 *0.298 - 0.665 0.609 28 1.04w/ TCT Filter (50%) *0.293 *0.283 - 0.642 0.588 28 1.055 ResultsWe \ufb01rst explore RQ1: whether relevance \ufb01ltering can improve the retrieval ofDoc2Query models. Table 1 compares the e\ufb00ectiveness of Doc2Query with var-ious \ufb01lters.", "start_char_idx": 10507, "end_char_idx": 13223, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8bf0243a-df0a-411a-800d-6293f8cfca58": {"__data__": {"id_": "8bf0243a-df0a-411a-800d-6293f8cfca58", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b054b4cf-4c7d-4fcc-8ed7-e2599a3213e1", "node_type": null, "metadata": {}, "hash": "28a1854ff1025f315a96541ba6917b8879f90b9da27a72400bc608490b342d2a"}, "2": {"node_id": "d62b6bf6-a9fa-47f4-8e30-ebcef27b9bd7", "node_type": null, "metadata": {}, "hash": "10abba2b15fe5e56abfbb225bed9a4c9aad8699ec00551391f70647998669388"}, "3": {"node_id": "b4c87216-d2e8-4abe-926f-65b7f4043ebf", "node_type": null, "metadata": {}, "hash": "2ebd6e77a7431186f812b701efcfb1a52bb378e0a9b2518ae6681916c715ec58"}}, "hash": "ca0f5354cd859b03264c7d67128e787542fb4a7b19dff15505d2612c0317c9a2", "text": "e\ufb00ectiveness of Doc2Query with var-ious \ufb01lters. We observe that all the \ufb01lters signi\ufb01cantly improve the retrievale\ufb00ectiveness on the Dev and Dev2 datasets at both n= 40andn= 80. We alsoobserve a large boost in performance on the Eval dataset.8Though the di\ufb00er-ences in DL\u201919 and DL\u201920 appear to be considerable (e.g., 0.627 to 0.670), thesedi\ufb00erences are not statistically signi\ufb01cant.Diggingalittledeeper,Figure2showstheretrievale\ufb00ectivenessofDoc2Querywith various numbers of generated queries (in dotted black) and the correspond-ing performance when \ufb01ltering using the top-performing ELECTRA scorer (insolid blue). We observe that performing relevance \ufb01ltering at each value of nimproves the retrieval e\ufb00ectiveness. For instance, keeping only 30% of expan-sion queries at n= 80, performance is increased from 0.279 to 0.323 \u2013 a 16%improvement.In aggregate, results from Table 1 and Figure 2 answer RQ1: Doc2Query--\ufb01ltering can signi\ufb01cantly improve the retrieval e\ufb00ectiveness of Doc2Query acrossvarious scoring models, numbers of generated queries ( n) and thresholds ( p).Next,weexplorethetrade-o\ufb00sintermsofe\ufb00ectiveness,e\ufb03ciency,andstoragewhen using Doc2Query--. Table 1 includes the mean response time and indexsizes for each of the settings. As expected, \ufb01ltering reduces the index size sincefewer terms are stored. For the best-performing setting ( n= 80with ELECTRA8Signi\ufb01cance cannot be determined due to the held-out nature of the dataset. Further,due to restrictions on the number of submissions to the leaderboard, we only are ableto submit two runs. The \ufb01rst aims to be a fair comparison with the existing Doc2QueryEval result, using the same number of generated queries and same base T5 model forscoring. The second is our overall best-performing setting, using the ELECTRA \ufb01lteratn= 80generated queries.6 Gospodinov et al.0 1 2 3 4 5Total Tokens 1e90.2250.2500.2750.3000.325RR@1090%80%70%60%50%40% 30%n=5n=10n=20n=40 n=80Generation PhaseFiltering PhaseFig. 2.E\ufb00ectiveness (RR@10) on the Dev set, compared with the total number ofindexed tokens. The generation phase is shown in dotted black (at various values ofn), and the ELECTRA \ufb01ltering phase is shown in solid blue (at various values of p).\ufb01lter), this amounts to a 33% reduction in index size (1.41 GB down to 0.95 GB).Naturally, such a reduction has an impact on query processing time as well; ityields a 23% reduction in mean response time (30ms down to 23ms).Doc2Query-- \ufb01ltering adds substantial cost an indexing time, mostly due toscoring each of the generated queries. Table 2 reports the cost (in hours of GPUtime) of the generation and \ufb01ltering phases. We observe that ELECTRA \ufb01lter-ing can yield up to a 78% increase in GPU time ( n= 10). However, we \ufb01nd thatthe improved e\ufb00ectiveness makes up for this cost. To demonstrate this, we al-locate the time spent \ufb01ltering to generating additional queries for each passage.For instance, the 15 hours spent scoring n= 5queries could instead be spentgenerating 6 more queries per passage (for a total of n= 11). We \ufb01nd that whencomparing against an un\ufb01ltered nthat closely approximates the total time whenTable 2. Retrieval e\ufb00ectiveness comparison for comparable indexing computationalbudgets (in hours", "start_char_idx": 13235, "end_char_idx": 16457, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b4c87216-d2e8-4abe-926f-65b7f4043ebf": {"__data__": {"id_": "b4c87216-d2e8-4abe-926f-65b7f4043ebf", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b054b4cf-4c7d-4fcc-8ed7-e2599a3213e1", "node_type": null, "metadata": {}, "hash": "28a1854ff1025f315a96541ba6917b8879f90b9da27a72400bc608490b342d2a"}, "2": {"node_id": "8bf0243a-df0a-411a-800d-6293f8cfca58", "node_type": null, "metadata": {}, "hash": "ca0f5354cd859b03264c7d67128e787542fb4a7b19dff15505d2612c0317c9a2"}, "3": {"node_id": "c4ebf6a3-5e9b-44b0-9da1-20f7d0aa4cc2", "node_type": null, "metadata": {}, "hash": "6cda9d5d43d427d33eb0faa7acc5f18cf5462e98600a8f061541fa04f3a667b8"}}, "hash": "2ebd6e77a7431186f812b701efcfb1a52bb378e0a9b2518ae6681916c715ec58", "text": "comparison for comparable indexing computationalbudgets (in hours of GPU time). Values of nwithout a \ufb01lter are chosen to best approx-imate the total compute hours or the Dev e\ufb00ectiveness of the corresponding \ufb01lteredversion. Signi\ufb01cant di\ufb00erences between in RR@10 performance are indicated with *(paired t-test, p < 0:05).GPU Hours RR@10nFilter Gen+Filt=Tot Dev Dev2 Comment5 ELECTRA 20 + 15 = 34 0.273 0.27011None 34 + 0 = 34 *0.261 *0.256 \u00004% Dev RR for sim. GPU hrs31None 99 + 0 = 99 0.273 0.265\u00022:9GPU hrs to match Dev RR10 ELECTRA 32 + 25 = 57 0.292 0.29218None 59 + 0 = 59 *0.270 *0.260 \u00008% Dev RR for sim. GPU hrs20 ELECTRA 66 + 47 = 113 0.307 0.30336None 113 + 0 = 113 *0.275 *0.265 \u000010% Dev RR for sim. GPU hrs40 ELECTRA 128 + 86 = 214 0.316 0.31068None 216 + 0 = 216 *0.279 *0.267 \u000012% Dev RR for sim. GPU hrsDoc2Query--: When Less is More 7\ufb01ltering, the \ufb01ltered results consistently yield signi\ufb01cantly higher retrieval e\ufb00ec-tiveness. As the computational budget increases, so does the margin betweenDoc2Query and Doc2Query--, from 4% at 34 hours up to 12% at 216 hours.From the opposite perspective, Doc2Query consumes 2.9 \u0002or more GPUtime than Doc2Query-- to achieve similar e\ufb00ectiveness ( n= 13with no \ufb01ltervs.n= 5with ELECTRA \ufb01lter). Since the e\ufb00ectiveness of Doc2Query \ufb02attensout between n= 40andn= 80(as seen in Figure 2), it likely requires amassive amount of additional compute to reach the e\ufb00ectiveness of Doc2Query--atn\u001510, if that e\ufb00ectiveness is achievable at all. These comparisons show thatif a deployment is targeting a certain level of e\ufb00ectiveness (rather than a targetcompute budget), Doc2Query-- is also preferable to Doc2Query.TheseresultscollectivelyanswerRQ2:Doc2Query--provideshighere\ufb00ective-ness at lower query-time costs, even when controlling for the additional computerequired at index time.6 ConclusionsThisworkdemonstratedthatthereareuntappedadvantagesingeneratingnatural-language for document expansion. Speci\ufb01cally, we presented Doc2Query--, whichisanewapproachforimprovingthee\ufb00ectivenessande\ufb03ciencyoftheDoc2Querymodel by \ufb01ltering out the least relevant queries. We observed that a 16% im-provement in retrieval e\ufb00ectiveness can be achieved, while reducing the indexsize by 33% and mean query execution time by 23%.The technique of \ufb01ltering text generated from language models using rel-evance scoring is ripe for future work. For instance, relevance \ufb01ltering couldpotentially apply to approaches that generate alternative forms of queries [38],training data [2], or natural language responses to queries [5] \u2014 all of whichare potentially a\ufb00ected by hallucinated content. Furthermore, future work couldexplore approaches for relevance \ufb01ltering over masked language modelling ex-pansion [19], rather than sequence-to-sequence expansion.AcknowledgementsSeanMacAvaneyandCraigMacdonaldacknowledgeEPSRCgrantEP/R018634/1:Closed-Loop Data Science for Complex, Computationally- & Data-Intensive An-alytics.References1. Amati, G., Van Rijsbergen, C.J.: Probabilistic models of information retrievalbased on measuring the divergence from randomness. ACM Trans. Inf. Syst. 20(4)(2002)2.", "start_char_idx": 16441, "end_char_idx": 19556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c4ebf6a3-5e9b-44b0-9da1-20f7d0aa4cc2": {"__data__": {"id_": "c4ebf6a3-5e9b-44b0-9da1-20f7d0aa4cc2", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b054b4cf-4c7d-4fcc-8ed7-e2599a3213e1", "node_type": null, "metadata": {}, "hash": "28a1854ff1025f315a96541ba6917b8879f90b9da27a72400bc608490b342d2a"}, "2": {"node_id": "b4c87216-d2e8-4abe-926f-65b7f4043ebf", "node_type": null, "metadata": {}, "hash": "2ebd6e77a7431186f812b701efcfb1a52bb378e0a9b2518ae6681916c715ec58"}, "3": {"node_id": "48402b8e-ac2f-4ce6-ab19-7669795e5c7a", "node_type": null, "metadata": {}, "hash": "2daac1ae27b47c9d7dd129f9ed7e2af9c219a2b214688981d76e9aa8bf2e4d97"}}, "hash": "6cda9d5d43d427d33eb0faa7acc5f18cf5462e98600a8f061541fa04f3a667b8", "text": "randomness. ACM Trans. Inf. Syst. 20(4)(2002)2. Bonifacio,L.,Abonizio,H.,Fadaee,M.,Nogueira,R.:InPars:Unsuperviseddatasetgeneration for information retrieval. In: Proceedings of SIGIR (2022)8 Gospodinov et al.3. Dai, Z., Callan, J.: Deeper text understanding for IR with contextual neural lan-guage modeling. In: Proceedings of SIGIR (2019)4. Dai, Z., Callan, J.: Context-aware document term weighting for ad-hoc search. In:Proceedings of The Web Conference (2020)5. Das, R., Dhuliawala, S., Zaheer, M., McCallum, A.: Multi-step retriever-readerinteraction for scalable open-domain question answering. In: Proceedings of ICLR(2019)6. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deepbidirectional transformers for language understanding. In: Proceedings of NAACL-HLT (2019)7. Ding, S., Suel, T.: Faster top-k document retrieval using block-max indexes. In:Proceedings of SIGIR (2011)8. Dumais, S.T., Furnas, G.W., Landauer, T.K., Deerwester, S., Harshman, R.: Usinglatent semantic analysis to improve access to textual information. In: Proceedingsof SIGCHI CHI (1988)9. Efron, M., Organisciak, P., Fenlon, K.: Improving retrieval of short texts throughdocument expansion. In: Proceedings of SIGIR (2012)10. Formal, T., Piwowarski, B., Clinchant, S.: SPLADE: Sparse lexical and expansionmodel for \ufb01rst stage ranking. In: Proceedings of SIGIR (2021)11. He, B., Ounis, I.: Studying query expansion e\ufb00ectiveness. In: Proceedings of ECIR(2009)12. Jaleel, N.A., Allan, J., Croft, W.B., Diaz, F., Larkey, L.S., Li, X., Smucker, M.D.,Wade, C.: Umass at TREC 2004: Novelty and HARD. In: TREC (2004)13. Johnson, J., Douze, M., Jegou, H.: Billion-scale similarity search with GPUs. IEEETransactions on Big Data 7(03) (2021)14. Khattab, O., Zaharia, M.: ColBERT: E\ufb03cient and e\ufb00ective passage search viacontextualized late interaction over BERT. In: Proceedings of SIGIR (2020)15. Lin, J., Ma, X., Mackenzie, J., Mallia, A.: On the separation of logical and physicalranking models for text retrieval applications. In: Proceedings of DESIRES (2021)16. Lin, S.C., Yang, J.H., Lin, J.: In-batch negatives for knowledge distillation withtightly-coupled teachers for dense retrieval. In: Proceedings of RepL4NLP (2021)17. MacAvaney, S., Macdonald, C.: A Python interface to PISA! In: Proceedings ofSIGIR (2022)18. MacAvaney,S.,Macdonald,C.,Ounis,I.:Streamliningevaluationwithir-measures.In: Proceedings of ECIR (2022)19. MacAvaney, S., Nardini, F.M., Perego, R., Tonellotto, N., Goharian, N., Frieder,O.: Expansion via prediction of importance with contextualization. In: Proceedingsof SIGIR (2020)20. MacAvaney, S., Tonellotto, N., Macdonald, C.: Adaptive re-ranking with a corpusgraph. In: Proceedings of CIKM (2022)21. MacAvaney, S., Yates, A., Feldman, S., Downey,", "start_char_idx": 19574, "end_char_idx": 22350, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "48402b8e-ac2f-4ce6-ab19-7669795e5c7a": {"__data__": {"id_": "48402b8e-ac2f-4ce6-ab19-7669795e5c7a", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b054b4cf-4c7d-4fcc-8ed7-e2599a3213e1", "node_type": null, "metadata": {}, "hash": "28a1854ff1025f315a96541ba6917b8879f90b9da27a72400bc608490b342d2a"}, "2": {"node_id": "c4ebf6a3-5e9b-44b0-9da1-20f7d0aa4cc2", "node_type": null, "metadata": {}, "hash": "6cda9d5d43d427d33eb0faa7acc5f18cf5462e98600a8f061541fa04f3a667b8"}, "3": {"node_id": "0b2ff66a-7653-421e-aafd-afa1920966c3", "node_type": null, "metadata": {}, "hash": "089104cd74a9efb8fa548290e1e4fbb79726bf1667245882e77b931041cae6d7"}}, "hash": "2daac1ae27b47c9d7dd129f9ed7e2af9c219a2b214688981d76e9aa8bf2e4d97", "text": "MacAvaney, S., Yates, A., Feldman, S., Downey, D., Cohan, A., Goharian, N.:Simpli\ufb01ed data wrangling with ir_datasets. In: Proceedings of SIGIR (2021)22. Macdonald, C., Tonellotto, N.: Declarative experimentation in information re-trieval using PyTerrier. In: Proceedings of ICTIR (2020)23. Mallia, A., Khattab, O., Suel, T., Tonellotto, N.: Learning passage impacts forinverted indexes. In: Proceedings of SIGIR (2021)24. Mallia, A., Siedlaczek, M., Mackenzie, J., Suel, T.: PISA: performant indexes andsearch for academia. In: Proceedings of OSIRRC@SIGIR (2019)25. Maynez, J., Narayan, S., Bohnet, B., McDonald, R.: On faithfulness and factualityin abstractive summarization. In: Proceedings of ACL (2020)Doc2Query--: When Less is More 926. Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R., Deng,L.: MS MARCO: A human generated machine reading comprehension dataset. In:Proceedings of CoCo@NIPS (2016)27. Nogueira, R., Cho, K.: Passage re-ranking with BERT. ArXiv abs/1901.04085(2019)28. Nogueira, R., Lin, J.: From doc2query to doctttttquery (2019)29. Nogueira, R., Yang, W., Lin, J.J., Cho, K.: Document expansion by query predic-tion. ArXiv abs/1904.08375 (2019)30. Pickens, J., Cooper, M., Golovchinsky, G.: Reverted indexing for feedback andexpansion. In: Proceedings of CIKM (2010)31. Pradeep, R., Liu, Y., Zhang, X., Li, Y., Yates, A., Lin, J.: Squeezing water from astone: A bag of tricks for further improving cross-encoder e\ufb00ectiveness for rerank-ing. In: Proceedings of ECIR (2022)32. Pradeep, R., Nogueira, R., Lin, J.: The expando-mono-duo design pattern for textranking with pretrained sequence-to-sequence models. ArXiv abs/2101.05667(2021)33. Ra\ufb00el, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y.,Li, W., Liu, P.J., et al.: Exploring the limits of transfer learning with a uni\ufb01edtext-to-text transformer. J. Mach. Learn. Res. 21(140) (2020)34. Scells, H., Zhuang, S., Zuccon, G.: Reduce, reuse, recycle: Green information re-trieval research. In: Proceedings of SIGIR (2022)35. Tao, T., Wang, X., Mei, Q., Zhai, C.: Language model information retrieval withdocument expansion. In: Proceedings of HLT-NAACL (2006)36. Wang, X., MacAvaney, S., Macdonald, C., Ounis, I.: An inspection of the repro-ducibility and replicability of TCT-ColBERT. In: Proceedings of SIGIR (2022)37. Xiong, L., Xiong, C., Li, Y., Tang, K.F., Liu, J., Bennett, P.N., Ahmed, J., Over-wijk,A.:Approximatenearestneighbornegativecontrastivelearningfordensetextretrieval. In: Proceedings of ICLR (2021)38. Yu, S.Y., Liu, J., Yang, J., Xiong, C., Bennett, P.N., Gao, J., Liu, Z.: Few-shotgenerative conversational query rewriting. In: Proceedings of SIGIR (2020)39. Zhao,", "start_char_idx": 22353, "end_char_idx": 25055, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0b2ff66a-7653-421e-aafd-afa1920966c3": {"__data__": {"id_": "0b2ff66a-7653-421e-aafd-afa1920966c3", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b054b4cf-4c7d-4fcc-8ed7-e2599a3213e1", "node_type": null, "metadata": {}, "hash": "28a1854ff1025f315a96541ba6917b8879f90b9da27a72400bc608490b342d2a"}, "2": {"node_id": "48402b8e-ac2f-4ce6-ab19-7669795e5c7a", "node_type": null, "metadata": {}, "hash": "2daac1ae27b47c9d7dd129f9ed7e2af9c219a2b214688981d76e9aa8bf2e4d97"}}, "hash": "089104cd74a9efb8fa548290e1e4fbb79726bf1667245882e77b931041cae6d7", "text": "In: Proceedings of SIGIR (2020)39. Zhao, T., Lu, X., Lee, K.: SPARTA: E\ufb03cient open-domain question answering viasparse transformer matching retrieval. arXiv abs/2009.13013 (2020)40. Zhuang, S., Zuccon, G.: TILDE: Term independent likelihood model for passagere-ranking. In: Proceedings of SIGIR (2021)", "start_char_idx": 25055, "end_char_idx": 25356, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8cb18571-79e9-4976-b199-cafc5bd7697f": {"__data__": {"id_": "8cb18571-79e9-4976-b199-cafc5bd7697f", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a", "node_type": null, "metadata": {}, "hash": "2e178ba08a1978993ef067e5d07dfdf7a5adbab83c605a67f988ffd8fb0e5a11"}, "3": {"node_id": "78774d3a-d138-4c85-8718-fcf6d36289ac", "node_type": null, "metadata": {}, "hash": "ee94609241c17c53f21b48a14b112fe22e2cb82fc41a29764985dca4fe5cd431"}}, "hash": "3af6a37c0f7c636d23712036477c51afe0de2e8cb8fe7f8eaf437e8890b2d0f4", "text": "Query2doc: Query Expansion with Large Language ModelsLiang Wang and Nan Yang and Furu WeiMicrosoft Research{wangliang,nanya,fuwei}@microsoft.comAbstractThis paper introduces a simple yet effec-tive query expansion approach, denoted asquery2doc , to improve both sparse and denseretrieval systems. The proposed method\ufb01rst generates pseudo-documents by few-shotprompting large language models (LLMs), andthen expands the query with generated pseudo-documents. LLMs are trained on web-scaletext corpora and are adept at knowledge mem-orization. The pseudo-documents from LLMsoften contain highly relevant information thatcan aid in query disambiguation and guidethe retrievers. Experimental results demon-strate that query2doc boosts the performanceof BM25 by 3% to 15% on ad-hoc IR datasets,such as MS-MARCO and TREC DL, with-out any model \ufb01ne-tuning. Furthermore, ourmethod also bene\ufb01ts state-of-the-art dense re-trievers in terms of both in-domain and out-of-domain results.1 IntroductionInformation retrieval (IR) aims to locate relevantdocuments from a large corpus given a user is-sued query. It is a core component in modernsearch engines and researchers have invested fordecades in this \ufb01eld. There are two mainstreamparadigms for IR: lexical-based sparse retrieval,such as BM25, and embedding-based dense re-trieval (Xiong et al., 2021; Qu et al., 2021). Al-though dense retrievers perform better when largeamounts of labeled data are available (Karpukhinet al., 2020), BM25 remains competitive on out-of-domain datasets (Thakur et al., 2021).Query expansion (Rocchio, 1971; Lavrenkoand Croft, 2001) is a long-standing techniquethat rewrites the query based on pseudo-relevancefeedback or external knowledge sources such asWordNet. For sparse retrieval, it can help bridgethe lexical gap between the query and the docu-ments. However, query expansion methods likeRM3 (Lavrenko and Croft, 2001; Lv and Zhai,2009) have only shown limited success on populardatasets (Campos et al., 2016), and most state-of-the-art dense retrievers do not adopt this technique.In the meantime, document expansion methods likedoc2query (Nogueira et al., 2019) have proven tobe effective for sparse retrieval.In this paper, we demonstrate the effectivenessof LLMs (Brown et al., 2020) as query expan-sion models by generating pseudo-documents con-ditioned on few-shot prompts. Given that searchqueries are often short, ambiguous, or lack neces-sary background information, LLMs can providerelevant information to guide retrieval systems, asthey memorize an enormous amount of knowledgeand language patterns by pre-training on trillionsof tokens.Our proposed method, called query2doc , gen-erates pseudo-documents by few-shot promptingLLMs and concatenates them with the originalquery to form a new query. This method is simpleto implement and does not require any changes intraining pipelines or model architectures, making itorthogonal to the progress in the \ufb01eld of LLMs andinformation retrieval. Future methods can easilybuild upon our query expansion framework.For in-domain evaluation, we adopt the MS-MARCO passage ranking (Campos et al., 2016),TREC DL 2019 and 2020 datasets. Pseudo-documents are generated by prompting an im-proved version of GPT-3 text-davinci-003 fromOpenAI (Brown et al., 2020). Results show thatquery2doc substantially improves the off-the-shelfBM25 algorithm without \ufb01ne-tuning any model,particularly for hard queries from the TREC DLtrack. Strong dense retrievers, including DPR(Karpukhin et al., 2020), SimLM (Wang et al.,2022a), and E5 (Wang et al.,", "start_char_idx": 0, "end_char_idx": 3566, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "78774d3a-d138-4c85-8718-fcf6d36289ac": {"__data__": {"id_": "78774d3a-d138-4c85-8718-fcf6d36289ac", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a", "node_type": null, "metadata": {}, "hash": "2e178ba08a1978993ef067e5d07dfdf7a5adbab83c605a67f988ffd8fb0e5a11"}, "2": {"node_id": "8cb18571-79e9-4976-b199-cafc5bd7697f", "node_type": null, "metadata": {}, "hash": "3af6a37c0f7c636d23712036477c51afe0de2e8cb8fe7f8eaf437e8890b2d0f4"}, "3": {"node_id": "6af489ec-5950-45b6-a1fd-a5d538289ec9", "node_type": null, "metadata": {}, "hash": "7498c6b78c5951cd6f8107e7e1c713b2e9107c5f4dc977b7de44c1f2959ea646"}}, "hash": "ee94609241c17c53f21b48a14b112fe22e2cb82fc41a29764985dca4fe5cd431", "text": "(Wang et al.,2022a), and E5 (Wang et al., 2022b) also bene-\ufb01t from query2doc , although the gains tend to bediminishing when distilling from a strong cross-encoder based re-ranker. Experiments in zero-shotOOD settings demonstrate that our method out-arXiv:2303.07678v1  [cs.IR]  14 Mar 2023performs strong baselines on most datasets. Fur-ther analysis also reveals the importance of modelscales: query2doc works best when combined withthe most capable LLMs while small language mod-els only provide marginal improvements over base-lines.To aid reproduction, we release allthe generations from text-davinci-003at https://huggingface.co/datasets/intfloat/query2doc_msmarco .2 MethodWrite a passage that answers the given query:Query: what state is this zip code 85282Passage: Welcome to TEMPE, AZ 85282. 85282 is a rural zip code in Tempe, Arizona. The population is primarily white\u2026\u2026Query: when was pokemon green releasedPassage:LLM PromptsPokemon Green was released in Japan on February 27th, 1996. It was the first in the Pokemon series of games and served as the basis for Pokemon Red and Blue, which were released in the US in 1998. The original Pokemon Green remains a beloved classic among fans of the series.LLM OutputFigure 1: Illustration of query2doc few-shot prompting.We omit some in-context examples for space reasons.Given a query q, we employ few-shot promptingto generate a pseudo-document d0as depicted inFigure 1. The prompt comprises a brief instruction\u201cWrite a passage that answers the given query:\u201dandklabeled pairs randomly sampled from atraining set. We use k= 4throughout this paper.Subsequently, we rewrite qto a new query q+by concatenating with the pseudo-document d0.There are slight differences in the concatenationoperation for sparse and dense retrievers, whichwe elaborate on in the following section.Sparse Retrieval Since the query qis typicallymuch shorter than pseudo-documents, we boost thequery term weights by repeating the query ntimesbefore concatenating with the pseudo-document d0:q+=concat(fqg\u0002n; d0) (1)Here, \u201cconcat\u201d denotes the string concatenationfunction. q+is used as the new query forBM25 retrieval. We \ufb01nd that n= 5 is a gener-ally good value and do not tune it on a dataset basis.Dense Retrieval The new query q+is a sim-ple concatenation of the original query qand thepseudo-document d0separated by [SEP]:q+=concat(q;[SEP]; d0) (2)For training dense retrievers, several factors canin\ufb02uence the \ufb01nal performance, such as hard nega-tive mining (Xiong et al., 2021), intermediate pre-training (Gao and Callan, 2021), and knowledgedistillation from a cross-encoder based re-ranker(Qu et al., 2021). In this paper, we investigate twosettings to gain a more comprehensive understand-ing of our method. The \ufb01rst setting is training DPR(Karpukhin et al., 2020) models initialized fromBERT basewith BM25 hard negatives only. The op-timization objective is a standard contrastive loss:Lcont=\u0000logehq\u0001hdehq\u0001hd+Pdi2Nehq\u0001hdi(3)where hqandhdrepresent the embeddings for thequery and document, respectively. Ndenotes theset of hard negatives.The second setting is to build upon state-of-the-art dense retrievers and use KL divergence to distillfrom a cross-encoder teacher model.minDKL(pce;pstu) +\u000bLcont (4)pceandpstuare the probabilities from the cross-encoder and our student model, respectively. \u000bisa coef\ufb01cient to balance the distillation loss andcontrastive", "start_char_idx": 3533, "end_char_idx": 6932, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6af489ec-5950-45b6-a1fd-a5d538289ec9": {"__data__": {"id_": "6af489ec-5950-45b6-a1fd-a5d538289ec9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a", "node_type": null, "metadata": {}, "hash": "2e178ba08a1978993ef067e5d07dfdf7a5adbab83c605a67f988ffd8fb0e5a11"}, "2": {"node_id": "78774d3a-d138-4c85-8718-fcf6d36289ac", "node_type": null, "metadata": {}, "hash": "ee94609241c17c53f21b48a14b112fe22e2cb82fc41a29764985dca4fe5cd431"}, "3": {"node_id": "01215cdd-59d5-4833-b636-be288b1a908c", "node_type": null, "metadata": {}, "hash": "75cf585727719eb1c78f83e502105052cafe6e886e08a9277d5a5236e3a84077"}}, "hash": "7498c6b78c5951cd6f8107e7e1c713b2e9107c5f4dc977b7de44c1f2959ea646", "text": "\u000bisa coef\ufb01cient to balance the distillation loss andcontrastive loss.Comparison with Pseudo-relevance FeedbackOur proposed method can be viewed as a variantof pseudo-relevance feedback (PRF) (Lavrenkoand Croft, 2001; Lv and Zhai, 2009). In conven-tional PRF, the feedback signals for query expan-sion come from the top-k documents obtained inthe initial retrieval step, while our method promptsLLMs to generate pseudo-documents. Our methoddoes not rely on the quality of the initial retrieval re-sults, which are often noisy or irrelevant. Rather, itexploits cutting-edge LLMs to generate documentsthat are more likely to contain relevant terms.Method Fine-tuningMS MARCO dev TREC DL 19 TREC DL 20MRR@10 R@50 R@1k nDCG@10 nDCG@10Sparse retrievalBM25 7 18.4 58.5 85.7 51.2\u000347.7\u0003+ query2doc 7 21.4+3.065.3+6.891.8+6.166.2+15.062.9+15.2BM25 + RM3 7 15.8 56.7 86.4 52.2 47.4docT5query (Nogueira and Lin) 3 27.7 75.6 94.7 64.2 -Dense retrieval w/o distillationANCE (Xiong et al., 2021) 3 33.0 - 95.9 64.5 64.6HyDE (Gao et al., 2022) 7 - - - 61.3 57.9DPR bert-base (our impl.) 3 33.7 80.5 95.9 64.7 64.1+ query2doc 3 35.1+1.482.6+2.197.2+1.368.7+4.067.1+3.0Dense retrieval w/ distillationRocketQAv2 (Ren et al., 2021) 3 38.8 86.2 98.1 - -AR2 (Zhang et al., 2021) 3 39.5 87.8 98.6 - -SimLM (Wang et al., 2022a) 3 41.1 87.8 98.7 71.4 69.7+ query2doc 3 41.5+0.488.0+0.298.8+0.172.9+1.571.6+1.9E5base+ KD (Wang et al., 2022b) 3 40.7 87.6 98.6 74.3 70.7+ query2doc 3 41.5+0.888.1+0.598.7+0.174.9+0.672.5+1.8Table 1: Main results on the MS-MARCO passage ranking and TREC datasets. The \u201cFine-tuning\u201d column indi-cates whether the method requires \ufb01ne-tuning model on labeled data or not. \u0003: our reproduction.3 Experiments3.1 SetupEvaluation Datasets For in-domain evaluation,we utilize the MS-MARCO passage ranking (Cam-pos et al., 2016), TREC DL 2019 (Craswell et al.,2020a) and 2020 (Craswell et al., 2020b) datasets.For zero-shot out-of-domain evaluation, we select\ufb01ve low-resource datasets from the BEIR bench-mark (Thakur et al., 2021). The evaluation met-rics include MRR@10, R@k ( k2f50;1kg), andnDCG@10.Hyperparameters For sparse retrieval includingBM25 and RM3, we adopt the default implementa-tion from Pyserini (Lin et al., 2021). When trainingdense retrievers, we use mostly the same hyper-parameters as SimLM (Wang et al., 2022a), withthe exception of increasing the maximum querylength to 144to include pseudo-documents. Whenprompting LLMs, we include 4in-context exam-ples and use the default temperature of 1to sampleat most 128tokens. For further details, please referto Appendix A.3.2 Main ResultsIn Table 1, we list the results on the MS-MARCOpassage ranking and TREC DL datasets. For sparseretrieval, \u201cBM25 +", "start_char_idx": 6911, "end_char_idx": 9625, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "01215cdd-59d5-4833-b636-be288b1a908c": {"__data__": {"id_": "01215cdd-59d5-4833-b636-be288b1a908c", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a", "node_type": null, "metadata": {}, "hash": "2e178ba08a1978993ef067e5d07dfdf7a5adbab83c605a67f988ffd8fb0e5a11"}, "2": {"node_id": "6af489ec-5950-45b6-a1fd-a5d538289ec9", "node_type": null, "metadata": {}, "hash": "7498c6b78c5951cd6f8107e7e1c713b2e9107c5f4dc977b7de44c1f2959ea646"}, "3": {"node_id": "48dbf910-759d-4211-a5da-dd84f5253ea9", "node_type": null, "metadata": {}, "hash": "3cff0cfbd6f20e751697984f4dc637a647eaf9ffde726c094833c15d8b317d22"}}, "hash": "75cf585727719eb1c78f83e502105052cafe6e886e08a9277d5a5236e3a84077", "text": "and TREC DL datasets. For sparseretrieval, \u201cBM25 + query2doc\u201d beats the BM25baseline with over 15%improvements on TRECDL 2019 and 2020 datasets. Our manual inspectionreveals that most queries from the TREC DL trackare long-tailed entity-centric queries, which bene\ufb01tmore from the exact lexical match. The traditionalquery expansion method RM3 only marginallyimproves the R@1k metric. Although the docu-ment expansion method docT5query achieves bet-ter numbers on the MS-MARCO dev set, it requirestraining a T5-based query generator with all theavailable labeled data, while \u201cBM25 + query2doc\u201ddoes not require any model \ufb01ne-tuning.For dense retrieval, the model variants that com-bine with query2doc also outperform the corre-sponding baselines on all metrics. However, thegain brought by query2doc tends to diminish whenusing intermediate pre-training or knowledge distil-lation from cross-encoder re-rankers, as shown bythe \u201cSimLM + query2doc\u201d and \u201cE5 + query2doc\u201dresults.For zero-shot out-of-domain retrieval, the resultsare mixed as shown in Table 2. Entity-centricdatasets like DBpedia see the largest improvements.On the NFCorpus and Scifact datasets, we observea minor decrease in ranking quality. This is likelydue to the distribution mismatch between trainingand evaluation.4 AnalysisScaling up LLMs is Critical For our proposedmethod, a question that naturally arises is: howdoes the model scale affect the quality of queryexpansion? Table 3 shows that the performancesteadily improves as we go from the 1.3B modelDBpedia NFCorpus Scifact Trec-Covid Touche2020BM25 31.3 32.5 66.5 65.6 36.7+ query2doc 37.0+5.734.9+2.468.6+2.172.2+6.639.8+3.1SimLM (Wang et al., 2022a) 34.9 32.7 62.4 55.0 18.9+ query2doc 38.3+3.432.1-0.659.5-2.959.9+4.925.6+6.7E5base+ KD (Wang et al., 2022b) 40.7 35.0 70.4 74.1 30.9+ query2doc 42.4+1.735.2+0.267.5-2.975.1+1.031.7+0.8Table 2: Zero-shot out-of-domain results on 5 low-resource datasets from the BEIR benchmark (Thakur et al.,2021). The reported numbers are nDCG@10. For a fair comparison, the in-context examples for prompting LLMscome from the MS-MARCO training set.# params TREC 19 TREC 20BM25 - 51.2 47.7w/ babbage 1.3B 52.0 50.2w/ curie 6.7B 55.1 50.1w/ davinci-001 175B 63.5 58.2w/ davinci-003 175B 66.2 62.9Table 3: Query expansion with different model sizes.to 175B models. Empirically, the texts generatedby smaller language models tend to be shorter andcontain more factual errors. Also, the \u201cdavinci-003\u201dmodel outperforms its earlier version \u201cdavinci-001\u201dby using better training data and improvedinstruction tuning.1 10 30 50 100% labeled data for fine-tuning202224262830323436MRR on dev set21.427.331.432.833.722.728.532.134.135.1DPR w/o query2docDPR w/ query2docFigure 2: MRR on MS-MARCO dev set w.r.t the per-centage of labeled data used for \ufb01ne-tuning.Performance Gains are Consistent across DataScales Figure 2 presents a comparison betweentwo variants of DPR models, which differ in theamount of labeled data used. The results showthat the", "start_char_idx": 9639, "end_char_idx": 12638, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "48dbf910-759d-4211-a5da-dd84f5253ea9": {"__data__": {"id_": "48dbf910-759d-4211-a5da-dd84f5253ea9", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a", "node_type": null, "metadata": {}, "hash": "2e178ba08a1978993ef067e5d07dfdf7a5adbab83c605a67f988ffd8fb0e5a11"}, "2": {"node_id": "01215cdd-59d5-4833-b636-be288b1a908c", "node_type": null, "metadata": {}, "hash": "75cf585727719eb1c78f83e502105052cafe6e886e08a9277d5a5236e3a84077"}, "3": {"node_id": "48c8e6c5-d64c-402c-bd84-f97d641d5e2e", "node_type": null, "metadata": {}, "hash": "c9db70c7c84765b03402c65bde497c76393555e446bcdd899d133c094599a535"}}, "hash": "3cff0cfbd6f20e751697984f4dc637a647eaf9ffde726c094833c15d8b317d22", "text": "models, which differ in theamount of labeled data used. The results showthat the \u201cDPR + query2doc\u201d variant consistentlyoutperforms the DPR baseline by approximately1%, regardless of the amount of data used for\ufb01ne-tuning. This observation highlights that ourcontribution is orthogonal to the continual scalingup of supervision signals.TREC 19 TREC 20BM25 + query2doc 66.2 62.9w/ query only 51.2 47.7w/ pseudo-doc only 48.7 44.5Table 4: Using the concatenation of the original queryand the generated pseudo-documents perform substan-tially better.How to Use Pseudo-documents In this paper,we concatenate the original query and pseudo-documents as the new query. Alternatively, one cansolely use the pseudo-documents, as done in theapproach of HyDE (Gao et al., 2022). The resultspresented in Table 4 demonstrate that the originalquery and pseudo-documents are complementary,and their combination leads to substantially betterperformance in sparse retrieval.Case Analysis In Table 5, we show two queriesalong with their respective pseudo-documents andgroundtruth. The pseudo-documents, which aregenerated by LLMs, offer detailed and mostly ac-curate information, thereby reducing the lexicalmismatch between the query and documents. Insome cases, the pseudo-documents are suf\ufb01cient tomeet the user\u2019s information needs, rendering theretrieval step unnecessary. However, it is worthnoting that the LLM generations may contain fac-tual errors. For instance, in the second query, thetheme song \"It\u2019s a Jungle Out There\" was used asof season two in 2003, not 20021. Although sucherrors may appear subtle and dif\ufb01cult to verify, theypose a signi\ufb01cant challenge to building trustworthysystems using LLMs.1Refer to https://en.wikipedia.org/wiki/It\u2019s_a_Jungle_Out_There_(song)query who killed nicholas ii of russiaLLM generation Nicholas II of Russia, the last Tsar of Russia, was executed by Bolshevik revolutionaries onJuly 17th, 1918 , along with his wife Alexandra and their \ufb01ve children . The execution wasordered by Vladimir Lenin , the leader of the Bolshevik Party. The actual execution was carriedout by a \ufb01ring squad of Bolshevik troops, and the bodies were later buried in a mass grave. . .Groundtruth (1868- 1918 ). Nicholas II was the lastemperor, or tsar, of Russia, serving from 1894 to1917. Nicholas, his wife, and their \ufb01ve children were killed by the Bolsheviks , members of arevolutionary group (led by Lenin ) who seized control of the government in Russia duringthe October Revolution (see Russian Revolution) of 1917.query who sings monk theme songLLM generation The theme song for the television show Monk is entitled \" It\u2019s a Jungle Out There \" and issung by American singer-songwriter Randy Newman . The song was written speci\ufb01cally forthe show, and it has been used as the theme song since the series premiered in 2002 . It hasbeen praised by critics and fans alike and is often regarded as one of the best theme songs intelevision history.Groundtruth exists and is an alternate of. The Monk theme song is It\u2019s a Jungle Out There byRandyNewman . The Monk theme song is It\u2019s a Jungle Out There byRandy Newman .Table 5: Examples from the TREC DL 2020 dataset. Bold texts are the overlapping words between groundtruthand pseudo-documents generated from LLMs. The italicized red sentence demonstrates a factual error in languagemodel generations.5 Related WorkQuery Expansion and Document Expansionare two classical techniques to improve retrievalquality, particularly for sparse retrieval systems.Both techniques aim to minimize the lexical gap be-tween the query and the documents. Query expan-sion typically involves rewriting the query basedon relevance feedback (Lavrenko and", "start_char_idx": 12614, "end_char_idx": 16304, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "48c8e6c5-d64c-402c-bd84-f97d641d5e2e": {"__data__": {"id_": "48c8e6c5-d64c-402c-bd84-f97d641d5e2e", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a", "node_type": null, "metadata": {}, "hash": "2e178ba08a1978993ef067e5d07dfdf7a5adbab83c605a67f988ffd8fb0e5a11"}, "2": {"node_id": "48dbf910-759d-4211-a5da-dd84f5253ea9", "node_type": null, "metadata": {}, "hash": "3cff0cfbd6f20e751697984f4dc637a647eaf9ffde726c094833c15d8b317d22"}, "3": {"node_id": "e53571d2-5585-4ad1-aca5-adcbafb53b44", "node_type": null, "metadata": {}, "hash": "46f091c688eaa77aac4ace0a8480e2444315347a7490f2f970e194a077e3898d"}}, "hash": "c9db70c7c84765b03402c65bde497c76393555e446bcdd899d133c094599a535", "text": "involves rewriting the query basedon relevance feedback (Lavrenko and Croft, 2001;Rocchio, 1971) or lexical resources such as Word-Net (Miller, 1992). In cases where labeled rele-vance feedback is not available, the top-k retrieveddocuments can serve as pseudo-relevance feedbacksignals (Lv and Zhai, 2009).In contrast, document expansion enriches thedocument representation by appending additionalrelevant terms. Doc2query (Nogueira et al., 2019)trains a seq2seq model to predict pseudo-queriesbased on documents and then adds generatedpseudo-queries to the document index. Learnedsparse retrieval models such as SPLADE (Formalet al., 2021) and uniCOIL (Lin and Ma, 2021)also learn document term weighting in an end-to-end fashion. However, most state-of-the-art denseretrievers (Ren et al., 2021; Wang et al., 2022a)do not adopt any expansion techniques. Our pa-per demonstrates that strong dense retrievers alsobene\ufb01t from query expansion using LLMs.Large Language Models (LLMs) such as GPT-3(Brown et al., 2020), PaLM (Chowdhery et al.,2022), and LLaMA (Touvron et al., 2023) aretrained on trillions of tokens with billions of param-eters, exhibiting unparalleled generalization abilityacross various tasks. LLMs can follow instruc-tions in a zero-shot manner or conduct in-contextlearning through few-shot prompting. Labeling afew high-quality examples only requires minimalhuman effort. In this paper, we employ few-shotprompting to generate pseudo-documents from agiven query. A closely related recent work HyDE(Gao et al., 2022) instead focuses on the zero-shot setting and uses embeddings of the pseudo-documents for similarity search. HyDE implicitlyassumes that the groundtruth document and pseudo-documents express the same semantics in differentwords, which may not hold for some queries. In the\ufb01eld of question answering, RECITE (Sun et al.,2022) and GENREAD (Yu et al., 2022) demon-strate that LLMs are powerful context generatorsand can encode abundant factual knowledge. How-ever, as our analysis shows, LLMs can sometimesgenerate false claims, hindering their practical ap-plication in critical areas.6 ConclusionThis paper presents a simple method query2docto leverage LLMs for query expansion. It \ufb01rstprompts LLMs with few-shot examples to gener-ate pseudo-documents and then integrates with ex-isting sparse or dense retrievers by augmentingqueries with generated pseudo-documents. The un-derlying motivation is to distill the LLMs throughprompting. Despite its simplicity, empirical evalua-tions demonstrate consistent improvements acrossvarious retrieval models and datasets.LimitationsLLM call Index searchBM25 - 16ms+ query2doc >2000ms 177msTable 6: Latency analysis for retrieval systems with ourproposed query2doc. We retrieve the top 100 results forMS-MARCO dev queries with a single thread and thenaverage over all the queries. The latency for LLM APIcalls depends on server load and is dif\ufb01cult to preciselymeasure.An apparent limitation is the ef\ufb01ciency of re-trieval. Our method requires running inference withLLMs which can be considerably slower due to thetoken-by-token autoregressive decoding. Moreover,with query2doc, searching the inverted index alsobecomes slower as the number of query terms in-creases after expansion. This is supported by thebenchmarking results in Table 6. Real-world de-ployment of our method should take these factorsinto consideration.ReferencesAlexander Bondarenko, Maik Fr\u00f6be, Johannes Kiesel,Shahbaz Syed, Timon Gurcke, Meriem Beloucif,Alexander Panchenko, Chris Biemann, Benno Stein,Henning Wachsmuth, et al. 2022. Overview oftouch\u00e9 2022: argument", "start_char_idx": 16312, "end_char_idx": 19926, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e53571d2-5585-4ad1-aca5-adcbafb53b44": {"__data__": {"id_": "e53571d2-5585-4ad1-aca5-adcbafb53b44", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a", "node_type": null, "metadata": {}, "hash": "2e178ba08a1978993ef067e5d07dfdf7a5adbab83c605a67f988ffd8fb0e5a11"}, "2": {"node_id": "48c8e6c5-d64c-402c-bd84-f97d641d5e2e", "node_type": null, "metadata": {}, "hash": "c9db70c7c84765b03402c65bde497c76393555e446bcdd899d133c094599a535"}, "3": {"node_id": "f197ee4d-20a1-45d1-8984-446d552d3e19", "node_type": null, "metadata": {}, "hash": "d4441018058d56ac018753bb5b0da37cc2b2eb9afd751ed249477b60a881d049"}}, "hash": "46f091c688eaa77aac4ace0a8480e2444315347a7490f2f970e194a077e3898d", "text": "Wachsmuth, et al. 2022. Overview oftouch\u00e9 2022: argument retrieval. In Interna-tional Conference of the Cross-Language Evalua-tion Forum for European Languages , pages 311\u2013336. Springer.Vera Boteva, Demian Gholipour, Artem Sokolov, andStefan Riezler. 2016. A full-text learning to rankdataset for medical information retrieval. In Euro-pean Conference on Information Retrieval , pages716\u2013722. Springer.Tom B. Brown, Benjamin Mann, Nick Ryder, MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, Sandhini Agarwal, Ariel Herbert-V oss,Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,Clemens Winter, Christopher Hesse, Mark Chen,Eric Sigler, Mateusz Litwin, Scott Gray, BenjaminChess, Jack Clark, Christopher Berner, Sam Mc-Candlish, Alec Radford, Ilya Sutskever, and DarioAmodei. 2020. Language models are few-shot learn-ers. In Advances in Neural Information ProcessingSystems 33: Annual Conference on Neural Informa-tion Processing Systems 2020, NeurIPS 2020, De-cember 6-12, 2020, virtual .Daniel Fernando Campos, Tri Nguyen, Mir Rosenberg,Xia Song, Jianfeng Gao, Saurabh Tiwary, RanganMajumder, Li Deng, and Bhaskar Mitra. 2016. Msmarco: A human generated machine reading com-prehension dataset. ArXiv , abs/1611.09268.Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton,Sebastian Gehrmann, Parker Schuh, Kensen Shi,Sasha Tsvyashchenko, Joshua Maynez, AbhishekRao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vin-odkumar Prabhakaran, Emily Reif, Nan Du, Ben-ton C. Hutchinson, Reiner Pope, James Bradbury, Ja-cob Austin, Michael Isard, Guy Gur-Ari, PengchengYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-mawat, Sunipa Dev, Henryk Michalewski, XavierGarc\u00eda, Vedant Misra, Kevin Robinson, Liam Fe-dus, Denny Zhou, Daphne Ippolito, David Luan,Hyeontaek Lim, Barret Zoph, Alexander Spiridonov,Ryan Sepassi, David Dohan, Shivani Agrawal, MarkOmernick, Andrew M. Dai, Thanumalayan Sankara-narayana Pillai, Marie Pellat, Aitor Lewkowycz,Erica Moreira, Rewon Child, Oleksandr Polozov,Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-nan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta,Jason Wei, Kathleen S. Meier-Hellstern, DouglasEck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.Palm: Scaling language modeling with pathways.ArXiv , abs/2204.02311.Nick Craswell, Bhaskar Mitra, Emine Yilmaz, DanielCampos, and Ellen M V oorhees. 2020a. Overviewof the trec 2019 deep learning track. ArXiv preprint ,abs/2003.07820.Nick Craswell, Bhaskar Mitra, Emine Yilmaz,Daniel Fernando Campos, and Ellen M. V oorhees.2020b. Overview of the trec 2020 deep learningtrack. ArXiv , abs/2003.07820.Thibault Formal, Benjamin Piwowarski, and St\u00e9phaneClinchant. 2021.", "start_char_idx": 19939, "end_char_idx": 22758, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f197ee4d-20a1-45d1-8984-446d552d3e19": {"__data__": {"id_": "f197ee4d-20a1-45d1-8984-446d552d3e19", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a", "node_type": null, "metadata": {}, "hash": "2e178ba08a1978993ef067e5d07dfdf7a5adbab83c605a67f988ffd8fb0e5a11"}, "2": {"node_id": "e53571d2-5585-4ad1-aca5-adcbafb53b44", "node_type": null, "metadata": {}, "hash": "46f091c688eaa77aac4ace0a8480e2444315347a7490f2f970e194a077e3898d"}, "3": {"node_id": "c41e041e-ede6-483c-97a7-ae54740694bb", "node_type": null, "metadata": {}, "hash": "745635bce5fab3ad2654739831a615d4cdaf428abaf1b4651c53edd90849ab5b"}}, "hash": "d4441018058d56ac018753bb5b0da37cc2b2eb9afd751ed249477b60a881d049", "text": "Benjamin Piwowarski, and St\u00e9phaneClinchant. 2021. Splade: Sparse lexical and expan-sion model for \ufb01rst stage ranking. Proceedings ofthe 44th International ACM SIGIR Conference onResearch and Development in Information Retrieval .Luyu Gao and Jamie Callan. 2021. Condenser: a pre-training architecture for dense retrieval. In Proceed-ings of the 2021 Conference on Empirical Methodsin Natural Language Processing , pages 981\u2013993,Online and Punta Cana, Dominican Republic. Asso-ciation for Computational Linguistics.Luyu Gao, Xueguang Ma, Jimmy Lin, and JamieCallan. 2022. Precise zero-shot dense retrieval with-out relevance labels. ArXiv , abs/2212.10496.Faegheh Hasibi, Fedor Nikolaev, Chenyan Xiong,Krisztian Balog, Svein Erik Bratsberg, AlexanderKotov, and Jamie Callan. 2017. Dbpedia-entity v2:A test collection for entity search. In Proceedingsof the 40th International ACM SIGIR Conference onResearch and Development in Information Retrieval,Shinjuku, Tokyo, Japan, August 7-11, 2017 , pages1265\u20131268. ACM.Vladimir Karpukhin, Barlas Oguz, Sewon Min, PatrickLewis, Ledell Wu, Sergey Edunov, Danqi Chen, andWen-tau Yih. 2020. Dense passage retrieval foropen-domain question answering. In Proceedings ofthe 2020 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP) , pages 6769\u20136781, Online. Association for Computational Lin-guistics.Victor Lavrenko and W. Bruce Croft. 2001. Relevance-based language models. ACM SIGIR Forum , 51:260\u2013 267.Jimmy J. Lin and Xueguang Ma. 2021. A few briefnotes on deepimpact, coil, and a conceptual frame-work for information retrieval techniques. ArXiv ,abs/2106.14807.Jimmy J. Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, Rodrigo Nogueira, andDavid R. Cheriton. 2021. Pyserini: A python toolkitfor reproducible information retrieval research withsparse and dense representations. Proceedings of the44th International ACM SIGIR Conference on Re-search and Development in Information Retrieval .Yuanhua Lv and ChengXiang Zhai. 2009. A compara-tive study of methods for estimating query languagemodels with pseudo feedback. Proceedings of the18th ACM conference on Information and knowl-edge management .George A. Miller. 1992. WordNet: A lexical databasefor English. In Speech and Natural Language: Pro-ceedings of a Workshop Held at Harriman, NewYork, February 23-26, 1992 .Rodrigo Nogueira and Jimmy Lin. From doc2query todoctttttquery.Rodrigo Nogueira, Wei Yang, Jimmy J. Lin, andKyunghyun Cho. 2019. Document expansion byquery prediction. ArXiv , abs/1904.08375.Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, RuiyangRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu,and Haifeng Wang. 2021. RocketQA: An opti-mized training approach to dense passage retrievalfor open-domain question answering. In Proceed-ings of the 2021 Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics: Human Language Technologies , pages5835\u20135847, Online. Association for ComputationalLinguistics.Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao,QiaoQiao She, Hua Wu, Haifeng Wang, and Ji-RongWen. 2021. RocketQAv2: A joint training methodfor dense passage retrieval and passage", "start_char_idx": 22763, "end_char_idx": 25920, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c41e041e-ede6-483c-97a7-ae54740694bb": {"__data__": {"id_": "c41e041e-ede6-483c-97a7-ae54740694bb", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a", "node_type": null, "metadata": {}, "hash": "2e178ba08a1978993ef067e5d07dfdf7a5adbab83c605a67f988ffd8fb0e5a11"}, "2": {"node_id": "f197ee4d-20a1-45d1-8984-446d552d3e19", "node_type": null, "metadata": {}, "hash": "d4441018058d56ac018753bb5b0da37cc2b2eb9afd751ed249477b60a881d049"}, "3": {"node_id": "b8ddd884-9b64-437e-a4b4-ef65397bbddc", "node_type": null, "metadata": {}, "hash": "a2c89d782800569634224b2c808ced00a2926199f2a0069bcfdb6b0b9b33bf1f"}}, "hash": "745635bce5fab3ad2654739831a615d4cdaf428abaf1b4651c53edd90849ab5b", "text": "A joint training methodfor dense passage retrieval and passage re-ranking.InProceedings of the 2021 Conference on Empiri-cal Methods in Natural Language Processing , pages2825\u20132835, Online and Punta Cana, Dominican Re-public. Association for Computational Linguistics.J. J. Rocchio. 1971. Relevance feedback in informationretrieval.Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, andDenny Zhou. 2022. Recitation-augmented languagemodels. ArXiv , abs/2210.01296.Nandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Ab-hishek Srivastava, and Iryna Gurevych. 2021. Beir:A heterogeneous benchmark for zero-shot evalua-tion of information retrieval models. In Thirty-\ufb01fthConference on Neural Information Processing Sys-tems Datasets and Benchmarks Track (Round 2) .Hugo Touvron, Thibaut Lavril, Gautier Izacard, XavierMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,Faisal Azhar, Aur\u2019elien Rodriguez, Armand Joulin,Edouard Grave, and Guillaume Lample. 2023.Llama: Open and ef\ufb01cient foundation language mod-els.ArXiv , abs/2302.13971.Ellen V oorhees, Tasmeer Alam, Steven Bedrick, DinaDemner-Fushman, William R Hersh, Kyle Lo, KirkRoberts, Ian Soboroff, and Lucy Lu Wang. 2021.Trec-covid: constructing a pandemic information re-trieval test collection. In ACM SIGIR Forum , vol-ume 54, pages 1\u201312. ACM New York, NY , USA.David Wadden, Shanchuan Lin, Kyle Lo, Lucy LuWang, Madeleine van Zuylen, Arman Cohan, andHannaneh Hajishirzi. 2020. Fact or \ufb01ction: Verify-ing scienti\ufb01c claims. In Proceedings of the 2020Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP) , pages 7534\u20137550, On-line. Association for Computational Linguistics.Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao,Linjun Yang, Daxin Jiang, Rangan Majumder, andFuru Wei. 2022a. Simlm: Pre-training with rep-resentation bottleneck for dense passage retrieval.ArXiv , abs/2207.02578.Liang Wang, Nan Yang, Xiaolong Huang, Binx-ing Jiao, Linjun Yang, Daxin Jiang, Rangan Ma-jumder, and Furu Wei. 2022b. Text embeddings byweakly-supervised contrastive pre-training. ArXiv ,abs/2212.03533.Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,Jialin Liu, Paul N. Bennett, Junaid Ahmed, andArnold Overwijk. 2021. Approximate nearest neigh-bor negative contrastive learning for dense text re-trieval. In 9th International Conference on LearningRepresentations, ICLR 2021, Virtual Event, Austria,May 3-7, 2021 . OpenReview.net.W. Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingx-uan Ju, Soumya Sanyal, Chenguang Zhu, MichaelZeng, and Meng Jiang. 2022. Generate rather thanretrieve: Large language models are strong contextgenerators. ArXiv , abs/2209.10063.Hang Zhang, Yeyun Gong, Yelong Shen, JianchengLv, Nan Duan, and Weizhu Chen. 2021. Adversar-ial retriever-ranker for dense text retrieval. ArXivpreprint , abs/2110.03611.query who", "start_char_idx": 25912, "end_char_idx": 28744, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b8ddd884-9b64-437e-a4b4-ef65397bbddc": {"__data__": {"id_": "b8ddd884-9b64-437e-a4b4-ef65397bbddc", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a", "node_type": null, "metadata": {}, "hash": "2e178ba08a1978993ef067e5d07dfdf7a5adbab83c605a67f988ffd8fb0e5a11"}, "2": {"node_id": "c41e041e-ede6-483c-97a7-ae54740694bb", "node_type": null, "metadata": {}, "hash": "745635bce5fab3ad2654739831a615d4cdaf428abaf1b4651c53edd90849ab5b"}, "3": {"node_id": "909b333f-56e4-4b4b-89d0-b198b7cc82ea", "node_type": null, "metadata": {}, "hash": "437c675bade54afab96b1ed2889580e4462e6229cb54763c40a3157dd4e0e77b"}}, "hash": "a2c89d782800569634224b2c808ced00a2926199f2a0069bcfdb6b0b9b33bf1f", "text": "retrieval. ArXivpreprint , abs/2110.03611.query who sings hey good lookingLLM generation Hey Good Lookin\u2019 was originally written and recorded by the country music singer HankWilliams in1951 . The song has since been covered by several other artists and bands,including Jimmy Buffett, Conway Twitty, and Willie Nelson.Groundtruth Hey, Good Lookin\u2019. (1951) Hey, Good Lookin\u2019 is a 1951 song written and recorded byHankWilliams , and his version was inducted into the Grammy Hall of Fame in 2001. Since itsoriginal 1951 recording it has been covered by a variety of artists .query trumbull marriott fax numberLLM generation The fax number for the Trumbull Marriott Shelton is 203-378 -4444 .Groundtruth Business name: Trumbull Marriott Merritt Parkway; Address: 180 Hawley Lane Trumbull,Connecticut 06611; Phone number: 203-378-4958; Fax number: 203-378 -1400; Businesshours: 24; Credit cards accepted: Yes; Number of employees: 10-19; Map:query what is hra and hsaLLM generation HRA stands for Health Reimbursement Arrangement. It is a type of bene\ufb01t plan where anemployer reimburses employees for qualifying medical expenses . It also allows employersto set aside money tax-free to reimburse employees for medical expenses that are not coveredunder the employer\u2019s health insurance plan. HSA stands for Health Savings Account. It isa savings account that is used in conjunction with a quali\ufb01ed High Deductible Health Plan(HDHP) . An HSA enables an individual to pay for current health expenses and save for futurehealth expenses on a tax-free basis. Contributions to the account are taxGroundtruth The key difference between HRA and HSA is that HRA is an employer funded health bene\ufb01tplan that reimburses formedical expenses including personal health insurance policy pre-miums of employees whereas HSA is also a tax-advantaged health bene\ufb01t plan exclusivelyavailable to taxpayers in the United States who are enrolled in a High-Deductible Health Plan(HDHP) .Table 7: More examples of LLM generations. The format is the same as in Table 5.A Implementation DetailsFor dense retrieval experiments in Table 1, welist the hyperparameters in Table 8. Whentraining dense retrievers with distillation fromcross-encoder, we use the same teacher scorereleased by Wang et al.. The SimLM andE5 checkpoints for initialization are pub-licly available at https://huggingface.co/intfloat/simlm-base-msmarco andhttps://huggingface.co/intfloat/e5-base-unsupervised . To compute thetext embeddings, we utilize the [CLS] vector forSimLM and mean pooling for E5. This makes surethat the pooling mechanisms remain consistentbetween intermediate pre-training and \ufb01ne-tuning.When prompting LLMs, we include 4 in-contextexamples from the MS-MARCO training set. Toincrease prompt diversity, we randomly select 4examples for each API call. A complete prompt isshown in Table 9.Regarding out-of-domain evaluations on DBpe-dia (Hasibi et al., 2017), NFCorpus (Boteva et al.,2016), Scifact (Wadden et al., 2020), Trec-Covid(V oorhees et al., 2021), and Touche2020 (Bon-darenko et al., 2022), SimLM\u2019s results are basedon the released checkpoint by Wang et al..DPR w/ distillationlearning rate 2\u000210\u000053\u000210\u00005PLM BERT base SimLM / E5 base-unsup# of GPUs 4 4warmup steps 1000 1000batch size 64 64epoch 3 6\u000b n.a. 0.2negatives depth 1000 200query length 144 144passage length 144 144# of negatives 15 23Table 8: Hyper-parameters for training dense retrieverson MS-MARCO passage ranking dataset.For ablation experiments in Figure 2, we \ufb01ne-tune for 40 epochs or 18k steps, whichever isreached", "start_char_idx": 28752, "end_char_idx": 32303, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "909b333f-56e4-4b4b-89d0-b198b7cc82ea": {"__data__": {"id_": "909b333f-56e4-4b4b-89d0-b198b7cc82ea", "embedding": null, "metadata": {}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a", "node_type": null, "metadata": {}, "hash": "2e178ba08a1978993ef067e5d07dfdf7a5adbab83c605a67f988ffd8fb0e5a11"}, "2": {"node_id": "b8ddd884-9b64-437e-a4b4-ef65397bbddc", "node_type": null, "metadata": {}, "hash": "a2c89d782800569634224b2c808ced00a2926199f2a0069bcfdb6b0b9b33bf1f"}}, "hash": "437c675bade54afab96b1ed2889580e4462e6229cb54763c40a3157dd4e0e77b", "text": "for 40 epochs or 18k steps, whichever isreached \ufb01rst.promptsWrite a passage that answers the given query:Query: what state is this zip code 85282Passage: Welcome to TEMPE, AZ 85282. 85282 is a rural zip code in Tempe, Arizona. The populationis primarily white, and mostly single. At $200,200 the average home value here is a bit higher thanaverage for the Phoenix-Mesa-Scottsdale metro area, so this probably isn\u2019t the place to look for housingbargains.5282 Zip code is located in the Mountain time zone at 33 degrees latitude (Fun Fact: this is thesame latitude as Damascus, Syria!) and -112 degrees longitude.Query: why is gibbs model of re\ufb02ection goodPassage: In this re\ufb02ection, I am going to use Gibbs (1988) Re\ufb02ective Cycle. This model is a recognisedframework for my re\ufb02ection. Gibbs (1988) consists of six stages to complete one cycle which is ableto improve my nursing practice continuously and learning from the experience for better practice in thefuture.n conclusion of my re\ufb02ective assignment, I mention the model that I chose, Gibbs (1988) Re\ufb02ectiveCycle as my framework of my re\ufb02ective. I state the reasons why I am choosing the model as well as somediscussion on the important of doing re\ufb02ection in nursing practice.Query: what does a thousand pardons meansPassage: Oh, that\u2019s all right, that\u2019s all right, give us a rest; never mind about the direction, hang thedirection - I beg pardon, I beg a thousand pardons, I am not well to-day; pay no attention when I soliloquize,it is an old habit, an old, bad habit, and hard to get rid of when one\u2019s digestion is all disordered with eatingfood that was raised forever and ever before he was born; good land! a man can\u2019t keep his functionsregular on spring chickens thirteen hundred years old.Query: what is a macro warningPassage: Macro virus warning appears when no macros exist in the \ufb01le in Word. When you opena Microsoft Word 2002 document or template, you may receive the following macro virus warning,even though the document or template does not contain macros: C:\\<path>\\<\ufb01le name>contains macros.Macros may contain viruses.Query: when was pokemon green releasedPassage:outputPokemon Green was released in Japan on February 27th, 1996. It was the \ufb01rst in the Pokemon series ofgames and served as the basis for Pokemon Red and Blue, which were released in the US in 1998. Theoriginal Pokemon Green remains a beloved classic among fans of the series.Table 9: The full prompt used for the example in Figure 1.", "start_char_idx": 32303, "end_char_idx": 34777, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}}, "docstore/ref_doc_info": {"fe7ef5a9-c427-43b4-946f-d6f93765dde7": {"node_ids": ["fd150fb1-388d-4359-8477-79a451983254", "72ceec9f-a67d-405d-a90d-5cf289ad9bb8", "c3774bb9-5d96-4b9e-8614-fbb5361baa07", "631617a8-4364-422b-bb85-da005f29f8c2", "6c6a9365-d48a-44e5-b3ad-a1b4a3a49a62", "84a5d28d-c149-4062-96d2-305e5209dd44", "e844b618-3096-4a04-9ef1-057604988838", "e69c32cb-fb80-4e56-9f6b-cef9238a95cf", "87689162-96e4-4395-9f13-31c3710a19cf", "8422b33b-b068-4bcf-b083-60fbb4fbe50b", "d1ff20f6-ccc9-45fa-a72a-60171d2b0c92", "c155ef56-7eca-4a18-a396-2f7eb428d7c4", "e7c512d8-cda4-4438-a1f3-36ecedbf8f73", "dc6779c2-4747-4628-bbdc-7af4ecf9449a", "0b70dbb3-7efd-49d8-84a7-813edd0196e6", "a8f228fa-d17f-4ef3-8175-48fe13ee2ad6", "bfdcb818-99f4-4a0c-b76a-eefb141e73be", "be3c7305-6e99-4db0-ad3f-d729adf163e1", "619ba4a9-6d0e-40d1-8e7a-5e5b15686c33"], "metadata": {}}, "60534e99-9827-4f3b-9823-84d6aaab600d": {"node_ids": ["c227d33d-2e43-4176-b557-b913384f103a", "b119e89a-46de-4864-ab90-faae17968b6d", "e9501756-36af-4385-a6cc-cdce40361954", "7c047e1b-d810-48e8-8349-c4c51d06c128", "50c4a20a-f71b-4772-930b-7ff05adb5450", "0c9e59be-ec21-4eac-b871-36c291565153", "e55d7182-4c9f-4466-b8aa-af0b01c9ca7d", "e04d0610-0d39-48fd-a88a-ab2eb23cf03f", "ede0823c-d276-454d-bdae-217d6a5b68e5", "90a7a2df-f028-4023-a33d-b46ca7473bd6", "b1b6fea6-94a1-4a48-856b-3bc96fd7ffa8", "b4c218e3-4e77-42d9-88d3-789cb5d1a843", "136fb64b-5597-468d-be0a-67296d92d1ce"], "metadata": {}}, "4dc5e6ce-203f-481b-9cc5-8ed7ea0ee624": {"node_ids": ["ca23bbc0-f0b4-4662-9f00-07cb35a6fca0", "55b0b76e-94f8-4fb6-a951-0e4ae2da4530", "5f43d60b-e4f3-4270-ac71-274b0e43925f", "61664e49-76ac-415f-a515-a427d5e69e5b", "efebbb85-ec13-4515-9c24-78d3590af7ac", "82f74e8a-c8c2-4ab9-b558-f3ea8c8dd10c", "01b25f4b-c875-474d-8f30-c0239c28dd84"], "metadata": {}}, "b054b4cf-4c7d-4fcc-8ed7-e2599a3213e1": {"node_ids": ["09444cf9-5cf7-43c1-916c-ed03555a5041", "ca504482-fe85-4ae5-af0d-016753d626e2", "216330e4-a90e-4a22-b0fa-e98462b7a588", "d62b6bf6-a9fa-47f4-8e30-ebcef27b9bd7", "8bf0243a-df0a-411a-800d-6293f8cfca58", "b4c87216-d2e8-4abe-926f-65b7f4043ebf", "c4ebf6a3-5e9b-44b0-9da1-20f7d0aa4cc2", "48402b8e-ac2f-4ce6-ab19-7669795e5c7a", "0b2ff66a-7653-421e-aafd-afa1920966c3"], "metadata": {}}, "ca1a7fd7-5a65-4fcc-b191-3db4b5abc99a": {"node_ids": ["8cb18571-79e9-4976-b199-cafc5bd7697f", "78774d3a-d138-4c85-8718-fcf6d36289ac", "6af489ec-5950-45b6-a1fd-a5d538289ec9", "01215cdd-59d5-4833-b636-be288b1a908c", "48dbf910-759d-4211-a5da-dd84f5253ea9", "48c8e6c5-d64c-402c-bd84-f97d641d5e2e", "e53571d2-5585-4ad1-aca5-adcbafb53b44", "f197ee4d-20a1-45d1-8984-446d552d3e19", "c41e041e-ede6-483c-97a7-ae54740694bb", "b8ddd884-9b64-437e-a4b4-ef65397bbddc", "909b333f-56e4-4b4b-89d0-b198b7cc82ea"], "metadata": {}}}}